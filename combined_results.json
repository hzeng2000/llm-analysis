[
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15122107392.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 15751293952.0,
        "latency_per_micro_batch": 0.02772931059786069,
        "latency_fwd": 0.00924310353262023,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 28.398703279651993,
        "device_tokens_per_sec": 72.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62521003008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 63150189568.0,
        "latency_per_micro_batch": 0.27177839081613164,
        "latency_fwd": 0.09059279693871054,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 34.79615175869187,
        "device_tokens_per_sec": 235.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43938706944.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 44253320704.0,
        "latency_per_micro_batch": 0.6299300383856662,
        "latency_fwd": 0.20997667946188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 40.332895549113566,
        "device_tokens_per_sec": 203.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22256082432.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22570696192.0,
        "latency_per_micro_batch": 0.15819884796777506,
        "latency_fwd": 0.052732949322591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 40.507792515029145,
        "device_tokens_per_sec": 101.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 53805426176.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.31491829010707206,
        "latency_fwd": 0.10497276336902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 40.34388554044055,
        "device_tokens_per_sec": 50.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 62183836672.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.29786699070566625,
        "latency_fwd": 0.09928899690188875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 38.161720749663736,
        "device_tokens_per_sec": 53.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43108870912.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 43266198272.0,
        "latency_per_micro_batch": 0.36354106949723064,
        "latency_fwd": 0.12118035649907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 46.54194356462269,
        "device_tokens_per_sec": 176.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21969578752.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 22126906112.0,
        "latency_per_micro_batch": 0.3529815989070721,
        "latency_fwd": 0.11766053296902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 45.1903313290824,
        "device_tokens_per_sec": 181.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 956856320.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1913712640.0,
        "optimizer_state_memory_per_gpu": 5741137920.0,
        "(weight+op_state)_memory_per_gpu": 6697994240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44982028288.0,
        "(weight+op_state+grad)_memory_per_gpu": 8611706880.0,
        "estimated_peak_memory_per_gpu": 46240360448.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 27.05149303736416,
        "device_tokens_per_sec": 151.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43340148224.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 43654761984.0,
        "latency_per_micro_batch": 0.1467558677392085,
        "latency_fwd": 0.04891862257973617,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.004194304,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 37.57376103625128,
        "device_tokens_per_sec": 218.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 74522820608.0,
        "activation_memory_attn_per_gpu": 22209363968.0,
        "activation_memory_mlp_per_gpu": 22145925120.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 80601558528.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 80916172288.0,
        "latency_per_micro_batch": 0.292976048049939,
        "latency_fwd": 0.097658682683313,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0018284797558716145,
        "latency_fwd_tp_comm": 0.008388608,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 37.50519304540609,
        "device_tokens_per_sec": 436.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 8735401728.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 8892729088.0,
        "latency_per_micro_batch": 0.044774927163302274,
        "latency_fwd": 0.014924975721100757,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.004164191048205128,
        "latency_fwd_mlp": 0.007571256451282051,
        "latency_fwd_layernorm": 6.285399160808674e-05,
        "latency_fwd_tp_comm": 0.002691345066666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 45.85396919424174,
        "device_tokens_per_sec": 44.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14020224768.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 14177552128.0,
        "latency_per_micro_batch": 0.04609486098707209,
        "latency_fwd": 0.015364953662357364,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.004164191048205128,
        "latency_fwd_mlp": 0.007571256451282051,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.002691345066666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 47.205581429782036,
        "device_tokens_per_sec": 43.38,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 55220712192.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 55378039552.0,
        "latency_per_micro_batch": 0.7250784451659833,
        "latency_fwd": 0.24169281505532778,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 46.422192939514,
        "device_tokens_per_sec": 176.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 22000426496.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22315040256.0,
        "latency_per_micro_batch": 0.08049863954707209,
        "latency_fwd": 0.026832879849024032,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 41.22419088337964,
        "device_tokens_per_sec": 49.68,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21728381952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 22357568512.0,
        "latency_per_micro_batch": 0.06800352332116548,
        "latency_fwd": 0.02266784110705516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 34.826321674663745,
        "device_tokens_per_sec": 58.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 50551737344.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 51180923904.0,
        "latency_per_micro_batch": 0.29807670590566626,
        "latency_fwd": 0.09935890196855542,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 38.171593103721044,
        "device_tokens_per_sec": 107.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34081420032.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 34238747392.0,
        "latency_per_micro_batch": 0.7039595039856662,
        "latency_fwd": 0.23465316799522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 45.070580703973704,
        "device_tokens_per_sec": 181.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 67650627072.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 67965240832.0,
        "latency_per_micro_batch": 0.6383518675772306,
        "latency_fwd": 0.2127839558590769,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 40.88886393167809,
        "device_tokens_per_sec": 100.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55914728448.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 56543915008.0,
        "latency_per_micro_batch": 0.10985872403277022,
        "latency_fwd": 0.036619574677590076,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 28.127722579831822,
        "device_tokens_per_sec": 291.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 73819171584.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 73976498944.0,
        "latency_per_micro_batch": 0.7265464515659833,
        "latency_fwd": 0.24218215052199443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 46.5076595692001,
        "device_tokens_per_sec": 352.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31299390464.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 31928577024.0,
        "latency_per_micro_batch": 0.13547135921385295,
        "latency_fwd": 0.04515711973795098,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 34.689185692973375,
        "device_tokens_per_sec": 118.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37960829952.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 38590016512.0,
        "latency_per_micro_batch": 0.14930619666707212,
        "latency_fwd": 0.04976873222235737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 38.240161094566226,
        "device_tokens_per_sec": 53.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 31315779072.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 31630392832.0,
        "latency_per_micro_batch": 0.16046159166566618,
        "latency_fwd": 0.05348719722188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 41.08705490168927,
        "device_tokens_per_sec": 99.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 58335274496.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.31944377750285435,
        "latency_fwd": 0.10648125916761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 40.923147927100686,
        "device_tokens_per_sec": 50.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44626683648.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 44888827648.0,
        "latency_per_micro_batch": 0.7083635231856662,
        "latency_fwd": 0.23612117439522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 45.33970926290285,
        "device_tokens_per_sec": 722.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 62058100224.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 62372713984.0,
        "latency_per_micro_batch": 0.6389810131772307,
        "latency_fwd": 0.21299367105907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 40.91215793577369,
        "device_tokens_per_sec": 200.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 21697799936.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 21855127296.0,
        "latency_per_micro_batch": 0.09165403454566617,
        "latency_fwd": 0.03055134484855539,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 46.931309466401295,
        "device_tokens_per_sec": 87.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11128153856.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 11285481216.0,
        "latency_per_micro_batch": 0.08901416689812652,
        "latency_fwd": 0.029671388966042177,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 45.579697230860994,
        "device_tokens_per_sec": 89.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44362771968.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 44677385728.0,
        "latency_per_micro_batch": 0.28886196859922786,
        "latency_fwd": 0.09628732286640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.008388608,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 36.978590875715064,
        "device_tokens_per_sec": 443.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 74774744064.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 75403930624.0,
        "latency_per_micro_batch": 0.5951982939828544,
        "latency_fwd": 0.19839943132761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 38.12743675424114,
        "device_tokens_per_sec": 107.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 956856320.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1913712640.0,
        "optimizer_state_memory_per_gpu": 5741137920.0,
        "(weight+op_state)_memory_per_gpu": 6697994240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25840011264.0,
        "(weight+op_state+grad)_memory_per_gpu": 8611706880.0,
        "estimated_peak_memory_per_gpu": 27098343424.0,
        "latency_per_micro_batch": 0.02668073459786069,
        "latency_fwd": 0.008893578199286897,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 27.3257650007449,
        "device_tokens_per_sec": 74.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17470578176.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 17785191936.0,
        "latency_per_micro_batch": 0.07936726769812653,
        "latency_fwd": 0.026455755899375513,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 40.644928496719515,
        "device_tokens_per_sec": 50.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31540587264.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 31697914624.0,
        "latency_per_micro_batch": 0.7054275103856662,
        "latency_fwd": 0.23514250346188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 45.15604733365981,
        "device_tokens_per_sec": 362.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 58590930432.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.6293008927856663,
        "latency_fwd": 0.20976696426188873,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 40.30960154501797,
        "device_tokens_per_sec": 101.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63653956608.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 64283143168.0,
        "latency_per_micro_batch": 0.5926007327856662,
        "latency_fwd": 0.19753357759522205,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 37.9442216460784,
        "device_tokens_per_sec": 215.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62794445568.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 63056589568.0,
        "latency_per_micro_batch": 2.8142309536572307,
        "latency_fwd": 0.9380769845524102,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.26650822708512817,
        "latency_fwd_mlp": 0.48456041288205126,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.17224608426666668,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.008603700512820512,
        "latency_per_iter": 45.04486770740676,
        "device_tokens_per_sec": 727.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24693115904.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 25322302464.0,
        "latency_per_micro_batch": 0.054922933767243366,
        "latency_fwd": 0.018307644589081122,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 28.12443131627125,
        "device_tokens_per_sec": 145.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25220754944.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 25535368704.0,
        "latency_per_micro_batch": 0.14469882801385295,
        "latency_fwd": 0.048232942671284314,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.004194304,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 37.04715886656025,
        "device_tokens_per_sec": 221.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25484666624.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 25641993984.0,
        "latency_per_micro_batch": 0.3544496053070721,
        "latency_fwd": 0.1181498684356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 45.37399325832544,
        "device_tokens_per_sec": 361.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 43835132928.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 44464319488.0,
        "latency_per_micro_batch": 0.10931018010600874,
        "latency_fwd": 0.03643672670200291,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 27.98729533458088,
        "device_tokens_per_sec": 292.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15649746432.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 15964360192.0,
        "latency_per_micro_batch": 0.07261725772116548,
        "latency_fwd": 0.024205752573721824,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.002097152,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 37.18429484825062,
        "device_tokens_per_sec": 110.15,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 3,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7248517120.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14497034240.0,
        "optimizer_state_memory_per_gpu": 43491102720.0,
        "(weight+op_state)_memory_per_gpu": 50739619840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 69881636864.0,
        "(weight+op_state+grad)_memory_per_gpu": 65236654080.0,
        "estimated_peak_memory_per_gpu": 71139969024.0,
        "latency_per_micro_batch": 0.28692131090707207,
        "latency_fwd": 0.09564043696902402,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 36.761477168935045,
        "device_tokens_per_sec": 55.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44511939584.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 45141126144.0,
        "latency_per_micro_batch": 0.2965682101070721,
        "latency_fwd": 0.0988560700356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 37.978505641500995,
        "device_tokens_per_sec": 107.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24748280832.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 25377467392.0,
        "latency_per_micro_batch": 0.06834636327539141,
        "latency_fwd": 0.022782121091797138,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 35.00185573122742,
        "device_tokens_per_sec": 58.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68161938944.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 68476552704.0,
        "latency_per_micro_batch": 1.2580660981428544,
        "latency_fwd": 0.41935536604761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 40.29245954730667,
        "device_tokens_per_sec": 203.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 82646806016.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 83171094016.0,
        "latency_per_micro_batch": 0.5771882497699777,
        "latency_fwd": 0.19239608325665924,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06057005161025641,
        "latency_fwd_mlp": 0.11012736656410256,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0018562840761593374,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 36.94430688029247,
        "device_tokens_per_sec": 886.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29582194176.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 32195975680.0,
        "latency_per_micro_batch": 0.15788427516777503,
        "latency_fwd": 0.05262809172259168,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 40.43574753538134,
        "device_tokens_per_sec": 50.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14791322368.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 16098233600.0,
        "latency_per_micro_batch": 0.08864716529812654,
        "latency_fwd": 0.029549055099375512,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 45.39603530161796,
        "device_tokens_per_sec": 45.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 20076145408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 20233472768.0,
        "latency_per_micro_batch": 0.09128703294566617,
        "latency_fwd": 0.030429010981888726,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 46.74764753715825,
        "device_tokens_per_sec": 43.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62598678528.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 63857010688.0,
        "latency_per_micro_batch": 0.2611795621992279,
        "latency_fwd": 0.08705985406640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 33.448019374154434,
        "device_tokens_per_sec": 122.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30244173824.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 31502505984.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 27.054578697393588,
        "device_tokens_per_sec": 75.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50682604288.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 50944748288.0,
        "latency_per_micro_batch": 1.4103193333428543,
        "latency_fwd": 0.4701064444476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 45.13890533594851,
        "device_tokens_per_sec": 725.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 15394090496.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 15708704256.0,
        "latency_per_micro_batch": 0.037090732506160635,
        "latency_fwd": 0.012363577502053545,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.001048576,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 37.98516898132239,
        "device_tokens_per_sec": 53.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 59163937792.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.2963584949070721,
        "latency_fwd": 0.09878616496902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 37.96863328744369,
        "device_tokens_per_sec": 53.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 26903163648.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.21014068927463186,
        "latency_fwd": 0.07004689642487728,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 53.813188903196824,
        "device_tokens_per_sec": 38.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50441407488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 51070594048.0,
        "latency_per_micro_batch": 0.2704070309992279,
        "latency_fwd": 0.09013567699974263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 34.62061770212819,
        "device_tokens_per_sec": 236.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 32187986688.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 32345314048.0,
        "latency_per_micro_batch": 0.21014068927463186,
        "latency_fwd": 0.07004689642487728,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 53.813188903196824,
        "device_tokens_per_sec": 38.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 49946484224.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 50261097984.0,
        "latency_per_micro_batch": 0.3203874959028543,
        "latency_fwd": 0.1067958319676181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 41.01848691084408,
        "device_tokens_per_sec": 199.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37052950272.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 37210277632.0,
        "latency_per_micro_batch": 0.18277238166285434,
        "latency_fwd": 0.060924127220951446,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 46.794173484710925,
        "device_tokens_per_sec": 175.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15913658112.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 16070985472.0,
        "latency_per_micro_batch": 0.17749264636777506,
        "latency_fwd": 0.05916421545592502,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 45.44256124917063,
        "device_tokens_per_sec": 180.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43427395072.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 43742008832.0,
        "latency_per_micro_batch": 0.3197583503028544,
        "latency_fwd": 0.10658611676761812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 40.94644193119629,
        "device_tokens_per_sec": 100.03,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43652428544.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 43809755904.0,
        "latency_per_micro_batch": 1.4073833205428543,
        "latency_fwd": 0.4691277735142848,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 45.053438706262405,
        "device_tokens_per_sec": 363.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 18142006272.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 18771192832.0,
        "latency_per_micro_batch": 0.027866446579551063,
        "latency_fwd": 0.009288815526517022,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 28.539130524902934,
        "device_tokens_per_sec": 71.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30732913664.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 31362100224.0,
        "latency_per_micro_batch": 0.055197205730624116,
        "latency_fwd": 0.018399068576874705,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 28.264858561522193,
        "device_tokens_per_sec": 144.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 34112042496.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34426656256.0,
        "latency_per_micro_batch": 0.16014701886566618,
        "latency_fwd": 0.05338233962188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 41.01500992204147,
        "device_tokens_per_sec": 49.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 39865561856.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 40022889216.0,
        "latency_per_micro_batch": 0.36280706629723064,
        "latency_fwd": 0.12093568876574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 46.45647693493659,
        "device_tokens_per_sec": 88.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29295915776.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.3522475957070721,
        "latency_fwd": 0.1174158652356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 45.104864699396295,
        "device_tokens_per_sec": 90.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75733552128.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 76362738688.0,
        "latency_per_micro_batch": 0.5956177243828544,
        "latency_fwd": 0.19853924146095148,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 38.137309108298446,
        "device_tokens_per_sec": 214.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 67763250944.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 67920578304.0,
        "latency_per_micro_batch": 0.36500907589723064,
        "latency_fwd": 0.12166969196574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 46.725605493865736,
        "device_tokens_per_sec": 350.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68734946304.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 69364132864.0,
        "latency_per_micro_batch": 0.5921813023856661,
        "latency_fwd": 0.19739376746188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 37.93434929202109,
        "device_tokens_per_sec": 107.98,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 5688426485514240.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 82910717696.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 83435005696.0,
        "latency_per_micro_batch": 1.4161913589428545,
        "latency_fwd": 0.4720637863142848,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.004093246209492671,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 45.32256726519156,
        "device_tokens_per_sec": 1445.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24709443072.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 25024056832.0,
        "latency_per_micro_batch": 0.07364577758384326,
        "latency_fwd": 0.024548592527947753,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.002097152,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 37.71089701794165,
        "device_tokens_per_sec": 108.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50969107968.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 51283721728.0,
        "latency_per_micro_batch": 0.6311883295856662,
        "latency_fwd": 0.21039610986188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 40.404940528761365,
        "device_tokens_per_sec": 405.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49386190848.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 50644523008.0,
        "latency_per_micro_batch": 0.10511587610600871,
        "latency_fwd": 0.03503862536866957,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 26.917442715703213,
        "device_tokens_per_sec": 152.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43456661504.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 44714993664.0,
        "latency_per_micro_batch": 0.13085762481385294,
        "latency_fwd": 0.04361920827128431,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 33.516587364999616,
        "device_tokens_per_sec": 61.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 27753720576.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 27911047936.0,
        "latency_per_micro_batch": 0.18203837846285434,
        "latency_fwd": 0.06067945948761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 46.61051155546788,
        "device_tokens_per_sec": 87.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17184074496.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 17341401856.0,
        "latency_per_micro_batch": 0.17675864316777506,
        "latency_fwd": 0.058919547722591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 45.258899319927586,
        "device_tokens_per_sec": 90.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34367698432.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34682312192.0,
        "latency_per_micro_batch": 0.3152328629070721,
        "latency_fwd": 0.10507762096902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 40.36717954453616,
        "device_tokens_per_sec": 101.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31827090944.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 32141704704.0,
        "latency_per_micro_batch": 0.3158620085070721,
        "latency_fwd": 0.10528733616902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 40.439224524183956,
        "device_tokens_per_sec": 202.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63080723968.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 63395337728.0,
        "latency_per_micro_batch": 1.2593243893428543,
        "latency_fwd": 0.4197747964476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 40.31575355140227,
        "device_tokens_per_sec": 406.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34940931072.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 35570117632.0,
        "latency_per_micro_batch": 0.14855194876777505,
        "latency_fwd": 0.049517316255925016,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 38.04707363234618,
        "device_tokens_per_sec": 53.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 37339188224.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 37968374784.0,
        "latency_per_micro_batch": 0.1361570391223048,
        "latency_fwd": 0.045385679707434935,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 34.86471974953705,
        "device_tokens_per_sec": 117.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 10864242176.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 11178855936.0,
        "latency_per_micro_batch": 0.036576472574821745,
        "latency_fwd": 0.012192157524940581,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 5.713999237098795e-05,
        "latency_fwd_tp_comm": 0.001048576,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 37.458566811631364,
        "device_tokens_per_sec": 54.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 36.46427234304,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 18142006272.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 18771192832.0,
        "latency_per_micro_batch": 0.027866446579551063,
        "latency_fwd": 0.009288815526517022,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.137699551807718,
        "device_tokens_per_sec": 286.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 3,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7248517120.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14497034240.0,
        "optimizer_state_memory_per_gpu": 43491102720.0,
        "(weight+op_state)_memory_per_gpu": 50739619840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 69881636864.0,
        "(weight+op_state+grad)_memory_per_gpu": 65236654080.0,
        "estimated_peak_memory_per_gpu": 71139969024.0,
        "latency_per_micro_batch": 0.28692131090707207,
        "latency_fwd": 0.09564043696902402,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0335227552,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.217031321856124,
        "device_tokens_per_sec": 222.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 22000426496.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22315040256.0,
        "latency_per_micro_batch": 0.08049863954707209,
        "latency_fwd": 0.026832879849024032,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 10.31271329730396,
        "device_tokens_per_sec": 198.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 20288932864.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 20918119424.0,
        "latency_per_micro_batch": 0.001156767059247243,
        "latency_fwd": 0.00038558901974908094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 0.5930682905876536,
        "device_tokens_per_sec": 6906.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21728381952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 22357568512.0,
        "latency_per_micro_batch": 0.06800352332116548,
        "latency_fwd": 0.02266784110705516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 8.7129687193362,
        "device_tokens_per_sec": 235.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 70,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 20529822464.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 20687149824.0,
        "latency_per_micro_batch": 0.06750584576724336,
        "latency_fwd": 0.02250194858908112,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.003914683733333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 8.641720581808045,
        "device_tokens_per_sec": 1895.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 74522820608.0,
        "activation_memory_attn_per_gpu": 22209363968.0,
        "activation_memory_mlp_per_gpu": 22145925120.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 80601558528.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 80916172288.0,
        "latency_per_micro_batch": 0.292976048049939,
        "latency_fwd": 0.097658682683313,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0018284797558716145,
        "latency_fwd_tp_comm": 0.008388608,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.379492432611944,
        "device_tokens_per_sec": 1746.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 62183836672.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.29786699070566625,
        "latency_fwd": 0.09928899690188875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0670455104,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.566489641919775,
        "device_tokens_per_sec": 214.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 53712636928.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 54341823488.0,
        "latency_per_micro_batch": 0.05561663613062412,
        "latency_fwd": 0.01853887871020804,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.121275816567743,
        "device_tokens_per_sec": 1150.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24709443072.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 25024056832.0,
        "latency_per_micro_batch": 0.07364577758384326,
        "latency_fwd": 0.024548592527947753,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.002097152,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 9.430918425745833,
        "device_tokens_per_sec": 434.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 62058100224.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 62372713984.0,
        "latency_per_micro_batch": 0.6389810131772307,
        "latency_fwd": 0.21299367105907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 10.24106930326662,
        "device_tokens_per_sec": 799.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12346578432.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 12661192192.0,
        "latency_per_micro_batch": 0.029826462597860693,
        "latency_fwd": 0.009942154199286898,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0008388608,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.6375190499338155,
        "device_tokens_per_sec": 536.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63080723968.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 63395337728.0,
        "latency_per_micro_batch": 1.2593243893428543,
        "latency_fwd": 0.4197747964476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 10.091968207173764,
        "device_tokens_per_sec": 1623.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 20816530944.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 21131144704.0,
        "latency_per_micro_batch": 0.030455608197860695,
        "latency_fwd": 0.010151869399286898,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0008388608,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.797808900156343,
        "device_tokens_per_sec": 1050.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44511939584.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 45141126144.0,
        "latency_per_micro_batch": 0.2965682101070721,
        "latency_fwd": 0.0988560700356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.507957471222069,
        "device_tokens_per_sec": 430.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22256082432.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22570696192.0,
        "latency_per_micro_batch": 0.15819884796777506,
        "latency_fwd": 0.052732949322591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 10.133613705216336,
        "device_tokens_per_sec": 404.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50969107968.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 51283721728.0,
        "latency_per_micro_batch": 0.6311883295856662,
        "latency_fwd": 0.21039610986188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 10.10790070864939,
        "device_tokens_per_sec": 1620.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43108870912.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 43266198272.0,
        "latency_per_micro_batch": 0.36354106949723064,
        "latency_fwd": 0.12118035649907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 11.642000892888548,
        "device_tokens_per_sec": 703.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21969578752.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 22126906112.0,
        "latency_per_micro_batch": 0.3529815989070721,
        "latency_fwd": 0.11766053296902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 11.304097834003475,
        "device_tokens_per_sec": 724.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12920015872.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 13549202432.0,
        "latency_per_micro_batch": 0.014237356613169352,
        "latency_fwd": 0.0047457855377231175,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 2.285599694839518e-05,
        "latency_fwd_tp_comm": 0.00013981013333333334,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 7.291872977790564,
        "device_tokens_per_sec": 280.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 74522820608.0,
        "activation_memory_attn_per_gpu": 22209363968.0,
        "activation_memory_mlp_per_gpu": 22145925120.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 77298390528.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 77613004288.0,
        "latency_per_micro_batch": 0.11934441988629318,
        "latency_fwd": 0.03978147329543106,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.0007313919023486457,
        "latency_fwd_tp_comm": 0.0033554432,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 7.639987497604242,
        "device_tokens_per_sec": 2144.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 70,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 10958813952.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 11116141312.0,
        "latency_per_micro_batch": 0.034020766597860685,
        "latency_fwd": 0.01134025553262023,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.001957341866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 8.71028857265323,
        "device_tokens_per_sec": 940.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 10864242176.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 11178855936.0,
        "latency_per_micro_batch": 0.036576472574821745,
        "latency_fwd": 0.012192157524940581,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 5.713999237098795e-05,
        "latency_fwd_tp_comm": 0.001048576,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 9.367835874168263,
        "device_tokens_per_sec": 218.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 32098106112.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 32255433472.0,
        "latency_per_micro_batch": 0.034980718469693285,
        "latency_fwd": 0.011660239489897762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.001957341866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 8.956036251842376,
        "device_tokens_per_sec": 914.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 5432172288.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 5589499648.0,
        "latency_per_micro_batch": 0.020862947201649877,
        "latency_fwd": 0.006954315733883293,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0018928141128205127,
        "latency_fwd_mlp": 0.003441480205128205,
        "latency_fwd_layernorm": 2.8569996185493976e-05,
        "latency_fwd_tp_comm": 0.0012233386666666668,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 10.683958442652072,
        "device_tokens_per_sec": 191.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 10716995328.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 10874322688.0,
        "latency_per_micro_batch": 0.02146291712154525,
        "latency_fwd": 0.00715430570718175,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0018928141128205127,
        "latency_fwd_mlp": 0.003441480205128205,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0012233386666666668,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 10.991143041638503,
        "device_tokens_per_sec": 186.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62521003008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 63150189568.0,
        "latency_per_micro_batch": 0.27177839081613164,
        "latency_fwd": 0.09059279693871054,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 8.70542624034323,
        "device_tokens_per_sec": 941.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 12090922496.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 12405536256.0,
        "latency_per_micro_batch": 0.015386778985704907,
        "latency_fwd": 0.005128926328568303,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0004194304,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 7.87997546556239,
        "device_tokens_per_sec": 259.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 43835132928.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 44464319488.0,
        "latency_per_micro_batch": 0.10931018010600874,
        "latency_fwd": 0.03643672670200291,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 6.999740754227205,
        "device_tokens_per_sec": 1170.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 18394570496.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 18551897856.0,
        "latency_per_micro_batch": 0.04239014681461249,
        "latency_fwd": 0.014130048938204163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0024466773333333336,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 10.854007059948133,
        "device_tokens_per_sec": 377.37,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43938706944.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 44253320704.0,
        "latency_per_micro_batch": 0.6299300383856662,
        "latency_fwd": 0.20997667946188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 10.096253706601589,
        "device_tokens_per_sec": 811.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 7824924416.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 7982251776.0,
        "latency_per_micro_batch": 0.041190206974821744,
        "latency_fwd": 0.013730068991607247,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 5.713999237098795e-05,
        "latency_fwd_tp_comm": 0.0024466773333333336,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 10.546822460961701,
        "device_tokens_per_sec": 388.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21406275072.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 21720888832.0,
        "latency_per_micro_batch": 0.030237870542931807,
        "latency_fwd": 0.010079290180977269,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0008388608,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.742839483872021,
        "device_tokens_per_sec": 529.0,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 51510545408.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 52139731968.0,
        "latency_per_micro_batch": 0.0017778466900164735,
        "latency_fwd": 0.0005926155633388245,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 0.4559323088972824,
        "device_tokens_per_sec": 17967.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41059603968.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 41374217728.0,
        "latency_per_micro_batch": 0.11769878810600873,
        "latency_fwd": 0.039232929368669574,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0033554432,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 7.534667063666037,
        "device_tokens_per_sec": 2174.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15649746432.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 15964360192.0,
        "latency_per_micro_batch": 0.07261725772116548,
        "latency_fwd": 0.024205752573721824,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.002097152,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 9.299267883323077,
        "device_tokens_per_sec": 440.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22491024384.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 23120210944.0,
        "latency_per_micro_batch": 0.027939025797860693,
        "latency_fwd": 0.009313008599286897,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.154736996100193,
        "device_tokens_per_sec": 572.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68161938944.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 68476552704.0,
        "latency_per_micro_batch": 1.2580660981428544,
        "latency_fwd": 0.41935536604761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.1340910208,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 10.098873191878162,
        "device_tokens_per_sec": 811.18,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17470578176.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 17785191936.0,
        "latency_per_micro_batch": 0.07936726769812653,
        "latency_fwd": 0.026455755899375513,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 10.167897700638928,
        "device_tokens_per_sec": 201.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34940931072.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 35570117632.0,
        "latency_per_micro_batch": 0.14855194876777505,
        "latency_fwd": 0.049517316255925016,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 9.525099468933364,
        "device_tokens_per_sec": 215.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 62808406784.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 62965734144.0,
        "latency_per_micro_batch": 0.06942574951090856,
        "latency_fwd": 0.023141916503636187,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.0007313919023486457,
        "latency_fwd_tp_comm": 0.003914683733333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 8.88746826099719,
        "device_tokens_per_sec": 1843.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75733552128.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 76362738688.0,
        "latency_per_micro_batch": 0.5956177243828544,
        "latency_fwd": 0.19853924146095148,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 9.547658337921431,
        "device_tokens_per_sec": 858.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 7561074176.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 7875687936.0,
        "latency_per_micro_batch": 0.01518107501316935,
        "latency_fwd": 0.005060358337723117,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 2.285599694839518e-05,
        "latency_fwd_tp_comm": 0.0004194304,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 7.774655031624185,
        "device_tokens_per_sec": 263.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 15394090496.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 15708704256.0,
        "latency_per_micro_batch": 0.037090732506160635,
        "latency_fwd": 0.012363577502053545,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.001048576,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 9.499486416591019,
        "device_tokens_per_sec": 215.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 74774744064.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 75403930624.0,
        "latency_per_micro_batch": 0.5951982939828544,
        "latency_fwd": 0.19839943132761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0670455104,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 9.557918643064125,
        "device_tokens_per_sec": 428.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44362771968.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 44677385728.0,
        "latency_per_micro_batch": 0.28886196859922786,
        "latency_fwd": 0.09628732286640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.008388608,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.247841890189187,
        "device_tokens_per_sec": 1771.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15913658112.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 16070985472.0,
        "latency_per_micro_batch": 0.17749264636777506,
        "latency_fwd": 0.05916421545592502,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 11.363973146557822,
        "device_tokens_per_sec": 720.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37052950272.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 37210277632.0,
        "latency_per_micro_batch": 0.18277238166285434,
        "latency_fwd": 0.060924127220951446,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 11.701876205442895,
        "device_tokens_per_sec": 700.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 10717924352.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 11347110912.0,
        "latency_per_micro_batch": 0.0008462272438626275,
        "latency_fwd": 0.00028207574795420916,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 0.8673402539683959,
        "device_tokens_per_sec": 2361.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 55220712192.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 55378039552.0,
        "latency_per_micro_batch": 0.7250784451659833,
        "latency_fwd": 0.24169281505532778,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 11.618427571546802,
        "device_tokens_per_sec": 705.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31299390464.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 31928577024.0,
        "latency_per_micro_batch": 0.13547135921385295,
        "latency_fwd": 0.04515711973795098,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 8.678684723913607,
        "device_tokens_per_sec": 471.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 31315779072.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 31630392832.0,
        "latency_per_micro_batch": 0.16046159166566618,
        "latency_fwd": 0.05348719722188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 10.278429301881367,
        "device_tokens_per_sec": 398.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34081420032.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 34238747392.0,
        "latency_per_micro_batch": 0.7039595039856662,
        "latency_fwd": 0.23465316799522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 11.280524512661728,
        "device_tokens_per_sec": 726.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30732913664.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 31362100224.0,
        "latency_per_micro_batch": 0.055197205730624116,
        "latency_fwd": 0.018399068576874705,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.069131560962533,
        "device_tokens_per_sec": 579.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 18,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38935924224.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 39250537984.0,
        "latency_per_micro_batch": 0.03086701614293181,
        "latency_fwd": 0.01028900538097727,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0008388608,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.903129334094548,
        "device_tokens_per_sec": 1036.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43456661504.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 44714993664.0,
        "latency_per_micro_batch": 0.13085762481385294,
        "latency_fwd": 0.04361920827128431,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.014746079999999998,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 8.391923400739849,
        "device_tokens_per_sec": 244.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50441407488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 51070594048.0,
        "latency_per_micro_batch": 0.2704070309992279,
        "latency_fwd": 0.09013567699974263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 8.661542726202311,
        "device_tokens_per_sec": 945.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 39865561856.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 40022889216.0,
        "latency_per_micro_batch": 0.3937925903207856,
        "latency_fwd": 0.13126419677359522,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 12.618535339156209,
        "device_tokens_per_sec": 324.6,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29295915776.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.3937925903207856,
        "latency_fwd": 0.13126419677359522,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 12.618535339156209,
        "device_tokens_per_sec": 324.6,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 49946484224.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 50261097984.0,
        "latency_per_micro_batch": 0.3203874959028543,
        "latency_fwd": 0.1067958319676181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 10.26128730417007,
        "device_tokens_per_sec": 798.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11128153856.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 11285481216.0,
        "latency_per_micro_batch": 0.08901416689812652,
        "latency_fwd": 0.029671388966042177,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 11.398257141980412,
        "device_tokens_per_sec": 359.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 21697799936.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 21855127296.0,
        "latency_per_micro_batch": 0.09165403454566617,
        "latency_fwd": 0.03055134484855539,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 11.736160200865488,
        "device_tokens_per_sec": 349.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 18,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 74522820608.0,
        "activation_memory_attn_per_gpu": 22209363968.0,
        "activation_memory_mlp_per_gpu": 22145925120.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 76197334528.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 76511948288.0,
        "latency_per_micro_batch": 0.0611983448573856,
        "latency_fwd": 0.0203994482857952,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.834561343249362,
        "device_tokens_per_sec": 2091.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55914728448.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 56543915008.0,
        "latency_per_micro_batch": 0.10985872403277022,
        "latency_fwd": 0.036619574677590076,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 7.03484756553994,
        "device_tokens_per_sec": 1164.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 18,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 20305219072.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 20619832832.0,
        "latency_per_micro_batch": 0.015701351785704906,
        "latency_fwd": 0.005233783928568303,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0004194304,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 8.040265315784918,
        "device_tokens_per_sec": 509.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 58335274496.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.4063497103207857,
        "latency_fwd": 0.1354499034402619,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.1340910208,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 13.03753513700047,
        "device_tokens_per_sec": 157.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15939914752.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 16569101312.0,
        "latency_per_micro_batch": 0.014305924604014541,
        "latency_fwd": 0.004768641534671513,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.00013981013333333334,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 7.326979789103301,
        "device_tokens_per_sec": 279.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31540587264.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 31697914624.0,
        "latency_per_micro_batch": 0.7054275103856662,
        "latency_fwd": 0.23514250346188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 11.295526835147827,
        "device_tokens_per_sec": 1450.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 50551737344.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 51180923904.0,
        "latency_per_micro_batch": 0.29807670590566626,
        "latency_fwd": 0.09935890196855542,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.556229336777081,
        "device_tokens_per_sec": 428.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49386190848.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 50644523008.0,
        "latency_per_micro_batch": 0.10511587610600871,
        "latency_fwd": 0.03503862536866957,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0036701354666666664,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 6.735194503349541,
        "device_tokens_per_sec": 608.15,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 34112042496.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34426656256.0,
        "latency_per_micro_batch": 0.19711171007463182,
        "latency_fwd": 0.06570390335821061,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 12.632522537207366,
        "device_tokens_per_sec": 162.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 956856320.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1913712640.0,
        "optimizer_state_memory_per_gpu": 5741137920.0,
        "(weight+op_state)_memory_per_gpu": 6697994240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44982028288.0,
        "(weight+op_state+grad)_memory_per_gpu": 8611706880.0,
        "estimated_peak_memory_per_gpu": 46240360448.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0015729152,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 6.766392838742708,
        "device_tokens_per_sec": 605.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 327690240.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 655380480.0,
        "optimizer_state_memory_per_gpu": 1966141440.0,
        "(weight+op_state)_memory_per_gpu": 2293831680.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 40577865728.0,
        "(weight+op_state+grad)_memory_per_gpu": 2949212160.0,
        "estimated_peak_memory_per_gpu": 41836197888.0,
        "latency_per_micro_batch": 0.0009389858900164734,
        "latency_fwd": 0.0003129952966721578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 0.24198750035034766,
        "device_tokens_per_sec": 16926.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 6460018176.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 6774631936.0,
        "latency_per_micro_batch": 0.008015667620823681,
        "latency_fwd": 0.0026718892069412274,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0007571256451282051,
        "latency_fwd_mlp": 0.001376592082051282,
        "latency_fwd_layernorm": 1.142799847419759e-05,
        "latency_fwd_tp_comm": 0.0002097152,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 8.209216845227456,
        "device_tokens_per_sec": 249.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41633041408.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 42262227968.0,
        "latency_per_micro_batch": 0.05534236416724337,
        "latency_fwd": 0.018447454722414457,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.086169005255007,
        "device_tokens_per_sec": 1156.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 58590930432.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.6293008927856663,
        "latency_fwd": 0.20976696426188873,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.1340910208,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 10.103158691305987,
        "device_tokens_per_sec": 405.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14791322368.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 16098233600.0,
        "latency_per_micro_batch": 0.12282375635155494,
        "latency_fwd": 0.04094125211718498,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 15.7301274819762,
        "device_tokens_per_sec": 130.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 128,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 327690240.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 655380480.0,
        "optimizer_state_memory_per_gpu": 1966141440.0,
        "(weight+op_state)_memory_per_gpu": 2293831680.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 40577865728.0,
        "(weight+op_state+grad)_memory_per_gpu": 2949212160.0,
        "estimated_peak_memory_per_gpu": 41836197888.0,
        "latency_per_micro_batch": 0.0007373366592472428,
        "latency_fwd": 0.0002457788864157476,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 0.3791234820407188,
        "device_tokens_per_sec": 10803.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 20076145408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 20233472768.0,
        "latency_per_micro_batch": 0.12282375635155494,
        "latency_fwd": 0.04094125211718498,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 15.7301274819762,
        "device_tokens_per_sec": 130.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25484666624.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 25641993984.0,
        "latency_per_micro_batch": 0.3544496053070721,
        "latency_fwd": 0.1181498684356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 11.346831148846524,
        "device_tokens_per_sec": 1443.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 9065380608.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 9222707968.0,
        "latency_per_micro_batch": 0.00914694518878183,
        "latency_fwd": 0.0030489817295939435,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0007571256451282051,
        "latency_fwd_mlp": 0.001376592082051282,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0004893354666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 9.367444196913489,
        "device_tokens_per_sec": 218.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 70,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 3780557568.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 3937884928.0,
        "latency_per_micro_batch": 0.00890695722082368,
        "latency_fwd": 0.0029689857402745603,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0007571256451282051,
        "latency_fwd_mlp": 0.001376592082051282,
        "latency_fwd_layernorm": 1.142799847419759e-05,
        "latency_fwd_tp_comm": 0.0004893354666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 9.121696517724343,
        "device_tokens_per_sec": 224.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25220754944.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 25535368704.0,
        "latency_per_micro_batch": 0.14469882801385295,
        "latency_fwd": 0.048232942671284314,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.004194304,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 9.264983887900485,
        "device_tokens_per_sec": 884.19,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 59163937792.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.2963584949070721,
        "latency_fwd": 0.09878616496902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0670455104,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 9.518217776364763,
        "device_tokens_per_sec": 215.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15122107392.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 15751293952.0,
        "latency_per_micro_batch": 0.02772931059786069,
        "latency_fwd": 0.00924310353262023,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.102592740494983,
        "device_tokens_per_sec": 288.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 40036980224.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 40351593984.0,
        "latency_per_micro_batch": 0.0599400536573856,
        "latency_fwd": 0.0199800178857952,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0016777216,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.6742714930268345,
        "device_tokens_per_sec": 1067.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 64460021504.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 64617348864.0,
        "latency_per_micro_batch": 0.1679535249730159,
        "latency_fwd": 0.05598450832433864,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0018284797558716145,
        "latency_fwd_tp_comm": 0.009786709333333334,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 10.751155073680353,
        "device_tokens_per_sec": 1523.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24748280832.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 25377467392.0,
        "latency_per_micro_batch": 0.06834636327539141,
        "latency_fwd": 0.022782121091797138,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 8.756852233477119,
        "device_tokens_per_sec": 233.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 26328730624.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 26957917184.0,
        "latency_per_micro_batch": 0.001156767059247243,
        "latency_fwd": 0.00038558901974908094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 0.5930682905876536,
        "device_tokens_per_sec": 6906.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 79917075456.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 80546262016.0,
        "latency_per_micro_batch": 0.11014904090600874,
        "latency_fwd": 0.03671634696866958,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.0007378030094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 7.051885009832415,
        "device_tokens_per_sec": 2323.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34367698432.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34682312192.0,
        "latency_per_micro_batch": 0.3152328629070721,
        "latency_fwd": 0.10507762096902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 10.104824705457236,
        "device_tokens_per_sec": 405.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33749720832.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 33907048192.0,
        "latency_per_micro_batch": 0.08424460620074697,
        "latency_fwd": 0.02808153540024899,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.004893354666666667,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 10.785439069102948,
        "device_tokens_per_sec": 759.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12610428672.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 12767756032.0,
        "latency_per_micro_batch": 0.08184472652116548,
        "latency_fwd": 0.02728157550705516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.004893354666666667,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 10.478254470116516,
        "device_tokens_per_sec": 781.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 37339188224.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 37968374784.0,
        "latency_per_micro_batch": 0.1361570391223048,
        "latency_fwd": 0.045385679707434935,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 8.722568238054526,
        "device_tokens_per_sec": 469.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 13737823232.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 14367009792.0,
        "latency_per_micro_batch": 0.0008462272438626275,
        "latency_fwd": 0.00028207574795420916,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 0.8673402539683959,
        "device_tokens_per_sec": 2361.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29582194176.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 32195975680.0,
        "latency_per_micro_batch": 0.19711171007463182,
        "latency_fwd": 0.06570390335821061,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 12.632522537207366,
        "device_tokens_per_sec": 162.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24693115904.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 25322302464.0,
        "latency_per_micro_batch": 0.054922933767243366,
        "latency_fwd": 0.018307644589081122,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.034024749649797,
        "device_tokens_per_sec": 582.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68734946304.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 69364132864.0,
        "latency_per_micro_batch": 0.5921813023856661,
        "latency_fwd": 0.19739376746188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0670455104,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 9.509646777509113,
        "device_tokens_per_sec": 430.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 73819171584.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 73976498944.0,
        "latency_per_micro_batch": 0.7265464515659833,
        "latency_fwd": 0.24218215052199443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 11.6334298940329,
        "device_tokens_per_sec": 1408.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 70,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 6173309696.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 6330637056.0,
        "latency_per_micro_batch": 0.01727822701316935,
        "latency_fwd": 0.005759409004389784,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 2.285599694839518e-05,
        "latency_fwd_tp_comm": 0.0009786709333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 8.847424554343602,
        "device_tokens_per_sec": 462.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 198288640.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 396505600.0,
        "optimizer_state_memory_per_gpu": 1189516800.0,
        "(weight+op_state)_memory_per_gpu": 1387805440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 16742955776.0,
        "(weight+op_state+grad)_memory_per_gpu": 1784311040.0,
        "estimated_peak_memory_per_gpu": 16900283136.0,
        "latency_per_micro_batch": 0.01775820294908565,
        "latency_fwd": 0.00591940098302855,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0009786709333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 9.093172233532748,
        "device_tokens_per_sec": 450.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 53805426176.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.4063497103207857,
        "latency_fwd": 0.1354499034402619,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.1340910208,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 13.03753513700047,
        "device_tokens_per_sec": 157.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 27753720576.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 27911047936.0,
        "latency_per_micro_batch": 0.18203837846285434,
        "latency_fwd": 0.06067945948761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 11.659142890599846,
        "device_tokens_per_sec": 351.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17184074496.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 17341401856.0,
        "latency_per_micro_batch": 0.17675864316777506,
        "latency_fwd": 0.058919547722591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 11.321239831714772,
        "device_tokens_per_sec": 361.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 128,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 327690240.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 655380480.0,
        "optimizer_state_memory_per_gpu": 1966141440.0,
        "(weight+op_state)_memory_per_gpu": 2293831680.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21435848704.0,
        "(weight+op_state+grad)_memory_per_gpu": 2949212160.0,
        "estimated_peak_memory_per_gpu": 22694180864.0,
        "latency_per_micro_batch": 0.0006365120438626274,
        "latency_fwd": 0.00021217068128754246,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 0.6533954454214609,
        "device_tokens_per_sec": 3134.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11245522432.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 11560136192.0,
        "latency_per_micro_batch": 0.01549564781316935,
        "latency_fwd": 0.005165215937723117,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 2.285599694839518e-05,
        "latency_fwd_tp_comm": 0.0004194304,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 7.934944881846713,
        "device_tokens_per_sec": 516.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63653956608.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 64283143168.0,
        "latency_per_micro_batch": 0.5926007327856662,
        "latency_fwd": 0.19753357759522205,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 9.49938647236642,
        "device_tokens_per_sec": 862.37,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31827090944.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 32141704704.0,
        "latency_per_micro_batch": 0.3158620085070721,
        "latency_fwd": 0.10528733616902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 10.116471707505038,
        "device_tokens_per_sec": 809.77,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 956856320.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1913712640.0,
        "optimizer_state_memory_per_gpu": 5741137920.0,
        "(weight+op_state)_memory_per_gpu": 6697994240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25840011264.0,
        "(weight+op_state+grad)_memory_per_gpu": 8611706880.0,
        "estimated_peak_memory_per_gpu": 27098343424.0,
        "latency_per_micro_batch": 0.02668073459786069,
        "latency_fwd": 0.008893578199286897,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0015729152,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 6.834960829587893,
        "device_tokens_per_sec": 299.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 327690240.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 655380480.0,
        "optimizer_state_memory_per_gpu": 1966141440.0,
        "(weight+op_state)_memory_per_gpu": 2293831680.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21435848704.0,
        "(weight+op_state+grad)_memory_per_gpu": 2949212160.0,
        "estimated_peak_memory_per_gpu": 22694180864.0,
        "latency_per_micro_batch": 0.0007373366592472428,
        "latency_fwd": 0.0002457788864157476,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 0.3791234820407188,
        "device_tokens_per_sec": 5401.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21917586944.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 22232200704.0,
        "latency_per_micro_batch": 0.05911723776724337,
        "latency_fwd": 0.019705745922414458,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0016777216,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.568951059088629,
        "device_tokens_per_sec": 1082.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30244173824.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 31502505984.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0036701354666666664,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 6.769478498772134,
        "device_tokens_per_sec": 302.53,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43427395072.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 43742008832.0,
        "latency_per_micro_batch": 0.3197583503028544,
        "latency_fwd": 0.10658611676761812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 10.24964030212227,
        "device_tokens_per_sec": 399.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37960829952.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 38590016512.0,
        "latency_per_micro_batch": 0.14930619666707212,
        "latency_fwd": 0.04976873222235737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 9.573371334488376,
        "device_tokens_per_sec": 213.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 28530822144.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 29160008704.0,
        "latency_per_micro_batch": 0.028076161779551065,
        "latency_fwd": 0.009358720593183689,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 7.189843807412928,
        "device_tokens_per_sec": 569.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43340148224.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 43654761984.0,
        "latency_per_micro_batch": 0.1467558677392085,
        "latency_fwd": 0.04891862257973617,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.004194304,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 9.39663443032324,
        "device_tokens_per_sec": 871.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22181437184.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 22338764544.0,
        "latency_per_micro_batch": 0.16315376561385295,
        "latency_fwd": 0.05438458853795098,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.009786709333333334,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 10.443970474693923,
        "device_tokens_per_sec": 1568.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 77714983936.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 78344170496.0,
        "latency_per_micro_batch": 0.0030200059515549356,
        "latency_fwd": 0.0010066686505183118,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0007378030094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 0.387364318052097,
        "device_tokens_per_sec": 42296.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 67650627072.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 67965240832.0,
        "latency_per_micro_batch": 0.6383518675772306,
        "latency_fwd": 0.2127839558590769,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.1340910208,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 10.247974287971017,
        "device_tokens_per_sec": 399.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43652428544.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 43809755904.0,
        "latency_per_micro_batch": 1.4073833205428543,
        "latency_fwd": 0.4691277735142848,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 11.276239013233903,
        "device_tokens_per_sec": 1452.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 18,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 10989866496.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 11304480256.0,
        "latency_per_micro_batch": 0.008118519607091462,
        "latency_fwd": 0.0027061732023638202,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0007571256451282051,
        "latency_fwd_mlp": 0.001376592082051282,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0002097152,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 3.3608205128205126e-05,
        "latency_per_iter": 8.314537279165663,
        "device_tokens_per_sec": 246.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 26903163648.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.3918123948746318,
        "latency_fwd": 0.13060413162487727,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 25.093165720867503,
        "device_tokens_per_sec": 81.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62598678528.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 63857010688.0,
        "latency_per_micro_batch": 0.2611795621992279,
        "latency_fwd": 0.08705985406640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.014746079999999998,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 8.374781403028553,
        "device_tokens_per_sec": 489.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 163845120.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 327690240.0,
        "optimizer_state_memory_per_gpu": 983070720.0,
        "(weight+op_state)_memory_per_gpu": 1146915840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 39430949888.0,
        "(weight+op_state+grad)_memory_per_gpu": 1474606080.0,
        "estimated_peak_memory_per_gpu": 40060136448.0,
        "latency_per_micro_batch": 0.0017778466900164735,
        "latency_fwd": 0.0005926155633388245,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 0.4559323088972824,
        "device_tokens_per_sec": 17967.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 67763250944.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 67920578304.0,
        "latency_per_micro_batch": 0.36500907589723064,
        "latency_fwd": 0.12166969196574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 11.684734207731598,
        "device_tokens_per_sec": 1402.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 32187986688.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 32345314048.0,
        "latency_per_micro_batch": 0.3918123948746318,
        "latency_fwd": 0.13060413162487727,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 25.093165720867503,
        "device_tokens_per_sec": 81.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14020224768.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 14177552128.0,
        "latency_per_micro_batch": 0.05321218909001648,
        "latency_fwd": 0.017737396363338827,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.004164191048205128,
        "latency_fwd_mlp": 0.007571256451282051,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.002691345066666667,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 13.626764186064436,
        "device_tokens_per_sec": 150.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 239229440.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 52428800.0,
        "weight_memory_mlp_per_gpu": 104857600.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 478428160.0,
        "optimizer_state_memory_per_gpu": 1435284480.0,
        "(weight+op_state)_memory_per_gpu": 1674513920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 39958547968.0,
        "(weight+op_state+grad)_memory_per_gpu": 2152942080.0,
        "estimated_peak_memory_per_gpu": 40273161728.0,
        "latency_per_micro_batch": 0.06037552896724337,
        "latency_fwd": 0.020125176322414458,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 7.729240909311157,
        "device_tokens_per_sec": 2119.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 8735401728.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 8892729088.0,
        "latency_per_micro_batch": 0.05321218909001648,
        "latency_fwd": 0.017737396363338827,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.004164191048205128,
        "latency_fwd_mlp": 0.007571256451282051,
        "latency_fwd_layernorm": 6.285399160808674e-05,
        "latency_fwd_tp_comm": 0.002691345066666667,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 13.626764186064436,
        "device_tokens_per_sec": 150.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 9.11606808576,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 3,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7248517120.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14497034240.0,
        "optimizer_state_memory_per_gpu": 43491102720.0,
        "(weight+op_state)_memory_per_gpu": 50739619840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 69881636864.0,
        "(weight+op_state+grad)_memory_per_gpu": 65236654080.0,
        "estimated_peak_memory_per_gpu": 71139969024.0,
        "latency_per_micro_batch": 0.28692131090707207,
        "latency_fwd": 0.09564043696902402,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 146.93926055725072,
        "device_tokens_per_sec": 13.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63653956608.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 64283143168.0,
        "latency_per_micro_batch": 0.5926007327856662,
        "latency_fwd": 0.19753357759522205,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 151.7235623409263,
        "device_tokens_per_sec": 53.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68161938944.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 68476552704.0,
        "latency_per_micro_batch": 1.2580660981428544,
        "latency_fwd": 0.41935536604761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 161.0668049690207,
        "device_tokens_per_sec": 50.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 59163937792.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.2963584949070721,
        "latency_fwd": 0.09878616496902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 151.77029533175937,
        "device_tokens_per_sec": 13.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 62058100224.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 62372713984.0,
        "latency_per_micro_batch": 0.6389810131772307,
        "latency_fwd": 0.21299367105907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 163.596512465802,
        "device_tokens_per_sec": 50.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37960829952.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 38590016512.0,
        "latency_per_micro_batch": 0.14930619666707212,
        "latency_fwd": 0.04976873222235737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 152.9073201348776,
        "device_tokens_per_sec": 13.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62794445568.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 63056589568.0,
        "latency_per_micro_batch": 2.8142309536572307,
        "latency_fwd": 0.9380769845524102,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.26650822708512817,
        "latency_fwd_mlp": 0.48456041288205126,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.17224608426666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.008603700512820512,
        "latency_per_iter": 180.12795348295384,
        "device_tokens_per_sec": 181.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34081420032.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 34238747392.0,
        "latency_per_micro_batch": 0.7039595039856662,
        "latency_fwd": 0.23465316799522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 180.23080546922162,
        "device_tokens_per_sec": 45.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 50551737344.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 51180923904.0,
        "latency_per_micro_batch": 0.29807670590566626,
        "latency_fwd": 0.09935890196855542,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 152.63304817149688,
        "device_tokens_per_sec": 26.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 55220712192.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 55378039552.0,
        "latency_per_micro_batch": 0.7250784451659833,
        "latency_fwd": 0.24169281505532778,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 185.6372544113828,
        "device_tokens_per_sec": 44.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68734946304.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 69364132864.0,
        "latency_per_micro_batch": 0.5921813023856661,
        "latency_fwd": 0.19739376746188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 151.63315935006898,
        "device_tokens_per_sec": 27.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43938706944.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 44253320704.0,
        "latency_per_micro_batch": 0.6299300383856662,
        "latency_fwd": 0.20997667946188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 161.2794629191615,
        "device_tokens_per_sec": 50.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43652428544.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 43809755904.0,
        "latency_per_micro_batch": 1.4073833205428543,
        "latency_fwd": 0.4691277735142848,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 180.16223747837643,
        "device_tokens_per_sec": 90.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34940931072.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 35570117632.0,
        "latency_per_micro_batch": 0.14855194876777505,
        "latency_fwd": 0.049517316255925016,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 152.1349702859974,
        "device_tokens_per_sec": 13.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34367698432.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34682312192.0,
        "latency_per_micro_batch": 0.3152328629070721,
        "latency_fwd": 0.10507762096902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 161.41659890085185,
        "device_tokens_per_sec": 25.38,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 53805426176.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.31491829010707206,
        "latency_fwd": 0.10497276336902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 161.27250894155623,
        "device_tokens_per_sec": 12.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43456661504.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 44714993664.0,
        "latency_per_micro_batch": 0.13085762481385294,
        "latency_fwd": 0.04361920827128431,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 134.01524322203866,
        "device_tokens_per_sec": 15.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 62183836672.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.29786699070566625,
        "latency_fwd": 0.09928899690188875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 152.54264518063957,
        "device_tokens_per_sec": 13.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75733552128.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 76362738688.0,
        "latency_per_micro_batch": 0.5956177243828544,
        "latency_fwd": 0.19853924146095148,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 152.4959121898065,
        "device_tokens_per_sec": 53.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29295915776.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.3522475957070721,
        "latency_fwd": 0.1174158652356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 180.36794145091199,
        "device_tokens_per_sec": 22.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 39865561856.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 40022889216.0,
        "latency_per_micro_batch": 0.36280706629723064,
        "latency_fwd": 0.12093568876574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 185.77439039307316,
        "device_tokens_per_sec": 22.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 34112042496.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34426656256.0,
        "latency_per_micro_batch": 0.16014701886566618,
        "latency_fwd": 0.05338233962188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 164.0079204108731,
        "device_tokens_per_sec": 12.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 67650627072.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 67965240832.0,
        "latency_per_micro_batch": 0.6383518675772306,
        "latency_fwd": 0.2127839558590769,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 163.45242250650637,
        "device_tokens_per_sec": 25.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43427395072.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 43742008832.0,
        "latency_per_micro_batch": 0.3197583503028544,
        "latency_fwd": 0.10658611676761812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 163.73364844749239,
        "device_tokens_per_sec": 25.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 58335274496.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.31944377750285435,
        "latency_fwd": 0.10648125916761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 163.58955848819676,
        "device_tokens_per_sec": 12.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 32187986688.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 32345314048.0,
        "latency_per_micro_batch": 0.18167137686285434,
        "latency_fwd": 0.060557125620951446,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 186.04866235645392,
        "device_tokens_per_sec": 11.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 26903163648.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.17639164156777506,
        "latency_fwd": 0.05879721385592502,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 180.64221341429274,
        "device_tokens_per_sec": 11.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63080723968.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 63395337728.0,
        "latency_per_micro_batch": 1.2593243893428543,
        "latency_fwd": 0.4197747964476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 161.2108949283163,
        "device_tokens_per_sec": 101.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 74774744064.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 75403930624.0,
        "latency_per_micro_batch": 0.5951982939828544,
        "latency_fwd": 0.19839943132761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 152.40550919894918,
        "device_tokens_per_sec": 26.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62598678528.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 63857010688.0,
        "latency_per_micro_batch": 0.2611795621992279,
        "latency_fwd": 0.08705985406640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 133.74097125865794,
        "device_tokens_per_sec": 30.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29582194176.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 32195975680.0,
        "latency_per_micro_batch": 0.15788427516777503,
        "latency_fwd": 0.05262809172259168,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 161.69087086423258,
        "device_tokens_per_sec": 12.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 58590930432.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.6293008927856663,
        "latency_fwd": 0.20976696426188873,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 161.1353729598659,
        "device_tokens_per_sec": 25.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44511939584.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 45141126144.0,
        "latency_per_micro_batch": 0.2965682101070721,
        "latency_fwd": 0.0988560700356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 151.86069832261668,
        "device_tokens_per_sec": 26.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 145.85708937216,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 35517882368.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 36323221504.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 34.83546862959345,
        "device_tokens_per_sec": 117.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33895495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34096855040.0,
        "latency_per_micro_batch": 0.3096333834668679,
        "latency_fwd": 0.1032111278222893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 39.649193428450936,
        "device_tokens_per_sec": 103.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14534719488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 15287502848.0,
        "latency_per_micro_batch": 0.07653908201957621,
        "latency_fwd": 0.02551302733985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 39.196230877619556,
        "device_tokens_per_sec": 52.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45875228672.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 46277914624.0,
        "latency_per_micro_batch": 0.5650209563171329,
        "latency_fwd": 0.1883403187723776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 36.17778282863959,
        "device_tokens_per_sec": 226.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 51618803712.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2825569809699576,
        "latency_fwd": 0.09418566032331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.1995339678383,
        "device_tokens_per_sec": 56.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23396380672.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 23799066624.0,
        "latency_per_micro_batch": 0.14174442369637003,
        "latency_fwd": 0.04724814123212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 36.29511470094349,
        "device_tokens_per_sec": 112.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 51371077632.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 51895365632.0,
        "latency_per_micro_batch": 0.2837313860899576,
        "latency_fwd": 0.09457712869665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.322209959352186,
        "device_tokens_per_sec": 451.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 50912376832.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 51436664832.0,
        "latency_per_micro_batch": 0.6113260724771329,
        "latency_fwd": 0.20377535749237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 39.12913979158539,
        "device_tokens_per_sec": 837.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 47371131904.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 47502203904.0,
        "latency_per_micro_batch": 0.7201224647160474,
        "latency_fwd": 0.24004082157201578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 46.095898057022936,
        "device_tokens_per_sec": 355.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17300273152.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17501632512.0,
        "latency_per_micro_batch": 0.15315293057637003,
        "latency_fwd": 0.05105097685879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 39.21142138059961,
        "device_tokens_per_sec": 208.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 25809664000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.15239795585637003,
        "latency_fwd": 0.05079931861879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 39.02999704392257,
        "device_tokens_per_sec": 52.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 67824793600.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 68349081600.0,
        "latency_per_micro_batch": 2.4319390445601843,
        "latency_fwd": 0.8106463481867281,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.08589934592,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 38.92714505765479,
        "device_tokens_per_sec": 841.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2690367514214400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56320728064.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 56845016064.0,
        "latency_per_micro_batch": 2.7809051373601843,
        "latency_fwd": 0.9269683791200614,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.20043180714666667,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0032745969675941366,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 44.502542512958854,
        "device_tokens_per_sec": 1472.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14833622016.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 14934318080.0,
        "latency_per_micro_batch": 0.09068389056944053,
        "latency_fwd": 0.030227963523146842,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 46.43426248477681,
        "device_tokens_per_sec": 88.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75471286272.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 75873972224.0,
        "latency_per_micro_batch": 1.1359636363140302,
        "latency_fwd": 0.3786545454380101,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 36.38307676573269,
        "device_tokens_per_sec": 225.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8667995136.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 8768691200.0,
        "latency_per_micro_batch": 0.0876120445795762,
        "latency_fwd": 0.029204014859858737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 44.86147733796628,
        "device_tokens_per_sec": 91.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 15987907584.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 16088603648.0,
        "latency_per_micro_batch": 0.09039028928944053,
        "latency_fwd": 0.03013009642981351,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 46.287888431389455,
        "device_tokens_per_sec": 44.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 12905094144.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.08731844329957622,
        "latency_fwd": 0.029106147766525405,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 44.71510328457893,
        "device_tokens_per_sec": 45.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 29880508416.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 30685847552.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 34.941227737966834,
        "device_tokens_per_sec": 117.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 30830784512.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 31233470464.0,
        "latency_per_micro_batch": 0.14245432181918838,
        "latency_fwd": 0.04748477393972946,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 36.48474801005531,
        "device_tokens_per_sec": 56.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22620551168.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 22821910528.0,
        "latency_per_micro_batch": 0.15528262494482514,
        "latency_fwd": 0.05176087498160838,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 39.76057286947177,
        "device_tokens_per_sec": 103.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 41733528576.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 41864600576.0,
        "latency_per_micro_batch": 0.3614499124494149,
        "latency_fwd": 0.12048330414980496,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 46.269699306748365,
        "device_tokens_per_sec": 354.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34141779968.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 34403923968.0,
        "latency_per_micro_batch": 0.6093128065571328,
        "latency_fwd": 0.20310426885237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 39.00424050325304,
        "device_tokens_per_sec": 420.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 71101345792.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 71625633792.0,
        "latency_per_micro_batch": 0.5692027260884064,
        "latency_fwd": 0.18973424202946879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 36.437516704330775,
        "device_tokens_per_sec": 449.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 69200699392.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 70006038528.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 34.76108639453705,
        "device_tokens_per_sec": 235.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21282189312.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 21684875264.0,
        "latency_per_micro_batch": 0.0716930941209854,
        "latency_fwd": 0.023897698040328468,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 36.711456729782135,
        "device_tokens_per_sec": 111.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2690367514214400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 50683124736.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 51207412736.0,
        "latency_per_micro_batch": 1.3953644641314833,
        "latency_fwd": 0.46512148804382775,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0032745969675941366,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 44.655773365430726,
        "device_tokens_per_sec": 1467.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55281072128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 55543216128.0,
        "latency_per_micro_batch": 0.6198448499509533,
        "latency_fwd": 0.20661494998365112,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 39.67829128045755,
        "device_tokens_per_sec": 412.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 13918564352.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 14321250304.0,
        "latency_per_micro_batch": 0.036060822031883905,
        "latency_fwd": 0.012020274010627968,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 36.93087430048673,
        "device_tokens_per_sec": 55.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18676473856.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 19481812992.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 35.160645308671434,
        "device_tokens_per_sec": 58.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17177131008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17378490368.0,
        "latency_per_micro_batch": 0.07785558744380378,
        "latency_fwd": 0.025951862481267927,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 39.87028165482407,
        "device_tokens_per_sec": 51.37,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38194409472.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 38597095424.0,
        "latency_per_micro_batch": 0.28448009369559435,
        "latency_fwd": 0.09482669789853146,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.429893617379165,
        "device_tokens_per_sec": 112.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7267490816.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 7643898880.0,
        "latency_per_micro_batch": 0.044020297261179314,
        "latency_fwd": 0.014673432420393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 45.08089490867088,
        "device_tokens_per_sec": 45.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 10350304256.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 10451000320.0,
        "latency_per_micro_batch": 0.045556220256111474,
        "latency_fwd": 0.015185406752037157,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 46.65368005548141,
        "device_tokens_per_sec": 43.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41646845952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 42049531904.0,
        "latency_per_micro_batch": 0.28481563801559434,
        "latency_fwd": 0.09493854600519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.46494390066884,
        "device_tokens_per_sec": 224.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14305598464.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.17420833665637,
        "latency_fwd": 0.05806944555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 44.60539449922663,
        "device_tokens_per_sec": 91.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 20471225344.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 20571921408.0,
        "latency_per_micro_batch": 0.18035202863609864,
        "latency_fwd": 0.06011734287869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 46.17817964603716,
        "device_tokens_per_sec": 88.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 28610672640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.30436736176995766,
        "latency_fwd": 0.10145578725665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 38.97514265124642,
        "device_tokens_per_sec": 105.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17071021056.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 17202093056.0,
        "latency_per_micro_batch": 0.3491625284899576,
        "latency_fwd": 0.11638750949665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 44.69691415993783,
        "device_tokens_per_sec": 366.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 57220820992.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.5646854119971328,
        "latency_fwd": 0.18822847066571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 36.172106771500225,
        "device_tokens_per_sec": 113.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17794363392.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 18197049344.0,
        "latency_per_micro_batch": 0.07108648681957622,
        "latency_fwd": 0.023695495606525407,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 36.40482348629579,
        "device_tokens_per_sec": 56.26,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17335728128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17537087488.0,
        "latency_per_micro_batch": 0.15264961409637,
        "latency_fwd": 0.050883204698790006,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 39.08652209226726,
        "device_tokens_per_sec": 104.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22708624384.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 22839696384.0,
        "latency_per_micro_batch": 0.6955476967971328,
        "latency_fwd": 0.23184923226571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 44.523112910212404,
        "device_tokens_per_sec": 367.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 28452075520.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.15503096670482516,
        "latency_fwd": 0.051676988901608385,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 39.704047821127084,
        "device_tokens_per_sec": 51.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24313847808.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 25119186944.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 34.94517741494574,
        "device_tokens_per_sec": 58.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28963008512.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 29365694464.0,
        "latency_per_micro_batch": 0.14207996801637002,
        "latency_fwd": 0.04735998933879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 36.377064352028334,
        "device_tokens_per_sec": 225.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 27869919232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28071278592.0,
        "latency_per_micro_batch": 0.15578594142482516,
        "latency_fwd": 0.051928647141608386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 39.885472157804124,
        "device_tokens_per_sec": 205.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17758973952.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 18161659904.0,
        "latency_per_micro_batch": 0.07125425897957621,
        "latency_fwd": 0.02375141965985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 36.48677313738063,
        "device_tokens_per_sec": 112.26,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 16983078912.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17184438272.0,
        "latency_per_micro_batch": 0.07810724568380378,
        "latency_fwd": 0.02603574856126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 39.99518094315642,
        "device_tokens_per_sec": 102.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22937745408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 23139104768.0,
        "latency_per_micro_batch": 0.3048706782499576,
        "latency_fwd": 0.10162355941665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 39.03166769959111,
        "device_tokens_per_sec": 209.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 66556016640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 66818160640.0,
        "latency_per_micro_batch": 1.2372478840391246,
        "latency_fwd": 0.4124159613463748,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 39.60805263394383,
        "device_tokens_per_sec": 413.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 69342126080.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 70147465216.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 34.776885102452695,
        "device_tokens_per_sec": 117.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 68424855552.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 68827541504.0,
        "latency_per_micro_batch": 1.1289422740514832,
        "latency_fwd": 0.37631409135049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 36.158393173331184,
        "device_tokens_per_sec": 226.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29069176832.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 30574710784.0,
        "latency_per_micro_batch": 0.14157665153637,
        "latency_fwd": 0.047192217178790005,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 36.26006441765381,
        "device_tokens_per_sec": 56.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57008484352.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 57532772352.0,
        "latency_per_micro_batch": 0.5656920449571329,
        "latency_fwd": 0.18856401498571096,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 36.21283311192927,
        "device_tokens_per_sec": 452.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34212689920.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34414049280.0,
        "latency_per_micro_batch": 0.6083061735971329,
        "latency_fwd": 0.20276872453237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 38.94771545490835,
        "device_tokens_per_sec": 210.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8897247232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 9098606592.0,
        "latency_per_micro_batch": 0.03860964510117931,
        "latency_fwd": 0.012869881700393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 39.5405477366565,
        "device_tokens_per_sec": 51.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 52921659392.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 53324345344.0,
        "latency_per_micro_batch": 0.5685316374484063,
        "latency_fwd": 0.18951054581613544,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 36.40246642104109,
        "device_tokens_per_sec": 225.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28275055616.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 28537199616.0,
        "latency_per_micro_batch": 0.6978965070371328,
        "latency_fwd": 0.2326321690123776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 44.66948696359976,
        "device_tokens_per_sec": 733.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49643599872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 49905743872.0,
        "latency_per_micro_batch": 0.3111433329068679,
        "latency_fwd": 0.10371444430228931,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 39.830617765127975,
        "device_tokens_per_sec": 411.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17106607104.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 17207303168.0,
        "latency_per_micro_batch": 0.34798812336995766,
        "latency_fwd": 0.11599604112331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 44.550540106550486,
        "device_tokens_per_sec": 183.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 26919596032.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 27322281984.0,
        "latency_per_micro_batch": 0.1426220939791884,
        "latency_fwd": 0.0475406979930628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 36.519798293345,
        "device_tokens_per_sec": 112.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29437860864.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 29538556928.0,
        "latency_per_micro_batch": 0.3602755073294149,
        "latency_fwd": 0.12009183577647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 46.12332525336101,
        "device_tokens_per_sec": 177.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 53380411392.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2843123215355944,
        "latency_fwd": 0.09477077384519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.4242175602398,
        "device_tokens_per_sec": 56.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 68283297792.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 68807585792.0,
        "latency_per_micro_batch": 1.1296133626914833,
        "latency_fwd": 0.3765377875638278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 36.16406923047055,
        "device_tokens_per_sec": 453.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 35588595712.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 36393934848.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 34.843367983551275,
        "device_tokens_per_sec": 58.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 65463939072.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 65988227072.0,
        "latency_per_micro_batch": 0.2854867266555944,
        "latency_fwd": 0.09516224221853145,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.54689355175369,
        "device_tokens_per_sec": 448.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33912658944.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 34174802944.0,
        "latency_per_micro_batch": 1.3906668436514833,
        "latency_fwd": 0.46355561455049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 44.50939931204337,
        "device_tokens_per_sec": 736.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 36009439232.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 36412125184.0,
        "latency_per_micro_batch": 0.1429576382991884,
        "latency_fwd": 0.047652546099729465,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 36.60174794442984,
        "device_tokens_per_sec": 223.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46792630272.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 47597969408.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 34.78851359087512,
        "device_tokens_per_sec": 117.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28504307712.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28766451712.0,
        "latency_per_micro_batch": 0.30587731120995765,
        "latency_fwd": 0.10195910373665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 39.15656698792346,
        "device_tokens_per_sec": 418.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19555971072.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 19958657024.0,
        "latency_per_micro_batch": 0.0715253219609854,
        "latency_fwd": 0.023841773986995135,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 36.629507078697294,
        "device_tokens_per_sec": 55.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 23800257536.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 23900953600.0,
        "latency_per_micro_batch": 0.18093923119609864,
        "latency_fwd": 0.06031307706536621,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 46.32455369942451,
        "device_tokens_per_sec": 176.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11469003776.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 11569699840.0,
        "latency_per_micro_batch": 0.17479553921637,
        "latency_fwd": 0.05826517973879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 44.75176855261398,
        "device_tokens_per_sec": 183.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 60744036352.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 61146722304.0,
        "latency_per_micro_batch": 0.5681960931284062,
        "latency_fwd": 0.18939869770946877,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 36.39679036390172,
        "device_tokens_per_sec": 112.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 33507391488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 33708750848.0,
        "latency_per_micro_batch": 0.3101366999468679,
        "latency_fwd": 0.1033788999822893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 39.705718476795624,
        "device_tokens_per_sec": 206.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 52288577536.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 53093916672.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 34.831518952614545,
        "device_tokens_per_sec": 235.19,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11698255872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11899615232.0,
        "latency_per_micro_batch": 0.07679074025957622,
        "latency_fwd": 0.02559691341985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 39.32113016595191,
        "device_tokens_per_sec": 104.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34671194112.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 35073880064.0,
        "latency_per_micro_batch": 0.28272475312995765,
        "latency_fwd": 0.09424158437665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.20521002497767,
        "device_tokens_per_sec": 113.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45416724480.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 45678868480.0,
        "latency_per_micro_batch": 1.2161837972514835,
        "latency_fwd": 0.4053945990838278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 38.934001856739314,
        "device_tokens_per_sec": 420.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 83237673984.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 83499817984.0,
        "latency_per_micro_batch": 1.4398163794893124,
        "latency_fwd": 0.4799387931631041,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.01872363270012533,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 46.0821844588539,
        "device_tokens_per_sec": 711.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34600415232.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 35003101184.0,
        "latency_per_micro_batch": 0.28306029744995764,
        "latency_fwd": 0.0943534324833192,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 36.240260308267345,
        "device_tokens_per_sec": 226.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 58138091520.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 61149126656.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 34.80431229879076,
        "device_tokens_per_sec": 58.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56549849088.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 57074137088.0,
        "latency_per_micro_batch": 1.2181970631714834,
        "latency_fwd": 0.4060656877238278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 38.990526905084,
        "device_tokens_per_sec": 840.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11539658752.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11741018112.0,
        "latency_per_micro_batch": 0.039267897813293096,
        "latency_fwd": 0.013089299271097699,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 40.21459851386101,
        "device_tokens_per_sec": 50.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12156956672.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 12559642624.0,
        "latency_per_micro_batch": 0.03584140446117931,
        "latency_fwd": 0.011947134820393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 36.706190708085224,
        "device_tokens_per_sec": 55.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 77600070656.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 77862214656.0,
        "latency_per_micro_batch": 0.7224712749560473,
        "latency_fwd": 0.24082375831868244,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 46.24227211041029,
        "device_tokens_per_sec": 708.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57925951488.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 58731290624.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 34.780614236917295,
        "device_tokens_per_sec": 235.53,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44782336000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 44983695360.0,
        "latency_per_micro_batch": 0.6188382169909534,
        "latency_fwd": 0.20627940566365113,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 39.62176623211286,
        "device_tokens_per_sec": 206.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 34.491891207876925,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14252219392.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 14383291392.0,
        "latency_per_micro_batch": 0.17596994433637,
        "latency_fwd": 0.05865664811212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.633173831000775,
        "device_tokens_per_sec": 2908.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24734953472.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 25137639424.0,
        "latency_per_micro_batch": 0.03634472094117931,
        "latency_fwd": 0.012114906980393103,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.653754549182194,
        "device_tokens_per_sec": 1760.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 28610672640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.30436736176995766,
        "latency_fwd": 0.10145578725665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.885998133011165,
        "device_tokens_per_sec": 838.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 3964459008.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 4165818368.0,
        "latency_per_micro_batch": 0.005421387797582023,
        "latency_fwd": 0.0018071292658606742,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0004955731495384615,
        "latency_fwd_mlp": 0.0008810189325128206,
        "latency_fwd_layernorm": 9.142398779358071e-06,
        "latency_fwd_tp_comm": 0.00016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 5.552316243543674,
        "device_tokens_per_sec": 368.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 27061821440.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 27867160576.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00100667392,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.397686771205312,
        "device_tokens_per_sec": 931.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24243134464.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 25048473600.0,
        "latency_per_micro_batch": 0.000751188712013179,
        "latency_fwd": 0.0002503962373377263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 0.19359000028027817,
        "device_tokens_per_sec": 21158.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 15644782592.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 16047468544.0,
        "latency_per_micro_batch": 0.0009254136473977942,
        "latency_fwd": 0.00030847121579926474,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 0.4744546324701228,
        "device_tokens_per_sec": 8633.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23641815040.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 23843174400.0,
        "latency_per_micro_batch": 0.0400228725332931,
        "latency_fwd": 0.0133409575110977,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.124236539399656,
        "device_tokens_per_sec": 1598.68,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 37505326080.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 37636398080.0,
        "latency_per_micro_batch": 0.09244549824944052,
        "latency_fwd": 0.03081516608314684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.917660049707968,
        "device_tokens_per_sec": 2768.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17335728128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17537087488.0,
        "latency_per_micro_batch": 0.15264961409637,
        "latency_fwd": 0.050883204698790006,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.893008534680375,
        "device_tokens_per_sec": 837.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 9338253312.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 9740939264.0,
        "latency_per_micro_batch": 0.01821886328198086,
        "latency_fwd": 0.0060729544273269535,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 4.666646692607132,
        "device_tokens_per_sec": 438.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 62645235712.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 63169523712.0,
        "latency_per_micro_batch": 0.14362872693918838,
        "latency_fwd": 0.0478762423130628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.5987369544740595,
        "device_tokens_per_sec": 3562.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29069176832.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 30574710784.0,
        "latency_per_micro_batch": 0.14157665153637,
        "latency_fwd": 0.047192217178790005,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.546894473506928,
        "device_tokens_per_sec": 450.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 5849193472.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 5949889536.0,
        "latency_per_micro_batch": 0.04431389854117931,
        "latency_fwd": 0.014771299513726436,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.674314625507886,
        "device_tokens_per_sec": 721.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 12014820352.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 12115516416.0,
        "latency_per_micro_batch": 0.04584982153611147,
        "latency_fwd": 0.015283273845370489,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.870912768859203,
        "device_tokens_per_sec": 697.68,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28963008512.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 29365694464.0,
        "latency_per_micro_batch": 0.14207996801637002,
        "latency_fwd": 0.04735998933879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.551151516361451,
        "device_tokens_per_sec": 1799.98,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55281072128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 55543216128.0,
        "latency_per_micro_batch": 0.6198448499509533,
        "latency_fwd": 0.20661494998365112,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.966979683204161,
        "device_tokens_per_sec": 3298.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 27869919232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28071278592.0,
        "latency_per_micro_batch": 0.15578594142482516,
        "latency_fwd": 0.051928647141608386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.989421278643285,
        "device_tokens_per_sec": 1641.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 57220820992.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.5646854119971328,
        "latency_fwd": 0.18822847066571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.06342045696,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.5497236996607855,
        "device_tokens_per_sec": 900.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 119,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 2334587904.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 2435283968.0,
        "latency_per_micro_batch": 0.006134419477582022,
        "latency_fwd": 0.0020448064925273407,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0004955731495384615,
        "latency_fwd_mlp": 0.0008810189325128206,
        "latency_fwd_layernorm": 9.142398779358071e-06,
        "latency_fwd_tp_comm": 0.00039146837333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 6.282299981541184,
        "device_tokens_per_sec": 326.0,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 5417401344.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 5518097408.0,
        "latency_per_micro_batch": 0.006326409851948541,
        "latency_fwd": 0.002108803283982847,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0004955731495384615,
        "latency_fwd_mlp": 0.0008810189325128206,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00039146837333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 6.4788981248925,
        "device_tokens_per_sec": 316.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 36800625664.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 36931697664.0,
        "latency_per_micro_batch": 0.047611429216111466,
        "latency_fwd": 0.01587047640537049,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 6.0949173761594615,
        "device_tokens_per_sec": 2688.14,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19572055040.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 19672751104.0,
        "latency_per_micro_batch": 0.04643702409611147,
        "latency_fwd": 0.015479008032037157,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.945087246046043,
        "device_tokens_per_sec": 1377.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 52921659392.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 53324345344.0,
        "latency_per_micro_batch": 0.5685316374484063,
        "latency_fwd": 0.18951054581613544,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.564694723930338,
        "device_tokens_per_sec": 1794.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 28452075520.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.19050266745662858,
        "latency_fwd": 0.06350088915220953,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 6.112205703303957,
        "device_tokens_per_sec": 335.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 7240801280.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 7341497344.0,
        "latency_per_micro_batch": 0.044901101101179314,
        "latency_fwd": 0.014967033700393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.748489102694727,
        "device_tokens_per_sec": 1425.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 13918564352.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 14321250304.0,
        "latency_per_micro_batch": 0.036060822031883905,
        "latency_fwd": 0.012020274010627968,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.6203777599187505,
        "device_tokens_per_sec": 443.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 59826532352.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 60350820352.0,
        "latency_per_micro_batch": 0.002416004761243948,
        "latency_fwd": 0.0008053349204146493,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 0.30989145444167754,
        "device_tokens_per_sec": 52870.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12050290688.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 12251650048.0,
        "latency_per_micro_batch": 0.010578788830410081,
        "latency_fwd": 0.003526262943470027,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00033554432,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 5.417155019989645,
        "device_tokens_per_sec": 756.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57008484352.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 57532772352.0,
        "latency_per_micro_batch": 0.5656920449571329,
        "latency_fwd": 0.18856401498571096,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.5340785943298325,
        "device_tokens_per_sec": 3613.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33190735872.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 33593421824.0,
        "latency_per_micro_batch": 0.0720286384409854,
        "latency_fwd": 0.024009546146995134,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.6124505526430974,
        "device_tokens_per_sec": 1776.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14481537024.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 14682896384.0,
        "latency_per_micro_batch": 0.07729405673957622,
        "latency_fwd": 0.02576468557985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.949115919107931,
        "device_tokens_per_sec": 1655.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14305598464.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.18479816825662856,
        "latency_fwd": 0.06159938941887619,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.921601699408016,
        "device_tokens_per_sec": 691.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 128,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 46651203584.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 47456542720.0,
        "latency_per_micro_batch": 0.000751188712013179,
        "latency_fwd": 0.0002503962373377263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 0.19359000028027817,
        "device_tokens_per_sec": 42316.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24313847808.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 25119186944.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0075500544,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.376184090335604,
        "device_tokens_per_sec": 467.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 20471225344.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 20571921408.0,
        "latency_per_micro_batch": 0.18479816825662856,
        "latency_fwd": 0.06159938941887619,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.921601699408016,
        "device_tokens_per_sec": 691.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 58138091520.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 61149126656.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.03171022848,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.379311754955515,
        "device_tokens_per_sec": 467.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 69200699392.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 70006038528.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.015603445760000001,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.3600846474976125,
        "device_tokens_per_sec": 1878.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7267490816.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 7643898880.0,
        "latency_per_micro_batch": 0.057741168281243954,
        "latency_fwd": 0.019247056093747984,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 7.394980053222484,
        "device_tokens_per_sec": 276.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 10350304256.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 10451000320.0,
        "latency_per_micro_batch": 0.057741168281243954,
        "latency_fwd": 0.019247056093747984,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 7.394980053222484,
        "device_tokens_per_sec": 276.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 29880508416.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 30685847552.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0023489058133333335,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.372234413356693,
        "device_tokens_per_sec": 936.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17177131008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17378490368.0,
        "latency_per_micro_batch": 0.09257141605970548,
        "latency_fwd": 0.030857138686568496,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.932791511417685,
        "device_tokens_per_sec": 345.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17071021056.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 17202093056.0,
        "latency_per_micro_batch": 0.3491625284899576,
        "latency_fwd": 0.11638750949665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 5.590710969062579,
        "device_tokens_per_sec": 2930.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24276203520.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 24538347520.0,
        "latency_per_micro_batch": 0.07830068969957621,
        "latency_fwd": 0.02610022989985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.012552995911017,
        "device_tokens_per_sec": 3268.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 26919596032.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 27322281984.0,
        "latency_per_micro_batch": 0.1426220939791884,
        "latency_fwd": 0.0475406979930628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.572449242006798,
        "device_tokens_per_sec": 895.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 9690509312.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 10093195264.0,
        "latency_per_micro_batch": 0.009462447085057783,
        "latency_fwd": 0.0031541490283525943,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00011184810666666666,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 4.846403176260827,
        "device_tokens_per_sec": 422.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 4,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 66556016640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 66818160640.0,
        "latency_per_micro_batch": 1.2372478840391246,
        "latency_fwd": 0.4124159613463748,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 4.96511188084834,
        "device_tokens_per_sec": 3299.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 12842818560.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 12973890560.0,
        "latency_per_micro_batch": 0.0893736522595762,
        "latency_fwd": 0.029791217419858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.721061906356652,
        "device_tokens_per_sec": 2863.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 4,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45416724480.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 45678868480.0,
        "latency_per_micro_batch": 1.2161837972514835,
        "latency_fwd": 0.4053945990838278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 4.880855533697776,
        "device_tokens_per_sec": 3356.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17054134272.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 17456820224.0,
        "latency_per_micro_batch": 0.018496344227333157,
        "latency_fwd": 0.006165448075777719,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 4.73669439090853,
        "device_tokens_per_sec": 864.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17300273152.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17501632512.0,
        "latency_per_micro_batch": 0.15315293057637003,
        "latency_fwd": 0.05105097685879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.905164931492721,
        "device_tokens_per_sec": 1670.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 51371077632.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 51895365632.0,
        "latency_per_micro_batch": 0.2837313860899576,
        "latency_fwd": 0.09457712869665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.544294717276933,
        "device_tokens_per_sec": 3605.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19555971072.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 19958657024.0,
        "latency_per_micro_batch": 0.0715253219609854,
        "latency_fwd": 0.023841773986995135,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.586162840175835,
        "device_tokens_per_sec": 446.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14833622016.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 14934318080.0,
        "latency_per_micro_batch": 0.09068389056944053,
        "latency_fwd": 0.030227963523146842,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.807879509667451,
        "device_tokens_per_sec": 705.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13072168960.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 13273528320.0,
        "latency_per_micro_batch": 0.039364619821179316,
        "latency_fwd": 0.013121539940393105,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.039980192249092,
        "device_tokens_per_sec": 1625.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8667995136.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 8768691200.0,
        "latency_per_micro_batch": 0.0876120445795762,
        "latency_fwd": 0.029204014859858737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.611281366316135,
        "device_tokens_per_sec": 729.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18676473856.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 19481812992.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0023489058133333335,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.399661609694768,
        "device_tokens_per_sec": 465.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45415495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 45677639680.0,
        "latency_per_micro_batch": 0.07961719512380377,
        "latency_fwd": 0.026539065041267927,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.096809343061581,
        "device_tokens_per_sec": 3214.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22620551168.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 22821910528.0,
        "latency_per_micro_batch": 0.15528262494482514,
        "latency_fwd": 0.05176087498160838,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.977264881830939,
        "device_tokens_per_sec": 822.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 35517882368.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 36323221504.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0075500544,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.362470492166568,
        "device_tokens_per_sec": 938.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38914726912.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 39045798912.0,
        "latency_per_micro_batch": 0.18211363631609864,
        "latency_fwd": 0.06070454543869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.8297719743520915,
        "device_tokens_per_sec": 2810.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 15987907584.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 16088603648.0,
        "latency_per_micro_batch": 0.18321401189970546,
        "latency_fwd": 0.06107133729990182,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.733757076777051,
        "device_tokens_per_sec": 174.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 23325601792.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 23728287744.0,
        "latency_per_micro_batch": 0.0014222773520131788,
        "latency_fwd": 0.0004740924506710596,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 0.364745847117826,
        "device_tokens_per_sec": 22459.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 12905094144.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.18321401189970546,
        "latency_fwd": 0.06107133729990182,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.733757076777051,
        "device_tokens_per_sec": 174.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 69342126080.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 70147465216.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.03171022848,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.375883355413257,
        "device_tokens_per_sec": 936.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 6606870528.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 6808229888.0,
        "latency_per_micro_batch": 0.0055036693865962456,
        "latency_fwd": 0.0018345564621987484,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0004955731495384615,
        "latency_fwd_mlp": 0.0008810189325128206,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 5.6365725906942385,
        "device_tokens_per_sec": 363.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33895495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34096855040.0,
        "latency_per_micro_batch": 0.3096333834668679,
        "latency_fwd": 0.1032111278222893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.970254480161729,
        "device_tokens_per_sec": 824.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21282189312.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 21684875264.0,
        "latency_per_micro_batch": 0.0716930941209854,
        "latency_fwd": 0.023897698040328468,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.592950563580676,
        "device_tokens_per_sec": 891.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 4,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75471286272.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 75873972224.0,
        "latency_per_micro_batch": 1.1359636363140302,
        "latency_fwd": 0.3786545454380101,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.06342045696,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 4.5760949489398435,
        "device_tokens_per_sec": 1790.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34141779968.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 34403923968.0,
        "latency_per_micro_batch": 0.6093128065571328,
        "latency_fwd": 0.20310426885237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.882723336053597,
        "device_tokens_per_sec": 3355.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 9900719104.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 10001415168.0,
        "latency_per_micro_batch": 0.012224269761114672,
        "latency_fwd": 0.004074756587038224,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0007829367466666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 6.259480554187906,
        "device_tokens_per_sec": 654.37,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 51618803712.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2825569809699576,
        "latency_fwd": 0.09418566032331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.06342045696,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.553152099203045,
        "device_tokens_per_sec": 449.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 119,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 3735092224.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 3835788288.0,
        "latency_per_micro_batch": 0.011840289012381634,
        "latency_fwd": 0.003946763004127211,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.0007829367466666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 6.062882410836591,
        "device_tokens_per_sec": 675.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7531502592.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 7632198656.0,
        "latency_per_micro_batch": 0.02520496407201318,
        "latency_fwd": 0.00840165469067106,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 6.454606414672309,
        "device_tokens_per_sec": 317.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 61235884032.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 61760172032.0,
        "latency_per_micro_batch": 0.0726997270809854,
        "latency_fwd": 0.024233242360328467,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.654412801894308,
        "device_tokens_per_sec": 3520.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34671194112.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 35073880064.0,
        "latency_per_micro_batch": 0.28272475312995765,
        "latency_fwd": 0.09424158437665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.54003767442241,
        "device_tokens_per_sec": 902.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 4448689152.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 4549385216.0,
        "latency_per_micro_batch": 0.02520496407201318,
        "latency_fwd": 0.00840165469067106,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 6.454606414672309,
        "device_tokens_per_sec": 317.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 41733528576.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 41864600576.0,
        "latency_per_micro_batch": 0.3614499124494149,
        "latency_fwd": 0.12048330414980496,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 5.787309112413896,
        "device_tokens_per_sec": 2831.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14940270592.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 15342956544.0,
        "latency_per_micro_batch": 0.03600917662117931,
        "latency_fwd": 0.012003058873726437,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.6117922999309835,
        "device_tokens_per_sec": 888.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8281157632.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 8683843584.0,
        "latency_per_micro_batch": 0.0006769817950901019,
        "latency_fwd": 0.00022566059836336732,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 0.6938722031747165,
        "device_tokens_per_sec": 2951.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14534719488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 15287502848.0,
        "latency_per_micro_batch": 0.09257141605970548,
        "latency_fwd": 0.030857138686568496,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.932791511417685,
        "device_tokens_per_sec": 345.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 15857786880.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 16663126016.0,
        "latency_per_micro_batch": 0.01738000248198086,
        "latency_fwd": 0.005793334160660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00100667392,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 4.4525411638814605,
        "device_tokens_per_sec": 459.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13039099904.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 13844439040.0,
        "latency_per_micro_batch": 0.0005898693273977942,
        "latency_fwd": 0.00019662310913259808,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 0.303298785632575,
        "device_tokens_per_sec": 6752.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 119,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 12138118144.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 12269190144.0,
        "latency_per_micro_batch": 0.04607550622117931,
        "latency_fwd": 0.015358502073726438,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.898319232808146,
        "device_tokens_per_sec": 2777.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25051183104.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 25252542464.0,
        "latency_per_micro_batch": 0.07861056216380378,
        "latency_fwd": 0.02620352072126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.033372266258495,
        "device_tokens_per_sec": 1627.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34600415232.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 35003101184.0,
        "latency_per_micro_batch": 0.28306029744995764,
        "latency_fwd": 0.0943534324833192,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.537506993872092,
        "device_tokens_per_sec": 1805.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 6765467648.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 6966827008.0,
        "latency_per_micro_batch": 0.010414225652381636,
        "latency_fwd": 0.0034714085507938784,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.00033554432,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 5.332898672839081,
        "device_tokens_per_sec": 768.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 25809664000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.19050266745662858,
        "latency_fwd": 0.06350088915220953,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 6.112205703303957,
        "device_tokens_per_sec": 335.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28504307712.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28766451712.0,
        "latency_per_micro_batch": 0.30587731120995765,
        "latency_fwd": 0.10195910373665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.898308132408203,
        "device_tokens_per_sec": 3344.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 7928901632.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 8331587584.0,
        "latency_per_micro_batch": 0.009407592692381634,
        "latency_fwd": 0.0031358642307938783,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.00011184810666666666,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 4.818317727210639,
        "device_tokens_per_sec": 425.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 23571519488.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 23833663488.0,
        "latency_per_micro_batch": 0.04037125278117931,
        "latency_fwd": 0.013457084260393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.168335494810635,
        "device_tokens_per_sec": 3170.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22708624384.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 22839696384.0,
        "latency_per_micro_batch": 0.6955476967971328,
        "latency_fwd": 0.23184923226571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 5.572441889572964,
        "device_tokens_per_sec": 2940.18,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23396380672.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 23799066624.0,
        "latency_per_micro_batch": 0.14174442369637003,
        "latency_fwd": 0.04724814123212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.54436379295661,
        "device_tokens_per_sec": 901.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 14164342784.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 14365702144.0,
        "latency_per_micro_batch": 0.039519556053293096,
        "latency_fwd": 0.013173185351097698,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.060799462596569,
        "device_tokens_per_sec": 809.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 12367484928.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 12568844288.0,
        "latency_per_micro_batch": 0.02039990136198086,
        "latency_fwd": 0.006799967120660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.223189887486783,
        "device_tokens_per_sec": 1568.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 71101345792.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 71625633792.0,
        "latency_per_micro_batch": 0.5692027260884064,
        "latency_fwd": 0.18973424202946879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.562164043380021,
        "device_tokens_per_sec": 3591.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 45733670912.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 46257958912.0,
        "latency_per_micro_batch": 0.002416004761243948,
        "latency_fwd": 0.0008053349204146493,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 0.30989145444167754,
        "device_tokens_per_sec": 52870.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 26144305152.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 26546991104.0,
        "latency_per_micro_batch": 0.07158980329957622,
        "latency_fwd": 0.023863267766525406,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.584365103592909,
        "device_tokens_per_sec": 1786.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 7311554560.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 7512913920.0,
        "latency_per_micro_batch": 0.010327130590410082,
        "latency_fwd": 0.0034423768634700273,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00033554432,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 5.288799717428102,
        "device_tokens_per_sec": 387.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 18867354624.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 18968050688.0,
        "latency_per_micro_batch": 0.024019989579446933,
        "latency_fwd": 0.008006663193148978,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 6.149771768835609,
        "device_tokens_per_sec": 1332.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 119,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 16,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 133465088.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 266872832.0,
        "optimizer_state_memory_per_gpu": 800618496.0,
        "(weight+op_state)_memory_per_gpu": 934083584.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 6536100864.0,
        "(weight+op_state+grad)_memory_per_gpu": 1200956416.0,
        "estimated_peak_memory_per_gpu": 6636796928.0,
        "latency_per_micro_batch": 0.02325202808198086,
        "latency_fwd": 0.007750676027326954,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.953173625484294,
        "device_tokens_per_sec": 1376.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11539658752.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11741018112.0,
        "latency_per_micro_batch": 0.039267897813293096,
        "latency_fwd": 0.013089299271097699,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.030562073150397,
        "device_tokens_per_sec": 407.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12156956672.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 12559642624.0,
        "latency_per_micro_batch": 0.03584140446117931,
        "latency_fwd": 0.011947134820393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.5922923108685625,
        "device_tokens_per_sec": 445.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45875228672.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 46277914624.0,
        "latency_per_micro_batch": 0.5650209563171329,
        "latency_fwd": 0.1883403187723776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.536609274880151,
        "device_tokens_per_sec": 1805.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46824863744.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 47087007744.0,
        "latency_per_micro_batch": 0.15679257438482516,
        "latency_fwd": 0.052264191461608385,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.019658668089458,
        "device_tokens_per_sec": 3263.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 36009439232.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 36412125184.0,
        "latency_per_micro_batch": 0.1429576382991884,
        "latency_fwd": 0.047652546099729465,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.579236965411639,
        "device_tokens_per_sec": 1788.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44782336000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 44983695360.0,
        "latency_per_micro_batch": 0.6188382169909534,
        "latency_fwd": 0.20627940566365113,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.9668260806194695,
        "device_tokens_per_sec": 1649.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 3039288320.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 3139984384.0,
        "latency_per_micro_batch": 0.011546687732381634,
        "latency_fwd": 0.0038488959107938784,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.0007829367466666667,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 5.913052280723171,
        "device_tokens_per_sec": 346.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 30830784512.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 31233470464.0,
        "latency_per_micro_batch": 0.14245432181918838,
        "latency_fwd": 0.04748477393972946,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.574979922557116,
        "device_tokens_per_sec": 447.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 6122101760.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 6222797824.0,
        "latency_per_micro_batch": 0.011930668481114674,
        "latency_fwd": 0.003976889493704891,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0007829367466666667,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 6.109650424074488,
        "device_tokens_per_sec": 335.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 60744036352.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 61146722304.0,
        "latency_per_micro_batch": 0.5681960931284062,
        "latency_fwd": 0.18939869770946877,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.06342045696,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.577809148710973,
        "device_tokens_per_sec": 894.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 35588595712.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 36393934848.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.015603445760000001,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.37036984612439,
        "device_tokens_per_sec": 468.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 12121567232.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 12524253184.0,
        "latency_per_micro_batch": 0.0009254136473977942,
        "latency_fwd": 0.00030847121579926474,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 0.4744546324701228,
        "device_tokens_per_sec": 8633.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 128,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13039099904.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 13844439040.0,
        "latency_per_micro_batch": 0.000509209635090102,
        "latency_fwd": 0.000169736545030034,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 0.5227163563371688,
        "device_tokens_per_sec": 3917.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57925951488.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 58731290624.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0075500544,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.355613693082049,
        "device_tokens_per_sec": 1880.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 6078511104.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 6279870464.0,
        "latency_per_micro_batch": 0.01964492664198086,
        "latency_fwd": 0.006548308880660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.031397508122153,
        "device_tokens_per_sec": 407.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11099860992.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 11502546944.0,
        "latency_per_micro_batch": 0.018328572067333158,
        "latency_fwd": 0.006109524022444386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 4.69473214165732,
        "device_tokens_per_sec": 436.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 4,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 68283297792.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 68807585792.0,
        "latency_per_micro_batch": 1.1296133626914833,
        "latency_fwd": 0.3765377875638278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 4.534895075109021,
        "device_tokens_per_sec": 3612.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 48552374272.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 49076662272.0,
        "latency_per_micro_batch": 0.14275105665637003,
        "latency_fwd": 0.04758368555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.570651505423872,
        "device_tokens_per_sec": 3584.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 33507391488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 33708750848.0,
        "latency_per_micro_batch": 0.3101366999468679,
        "latency_fwd": 0.1033788999822893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.97040808274642,
        "device_tokens_per_sec": 1648.15,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 7470151680.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 7671511040.0,
        "latency_per_micro_batch": 0.019896584881980862,
        "latency_fwd": 0.0066321949606602875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.09483458492524,
        "device_tokens_per_sec": 803.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 23800257536.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 23900953600.0,
        "latency_per_micro_batch": 0.18093923119609864,
        "latency_fwd": 0.06031307706536621,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.794165911498414,
        "device_tokens_per_sec": 1413.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11469003776.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 11569699840.0,
        "latency_per_micro_batch": 0.17479553921637,
        "latency_fwd": 0.05826517973879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 5.597567768147098,
        "device_tokens_per_sec": 1463.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11698255872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11899615232.0,
        "latency_per_micro_batch": 0.07679074025957622,
        "latency_fwd": 0.02559691341985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.918878529661758,
        "device_tokens_per_sec": 832.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 31781384192.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 32184070144.0,
        "latency_per_micro_batch": 0.036564138511883906,
        "latency_fwd": 0.012188046170627968,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.681839998232382,
        "device_tokens_per_sec": 1749.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8897247232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 9098606592.0,
        "latency_per_micro_batch": 0.03860964510117931,
        "latency_fwd": 0.012869881700393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.946305725999832,
        "device_tokens_per_sec": 414.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 47143022592.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 47667310592.0,
        "latency_per_micro_batch": 0.07226089193957622,
        "latency_fwd": 0.02408696397985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.62632735284412,
        "device_tokens_per_sec": 3541.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 52288577536.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 53093916672.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0023489058133333335,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.358520815187656,
        "device_tokens_per_sec": 1879.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 4669143040.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 4870502400.0,
        "latency_per_micro_batch": 0.010162567412381637,
        "latency_fwd": 0.0033875224707938787,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.00033554432,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 5.204543370277538,
        "device_tokens_per_sec": 393.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41646845952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 42049531904.0,
        "latency_per_micro_batch": 0.28481563801559434,
        "latency_fwd": 0.09493854600519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.565592442922279,
        "device_tokens_per_sec": 1794.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22937131008.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 23138490368.0,
        "latency_per_micro_batch": 0.02072902771803775,
        "latency_fwd": 0.006909675906012584,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.307446234637347,
        "device_tokens_per_sec": 1543.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8879519744.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 9080879104.0,
        "latency_per_micro_batch": 0.038861303341179314,
        "latency_fwd": 0.012953767780393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.976543115446005,
        "device_tokens_per_sec": 823.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18463485952.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 18866171904.0,
        "latency_per_micro_batch": 0.0362285941918839,
        "latency_fwd": 0.012076198063961301,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 4.639877748981171,
        "device_tokens_per_sec": 882.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46792630272.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 47597969408.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.015603445760000001,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.363513047039871,
        "device_tokens_per_sec": 938.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17106607104.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 17207303168.0,
        "latency_per_micro_batch": 0.34798812336995766,
        "latency_fwd": 0.11599604112331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 5.575870289115224,
        "device_tokens_per_sec": 1469.19,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29437860864.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 29538556928.0,
        "latency_per_micro_batch": 0.3602755073294149,
        "latency_fwd": 0.12009183577647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 5.77246843246654,
        "device_tokens_per_sec": 1419.15,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 6519549952.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 6922235904.0,
        "latency_per_micro_batch": 0.0006769817950901019,
        "latency_fwd": 0.00022566059836336732,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 2.6886564102564102e-05,
        "latency_per_iter": 0.6938722031747165,
        "device_tokens_per_sec": 2951.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 128,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24243134464.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 25048473600.0,
        "latency_per_micro_batch": 0.0005898693273977942,
        "latency_fwd": 0.00019662310913259808,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 0.303298785632575,
        "device_tokens_per_sec": 13504.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 47371131904.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 47502203904.0,
        "latency_per_micro_batch": 0.7201224647160474,
        "latency_fwd": 0.24004082157201578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 5.769040032924281,
        "device_tokens_per_sec": 2839.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 25685571584.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 25947715584.0,
        "latency_per_micro_batch": 0.15415956353637003,
        "latency_fwd": 0.05138652117879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 4.935402320938894,
        "device_tokens_per_sec": 3319.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 53380411392.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2843123215355944,
        "latency_fwd": 0.09477077384519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.06342045696,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.581237548253233,
        "device_tokens_per_sec": 447.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 10605419520.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 10706115584.0,
        "latency_per_micro_batch": 0.02343278701944694,
        "latency_fwd": 0.007810929006482313,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.999941638722191,
        "device_tokens_per_sec": 682.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 4439792640.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 4540488704.0,
        "latency_per_micro_batch": 0.02266482552198086,
        "latency_fwd": 0.007554941840660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.803343495370875,
        "device_tokens_per_sec": 705.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38194409472.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 38597095424.0,
        "latency_per_micro_batch": 0.28448009369559435,
        "latency_fwd": 0.09482669789853146,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.568123123472597,
        "device_tokens_per_sec": 896.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17758973952.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 18161659904.0,
        "latency_per_micro_batch": 0.07125425897957621,
        "latency_fwd": 0.02375141965985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.564865114530488,
        "device_tokens_per_sec": 897.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 16983078912.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17184438272.0,
        "latency_per_micro_batch": 0.07810724568380378,
        "latency_fwd": 0.02603574856126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.0031348768123225,
        "device_tokens_per_sec": 818.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8650202112.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 8750898176.0,
        "latency_per_micro_batch": 0.08819924713957622,
        "latency_fwd": 0.029399749046525404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.646887429169813,
        "device_tokens_per_sec": 1450.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 20981455872.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 21082151936.0,
        "latency_per_micro_batch": 0.09127109312944053,
        "latency_fwd": 0.03042369770981351,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 5.8434855725211285,
        "device_tokens_per_sec": 1401.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 4,
        "global_batch_size": 2048,
        "dp_size": 64,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 68424855552.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 68827541504.0,
        "latency_per_micro_batch": 1.1289422740514832,
        "latency_fwd": 0.37631409135049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.06342045696,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 4.548009499889655,
        "device_tokens_per_sec": 1801.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22937745408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 23139104768.0,
        "latency_per_micro_batch": 0.3048706782499576,
        "latency_fwd": 0.10162355941665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.886151735595856,
        "device_tokens_per_sec": 1676.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 131076096.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 262152192.0,
        "optimizer_state_memory_per_gpu": 786456576.0,
        "(weight+op_state)_memory_per_gpu": 917532672.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 30372032512.0,
        "(weight+op_state+grad)_memory_per_gpu": 1179684864.0,
        "estimated_peak_memory_per_gpu": 30774718464.0,
        "latency_per_micro_batch": 0.0014222773520131788,
        "latency_fwd": 0.0004740924506710596,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0.0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 0.364745847117826,
        "device_tokens_per_sec": 22459.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12754974720.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 12956334080.0,
        "latency_per_micro_batch": 0.020225711238037753,
        "latency_fwd": 0.006741903746012585,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.1790909320758045,
        "device_tokens_per_sec": 790.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 65463939072.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 65988227072.0,
        "latency_per_micro_batch": 0.2854867266555944,
        "latency_fwd": 0.09516224221853145,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.572380166327121,
        "device_tokens_per_sec": 3583.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17794363392.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 18197049344.0,
        "latency_per_micro_batch": 0.07108648681957622,
        "latency_fwd": 0.023695495606525407,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.558077391125647,
        "device_tokens_per_sec": 449.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8720922624.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 8922281984.0,
        "latency_per_micro_batch": 0.019974052998037752,
        "latency_fwd": 0.006658017666012584,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 5.1156538552727175,
        "device_tokens_per_sec": 400.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 46651203584.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 47456542720.0,
        "latency_per_micro_batch": 0.0010738274812439482,
        "latency_fwd": 0.0003579424937479827,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 0.13873560760412973,
        "device_tokens_per_sec": 59047.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 49469890560.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 50275229696.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00100667392,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 4.370259574867237,
        "device_tokens_per_sec": 1874.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 166217728.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 33554432.0,
        "weight_memory_mlp_per_gpu": 67108864.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 332410880.0,
        "optimizer_state_memory_per_gpu": 997232640.0,
        "(weight+op_state)_memory_per_gpu": 1163450368.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44710811648.0,
        "(weight+op_state+grad)_memory_per_gpu": 1495861248.0,
        "estimated_peak_memory_per_gpu": 44972955648.0,
        "latency_per_micro_batch": 0.041029505493293095,
        "latency_fwd": 0.013676501831097698,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 5.252591841961199,
        "device_tokens_per_sec": 3119.22,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49643599872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 49905743872.0,
        "latency_per_micro_batch": 0.3111433329068679,
        "latency_fwd": 0.10371444430228931,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 4.982564479558767,
        "device_tokens_per_sec": 3288.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13530918912.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 13933604864.0,
        "latency_per_micro_batch": 0.01838663544198086,
        "latency_fwd": 0.006128878480660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 4.708608941858342,
        "device_tokens_per_sec": 869.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 128,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34212689920.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34414049280.0,
        "latency_per_micro_batch": 0.6083061735971329,
        "latency_fwd": 0.20276872453237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 4.882569733468905,
        "device_tokens_per_sec": 1677.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 4.311486400984616,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 49946484224.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 50261097984.0,
        "latency_per_micro_batch": 0.3203874959028543,
        "latency_fwd": 0.1067958319676181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 20.513687173061406,
        "device_tokens_per_sec": 399.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50441407488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 51070594048.0,
        "latency_per_micro_batch": 0.2704070309992279,
        "latency_fwd": 0.09013567699974263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 17.314567718177603,
        "device_tokens_per_sec": 473.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 12090922496.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 12405536256.0,
        "latency_per_micro_batch": 0.015386778985704907,
        "latency_fwd": 0.005128926328568303,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0004194304,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 15.758006306243303,
        "device_tokens_per_sec": 129.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63080723968.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 63395337728.0,
        "latency_per_micro_batch": 1.2593243893428543,
        "latency_fwd": 0.4197747964476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 20.1665633219166,
        "device_tokens_per_sec": 812.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 79343638016.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 79867926016.0,
        "latency_per_micro_batch": 0.23486188878353947,
        "latency_fwd": 0.07828729626117982,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.024228020644102562,
        "latency_fwd_mlp": 0.044050946625641026,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0018562840761593374,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 15.033105507028004,
        "device_tokens_per_sec": 2179.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43938706944.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 44253320704.0,
        "latency_per_micro_batch": 0.6299300383856662,
        "latency_fwd": 0.20997667946188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 20.175134320772248,
        "device_tokens_per_sec": 406.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 74522820608.0,
        "activation_memory_attn_per_gpu": 22209363968.0,
        "activation_memory_mlp_per_gpu": 22145925120.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 80601558528.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 80916172288.0,
        "latency_per_micro_batch": 0.292976048049939,
        "latency_fwd": 0.097658682683313,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0018284797558716145,
        "latency_fwd_tp_comm": 0.008388608,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 18.75472597020999,
        "device_tokens_per_sec": 873.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68734946304.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 69364132864.0,
        "latency_per_micro_batch": 0.5921813023856661,
        "latency_fwd": 0.19739376746188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 18.984547615679773,
        "device_tokens_per_sec": 215.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30732913664.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 31362100224.0,
        "latency_per_micro_batch": 0.055197205730624116,
        "latency_fwd": 0.018399068576874705,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 14.13437389448242,
        "device_tokens_per_sec": 289.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 53805426176.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.39337315992078564,
        "latency_fwd": 0.13112438664026188,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 25.21022664166561,
        "device_tokens_per_sec": 81.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12920015872.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 13549202432.0,
        "latency_per_micro_batch": 0.014237356613169352,
        "latency_fwd": 0.0047457855377231175,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 2.285599694839518e-05,
        "latency_fwd_tp_comm": 0.00013981013333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 14.581399563733273,
        "device_tokens_per_sec": 140.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 10864242176.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 11178855936.0,
        "latency_per_micro_batch": 0.036576472574821745,
        "latency_fwd": 0.012192157524940581,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 5.713999237098795e-05,
        "latency_fwd_tp_comm": 0.001048576,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 18.731412853322627,
        "device_tokens_per_sec": 109.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12346578432.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 12661192192.0,
        "latency_per_micro_batch": 0.029826462597860693,
        "latency_fwd": 0.009942154199286898,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0008388608,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 15.273093474986153,
        "device_tokens_per_sec": 268.18,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30244173824.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 31502505984.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 13.531178564979285,
        "device_tokens_per_sec": 151.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22181437184.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 22338764544.0,
        "latency_per_micro_batch": 0.16315376561385295,
        "latency_fwd": 0.05438458853795098,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.009786709333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 20.88581147398051,
        "device_tokens_per_sec": 784.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 20076145408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 20233472768.0,
        "latency_per_micro_batch": 0.10552168915155494,
        "latency_fwd": 0.03517389638385165,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 27.02223909177523,
        "device_tokens_per_sec": 75.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 53712636928.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 54341823488.0,
        "latency_per_micro_batch": 0.05561663613062412,
        "latency_fwd": 0.01853887871020804,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 14.240205241287631,
        "device_tokens_per_sec": 575.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14791322368.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 16098233600.0,
        "latency_per_micro_batch": 0.10552168915155494,
        "latency_fwd": 0.03517389638385165,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 27.02223909177523,
        "device_tokens_per_sec": 75.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 50551737344.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 51180923904.0,
        "latency_per_micro_batch": 0.29807670590566626,
        "latency_fwd": 0.09935890196855542,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 19.0946839257584,
        "device_tokens_per_sec": 214.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34081420032.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 34238747392.0,
        "latency_per_micro_batch": 0.7039595039856662,
        "latency_fwd": 0.23465316799522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 22.543876576432385,
        "device_tokens_per_sec": 363.38,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 55220712192.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 55378039552.0,
        "latency_per_micro_batch": 0.7250784451659833,
        "latency_fwd": 0.24169281505532778,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 23.219682694202533,
        "device_tokens_per_sec": 352.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 67650627072.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 67965240832.0,
        "latency_per_micro_batch": 0.6383518675772306,
        "latency_fwd": 0.2127839558590769,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 20.46160416920671,
        "device_tokens_per_sec": 200.18,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50969107968.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 51283721728.0,
        "latency_per_micro_batch": 0.6311883295856662,
        "latency_fwd": 0.21039610986188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 20.206913982020048,
        "device_tokens_per_sec": 810.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 79917075456.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 80546262016.0,
        "latency_per_micro_batch": 0.11014904090600874,
        "latency_fwd": 0.03671634696866958,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0007378030094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 14.101423627816976,
        "device_tokens_per_sec": 1161.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 18394570496.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 18551897856.0,
        "latency_per_micro_batch": 0.04239014681461249,
        "latency_fwd": 0.014130048938204163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0024466773333333336,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 21.70588464448893,
        "device_tokens_per_sec": 188.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 7824924416.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 7982251776.0,
        "latency_per_micro_batch": 0.041190206974821744,
        "latency_fwd": 0.013730068991607247,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 5.713999237098795e-05,
        "latency_fwd_tp_comm": 0.0024466773333333336,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 21.091515446516066,
        "device_tokens_per_sec": 194.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24748280832.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 25377467392.0,
        "latency_per_micro_batch": 0.06834636327539141,
        "latency_fwd": 0.022782121091797138,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 17.505186732727218,
        "device_tokens_per_sec": 116.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24709443072.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 25024056832.0,
        "latency_per_micro_batch": 0.07364577758384326,
        "latency_fwd": 0.024548592527947753,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.002097152,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 18.85757795647777,
        "device_tokens_per_sec": 217.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 15394090496.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 15708704256.0,
        "latency_per_micro_batch": 0.037090732506160635,
        "latency_fwd": 0.012363577502053545,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0037856282256410255,
        "latency_fwd_mlp": 0.00688296041025641,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.001048576,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 18.99471393816814,
        "device_tokens_per_sec": 107.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 37339188224.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 37968374784.0,
        "latency_per_micro_batch": 0.1361570391223048,
        "latency_fwd": 0.045385679707434935,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 17.436618741882032,
        "device_tokens_per_sec": 234.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 5432172288.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 5589499648.0,
        "latency_per_micro_batch": 0.020862947201649877,
        "latency_fwd": 0.006954315733883293,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0018928141128205127,
        "latency_fwd_mlp": 0.003441480205128205,
        "latency_fwd_layernorm": 2.8569996185493976e-05,
        "latency_fwd_tp_comm": 0.0012233386666666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 21.365787409896807,
        "device_tokens_per_sec": 95.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 10716995328.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 10874322688.0,
        "latency_per_micro_batch": 0.02146291712154525,
        "latency_fwd": 0.00715430570718175,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0018928141128205127,
        "latency_fwd_mlp": 0.003441480205128205,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0012233386666666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 21.98015660786967,
        "device_tokens_per_sec": 93.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43652428544.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 43809755904.0,
        "latency_per_micro_batch": 1.4073833205428543,
        "latency_fwd": 0.4691277735142848,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 22.535305577576736,
        "device_tokens_per_sec": 727.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50682604288.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 50944748288.0,
        "latency_per_micro_batch": 1.4103193333428543,
        "latency_fwd": 0.4701064444476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 22.573796002462835,
        "device_tokens_per_sec": 1451.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 58335274496.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.39337315992078564,
        "latency_fwd": 0.13112438664026188,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 25.21022664166561,
        "device_tokens_per_sec": 81.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 43835132928.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 44464319488.0,
        "latency_per_micro_batch": 0.10931018010600874,
        "latency_fwd": 0.03643672670200291,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 13.995592281011765,
        "device_tokens_per_sec": 585.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37960829952.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 38590016512.0,
        "latency_per_micro_batch": 0.14930619666707212,
        "latency_fwd": 0.04976873222235737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 19.12896792118099,
        "device_tokens_per_sec": 107.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37052950272.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 37210277632.0,
        "latency_per_micro_batch": 0.18277238166285434,
        "latency_fwd": 0.060924127220951446,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 23.399308631865573,
        "device_tokens_per_sec": 350.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15913658112.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 16070985472.0,
        "latency_per_micro_batch": 0.17749264636777506,
        "latency_fwd": 0.05916421545592502,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 22.723502514095426,
        "device_tokens_per_sec": 360.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 62058100224.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 62372713984.0,
        "latency_per_micro_batch": 0.6389810131772307,
        "latency_fwd": 0.21299367105907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 20.46476551410231,
        "device_tokens_per_sec": 400.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17184074496.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 17341401856.0,
        "latency_per_micro_batch": 0.17675864316777506,
        "latency_fwd": 0.058919547722591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 22.633792994452374,
        "device_tokens_per_sec": 180.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 27753720576.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 27911047936.0,
        "latency_per_micro_batch": 0.18203837846285434,
        "latency_fwd": 0.06067945948761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 23.30959911222252,
        "device_tokens_per_sec": 175.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49386190848.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 50644523008.0,
        "latency_per_micro_batch": 0.10511587610600871,
        "latency_fwd": 0.03503862536866957,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0031458304,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 13.462610574134098,
        "device_tokens_per_sec": 304.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31827090944.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 32141704704.0,
        "latency_per_micro_batch": 0.3158620085070721,
        "latency_fwd": 0.10528733616902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 20.224055979731343,
        "device_tokens_per_sec": 405.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 7561074176.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 7875687936.0,
        "latency_per_micro_batch": 0.01518107501316935,
        "latency_fwd": 0.005060358337723117,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 2.285599694839518e-05,
        "latency_fwd_tp_comm": 0.0004194304,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 15.547365438366892,
        "device_tokens_per_sec": 131.73,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15649746432.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 15964360192.0,
        "latency_per_micro_batch": 0.07261725772116548,
        "latency_fwd": 0.024205752573721824,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.002097152,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 18.594276871632257,
        "device_tokens_per_sec": 220.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44626683648.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 44888827648.0,
        "latency_per_micro_batch": 0.7083635231856662,
        "latency_fwd": 0.23612117439522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 22.672076520961536,
        "device_tokens_per_sec": 1445.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22491024384.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 23120210944.0,
        "latency_per_micro_batch": 0.027939025797860693,
        "latency_fwd": 0.009313008599286897,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 14.307127600352532,
        "device_tokens_per_sec": 286.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63653956608.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 64283143168.0,
        "latency_per_micro_batch": 0.5926007327856662,
        "latency_fwd": 0.19753357759522205,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 18.980998196937076,
        "device_tokens_per_sec": 431.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 18142006272.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 18771192832.0,
        "latency_per_micro_batch": 0.027866446579551063,
        "latency_fwd": 0.009288815526517022,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 14.271509876172791,
        "device_tokens_per_sec": 143.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 58590930432.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.6293008927856663,
        "latency_fwd": 0.20976696426188873,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 20.17197297587665,
        "device_tokens_per_sec": 203.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 59163937792.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.2963584949070721,
        "latency_fwd": 0.09878616496902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 19.001689613391072,
        "device_tokens_per_sec": 107.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44362771968.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 44677385728.0,
        "latency_per_micro_batch": 0.28886196859922786,
        "latency_fwd": 0.09628732286640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.008388608,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 18.491424885364477,
        "device_tokens_per_sec": 886.03,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 64460021504.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 64617348864.0,
        "latency_per_micro_batch": 0.1679535249730159,
        "latency_fwd": 0.05598450832433864,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0018284797558716145,
        "latency_fwd_tp_comm": 0.009786709333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 21.50018067195337,
        "device_tokens_per_sec": 762.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21406275072.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 21720888832.0,
        "latency_per_micro_batch": 0.030237870542931807,
        "latency_fwd": 0.010079290180977269,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0008388608,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 15.483734342862563,
        "device_tokens_per_sec": 264.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62598678528.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 63857010688.0,
        "latency_per_micro_batch": 0.2611795621992279,
        "latency_fwd": 0.08705985406640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 16.732527393403846,
        "device_tokens_per_sec": 244.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31540587264.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 31697914624.0,
        "latency_per_micro_batch": 0.7054275103856662,
        "latency_fwd": 0.23514250346188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 22.582367001318485,
        "device_tokens_per_sec": 725.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62794445568.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 63056589568.0,
        "latency_per_micro_batch": 2.8142309536572307,
        "latency_fwd": 0.9380769845524102,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.26650822708512817,
        "latency_fwd_mlp": 0.48456041288205126,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.17224608426666668,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.008603700512820512,
        "latency_per_iter": 22.531020078148913,
        "device_tokens_per_sec": 1454.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 40036980224.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 40351593984.0,
        "latency_per_micro_batch": 0.0599400536573856,
        "latency_fwd": 0.0199800178857952,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 15.346598361172191,
        "device_tokens_per_sec": 533.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21728381952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 22357568512.0,
        "latency_per_micro_batch": 0.06800352332116548,
        "latency_fwd": 0.02266784110705516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 17.41741970444538,
        "device_tokens_per_sec": 117.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 22000426496.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22315040256.0,
        "latency_per_micro_batch": 0.08049863954707209,
        "latency_fwd": 0.026832879849024032,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 20.616539159329186,
        "device_tokens_per_sec": 99.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29582194176.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 32195975680.0,
        "latency_per_micro_batch": 0.15788427516777503,
        "latency_fwd": 0.05262809172259168,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 20.226560313906134,
        "device_tokens_per_sec": 101.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41633041408.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 42262227968.0,
        "latency_per_micro_batch": 0.05534236416724337,
        "latency_fwd": 0.018447454722414457,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 14.16999161866216,
        "device_tokens_per_sec": 578.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68161938944.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 68476552704.0,
        "latency_per_micro_batch": 1.2580660981428544,
        "latency_fwd": 0.41935536604761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.12976550399999998,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 20.163401977021,
        "device_tokens_per_sec": 406.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44511939584.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 45141126144.0,
        "latency_per_micro_batch": 0.2965682101070721,
        "latency_fwd": 0.0988560700356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 18.998140194648375,
        "device_tokens_per_sec": 215.6,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 327690240.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 655380480.0,
        "optimizer_state_memory_per_gpu": 1966141440.0,
        "(weight+op_state)_memory_per_gpu": 2293831680.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 40577865728.0,
        "(weight+op_state+grad)_memory_per_gpu": 2949212160.0,
        "estimated_peak_memory_per_gpu": 41836197888.0,
        "latency_per_micro_batch": 0.0009389858900164734,
        "latency_fwd": 0.0003129952966721578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 0.48236788819456483,
        "device_tokens_per_sec": 8491.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25220754944.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 25535368704.0,
        "latency_per_micro_batch": 0.14469882801385295,
        "latency_fwd": 0.048232942671284314,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.004194304,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 18.52570888078707,
        "device_tokens_per_sec": 442.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22256082432.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22570696192.0,
        "latency_per_micro_batch": 0.15819884796777506,
        "latency_fwd": 0.052732949322591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 20.258339975153937,
        "device_tokens_per_sec": 202.19,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 21697799936.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 21855127296.0,
        "latency_per_micro_batch": 0.09165403454566617,
        "latency_fwd": 0.03055134484855539,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 23.467876622710758,
        "device_tokens_per_sec": 174.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11128153856.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 11285481216.0,
        "latency_per_micro_batch": 0.08901416689812652,
        "latency_fwd": 0.029671388966042177,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 22.792070504940607,
        "device_tokens_per_sec": 179.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24693115904.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 25322302464.0,
        "latency_per_micro_batch": 0.054922933767243366,
        "latency_fwd": 0.018307644589081122,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0005592405333333334,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 14.064160271856949,
        "device_tokens_per_sec": 291.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 32187986688.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 32345314048.0,
        "latency_per_micro_batch": 0.24474482367463185,
        "latency_fwd": 0.08158160789154395,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 31.344509879243944,
        "device_tokens_per_sec": 65.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 26903163648.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.24474482367463185,
        "latency_fwd": 0.08158160789154395,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 31.344509879243944,
        "device_tokens_per_sec": 65.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21969578752.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 22126906112.0,
        "latency_per_micro_batch": 0.3529815989070721,
        "latency_fwd": 0.11766053296902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 22.59950899902978,
        "device_tokens_per_sec": 362.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15939914752.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 16569101312.0,
        "latency_per_micro_batch": 0.014305924604014541,
        "latency_fwd": 0.004768641534671513,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0015142512902564101,
        "latency_fwd_mlp": 0.002753184164102564,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.00013981013333333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 14.651613186358746,
        "device_tokens_per_sec": 139.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25484666624.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 25641993984.0,
        "latency_per_micro_batch": 0.3544496053070721,
        "latency_fwd": 0.1181498684356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 22.68921851867283,
        "device_tokens_per_sec": 722.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43108870912.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 43266198272.0,
        "latency_per_micro_batch": 0.36354106949723064,
        "latency_fwd": 0.12118035649907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 23.275315116799927,
        "device_tokens_per_sec": 351.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43427395072.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 43742008832.0,
        "latency_per_micro_batch": 0.3197583503028544,
        "latency_fwd": 0.10658611676761812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 20.48190751181361,
        "device_tokens_per_sec": 199.98,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 956856320.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1913712640.0,
        "optimizer_state_memory_per_gpu": 5741137920.0,
        "(weight+op_state)_memory_per_gpu": 6697994240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25840011264.0,
        "(weight+op_state+grad)_memory_per_gpu": 8611706880.0,
        "estimated_peak_memory_per_gpu": 27098343424.0,
        "latency_per_micro_batch": 0.02668073459786069,
        "latency_fwd": 0.008893578199286897,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 13.665228886640229,
        "device_tokens_per_sec": 149.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 74774744064.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 75403930624.0,
        "latency_per_micro_batch": 0.5951982939828544,
        "latency_fwd": 0.19839943132761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 19.081091346789798,
        "device_tokens_per_sec": 214.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 74522820608.0,
        "activation_memory_attn_per_gpu": 22209363968.0,
        "activation_memory_mlp_per_gpu": 22145925120.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 77298390528.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 77613004288.0,
        "latency_per_micro_batch": 0.11934441988629318,
        "latency_fwd": 0.03978147329543106,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.0007313919023486457,
        "latency_fwd_tp_comm": 0.0033554432,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 15.278030370327006,
        "device_tokens_per_sec": 1072.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 76568068096.0,
        "activation_memory_attn_per_gpu": 27809284096.0,
        "activation_memory_mlp_per_gpu": 33218887680.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 83886080.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 82646806016.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 83171094016.0,
        "latency_per_micro_batch": 0.5771882497699777,
        "latency_fwd": 0.19239608325665924,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06057005161025641,
        "latency_fwd_mlp": 0.11012736656410256,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.016777216,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0018562840761593374,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 18.47428288765318,
        "device_tokens_per_sec": 1773.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62521003008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 63150189568.0,
        "latency_per_micro_batch": 0.27177839081613164,
        "latency_fwd": 0.09059279693871054,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 17.40233474645944,
        "device_tokens_per_sec": 470.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33749720832.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 33907048192.0,
        "latency_per_micro_batch": 0.08424460620074697,
        "latency_fwd": 0.02808153540024899,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.004893354666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 21.56874866279856,
        "device_tokens_per_sec": 379.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12610428672.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 12767756032.0,
        "latency_per_micro_batch": 0.08184472652116548,
        "latency_fwd": 0.02728157550705516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.004893354666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 20.954379464825696,
        "device_tokens_per_sec": 390.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 3,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7248517120.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14497034240.0,
        "optimizer_state_memory_per_gpu": 43491102720.0,
        "(weight+op_state)_memory_per_gpu": 50739619840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 69881636864.0,
        "(weight+op_state+grad)_memory_per_gpu": 65236654080.0,
        "estimated_peak_memory_per_gpu": 71139969024.0,
        "latency_per_micro_batch": 0.28692131090707207,
        "latency_fwd": 0.09564043696902402,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.032441375999999994,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 18.398513270882432,
        "device_tokens_per_sec": 111.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 17,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 868456960.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 262144000.0,
        "weight_memory_mlp_per_gpu": 524288000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1736760320.0,
        "optimizer_state_memory_per_gpu": 5210280960.0,
        "(weight+op_state)_memory_per_gpu": 6078737920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43340148224.0,
        "(weight+op_state+grad)_memory_per_gpu": 7815498240.0,
        "estimated_peak_memory_per_gpu": 43654761984.0,
        "latency_per_micro_batch": 0.1467558677392085,
        "latency_fwd": 0.04891862257973617,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.004194304,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 18.789009965632584,
        "device_tokens_per_sec": 436.0,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 62183836672.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.29786699070566625,
        "latency_fwd": 0.09928899690188875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.06488275199999999,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 19.098233344501097,
        "device_tokens_per_sec": 107.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 956856320.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1913712640.0,
        "optimizer_state_memory_per_gpu": 5741137920.0,
        "(weight+op_state)_memory_per_gpu": 6697994240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 44982028288.0,
        "(weight+op_state+grad)_memory_per_gpu": 8611706880.0,
        "estimated_peak_memory_per_gpu": 46240360448.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0010486101333333333,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 13.528092904949858,
        "device_tokens_per_sec": 302.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41059603968.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 41374217728.0,
        "latency_per_micro_batch": 0.11769878810600873,
        "latency_fwd": 0.039232929368669574,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0.0033554432,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 15.067389502450595,
        "device_tokens_per_sec": 1087.38,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 478438400.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 20480.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 956856320.0,
        "optimizer_state_memory_per_gpu": 2870568960.0,
        "(weight+op_state)_memory_per_gpu": 3349007360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 28530822144.0,
        "(weight+op_state+grad)_memory_per_gpu": 4305863680.0,
        "estimated_peak_memory_per_gpu": 29160008704.0,
        "latency_per_micro_batch": 0.028076161779551065,
        "latency_fwd": 0.009358720593183689,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 14.377341222978002,
        "device_tokens_per_sec": 284.89,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 34112042496.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34426656256.0,
        "latency_per_micro_batch": 0.16014701886566618,
        "latency_fwd": 0.05338233962188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 20.5161915072362,
        "device_tokens_per_sec": 99.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 73819171584.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 73976498944.0,
        "latency_per_micro_batch": 0.7265464515659833,
        "latency_fwd": 0.24218215052199443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 23.25817311908863,
        "device_tokens_per_sec": 704.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 66,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 8735401728.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 8892729088.0,
        "latency_per_micro_batch": 0.044774927163302274,
        "latency_fwd": 0.014924975721100757,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.004164191048205128,
        "latency_fwd_mlp": 0.007571256451282051,
        "latency_fwd_layernorm": 6.285399160808674e-05,
        "latency_fwd_tp_comm": 0.002691345066666667,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 22.92920648663098,
        "device_tokens_per_sec": 89.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14020224768.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 14177552128.0,
        "latency_per_micro_batch": 0.04609486098707209,
        "latency_fwd": 0.015364953662357364,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.004164191048205128,
        "latency_fwd_mlp": 0.007571256451282051,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.002691345066666667,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 23.60501260440113,
        "device_tokens_per_sec": 86.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29295915776.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.3522475957070721,
        "latency_fwd": 0.1174158652356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 22.56101857414368,
        "device_tokens_per_sec": 181.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75733552128.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 76362738688.0,
        "latency_per_micro_batch": 0.5956177243828544,
        "latency_fwd": 0.19853924146095148,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 19.0775419280471,
        "device_tokens_per_sec": 429.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 39865561856.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 40022889216.0,
        "latency_per_micro_batch": 0.36280706629723064,
        "latency_fwd": 0.12093568876574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.08074298026666667,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 23.236824691913828,
        "device_tokens_per_sec": 176.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 15122107392.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 15751293952.0,
        "latency_per_micro_batch": 0.02772931059786069,
        "latency_fwd": 0.00924310353262023,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0030285025805128203,
        "latency_fwd_mlp": 0.005506368328205128,
        "latency_fwd_layernorm": 4.571199389679036e-05,
        "latency_fwd_tp_comm": 0.0002796202666666667,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.0001344328205128205,
        "latency_per_iter": 14.201296253547321,
        "device_tokens_per_sec": 144.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 34,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 396536320.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 104857600.0,
        "weight_memory_mlp_per_gpu": 209715200.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 793011200.0,
        "optimizer_state_memory_per_gpu": 2379033600.0,
        "(weight+op_state)_memory_per_gpu": 2775569920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21917586944.0,
        "(weight+op_state+grad)_memory_per_gpu": 3568581120.0,
        "estimated_peak_memory_per_gpu": 22232200704.0,
        "latency_per_micro_batch": 0.05911723776724337,
        "latency_fwd": 0.019705745922414458,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0.0016777216,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 15.13595749329578,
        "device_tokens_per_sec": 541.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17470578176.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 17785191936.0,
        "latency_per_micro_batch": 0.07936726769812653,
        "latency_fwd": 0.026455755899375513,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 20.326907965999123,
        "device_tokens_per_sec": 100.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 8,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 327690240.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 655380480.0,
        "optimizer_state_memory_per_gpu": 1966141440.0,
        "(weight+op_state)_memory_per_gpu": 2293831680.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21435848704.0,
        "(weight+op_state+grad)_memory_per_gpu": 2949212160.0,
        "estimated_peak_memory_per_gpu": 22694180864.0,
        "latency_per_micro_batch": 0.0007373366592472428,
        "latency_fwd": 0.0002457788864157476,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 6.721641025641025e-05,
        "latency_per_iter": 0.7566398515753071,
        "device_tokens_per_sec": 2706.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 68,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 434279680.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 131072000.0,
        "weight_memory_mlp_per_gpu": 262144000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 868380160.0,
        "optimizer_state_memory_per_gpu": 2605140480.0,
        "(weight+op_state)_memory_per_gpu": 3039420160.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41323454208.0,
        "(weight+op_state+grad)_memory_per_gpu": 3907800320.0,
        "estimated_peak_memory_per_gpu": 41585598208.0,
        "latency_per_micro_batch": 0.32577184379922786,
        "latency_fwd": 0.10859061459974262,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.01957341866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 20.851527478557916,
        "device_tokens_per_sec": 1571.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 793031680.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 209715200.0,
        "weight_memory_mlp_per_gpu": 419430400.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1586022400.0,
        "optimizer_state_memory_per_gpu": 4758067200.0,
        "(weight+op_state)_memory_per_gpu": 5551098880.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55914728448.0,
        "(weight+op_state+grad)_memory_per_gpu": 7137121280.0,
        "estimated_peak_memory_per_gpu": 56543915008.0,
        "latency_per_micro_batch": 0.10985872403277022,
        "latency_fwd": 0.036619574677590076,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.0003656959511743229,
        "latency_fwd_tp_comm": 0.0011184810666666667,
        "latency_fwd_sharded_dp_comm": 0.0020972202666666665,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 14.065805903637235,
        "device_tokens_per_sec": 582.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34940931072.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 35570117632.0,
        "latency_per_micro_batch": 0.14855194876777505,
        "latency_fwd": 0.049517316255925016,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.020185745066666667,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 19.032424190070966,
        "device_tokens_per_sec": 107.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34367698432.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34682312192.0,
        "latency_per_micro_batch": 0.3152328629070721,
        "latency_fwd": 0.10507762096902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.04037149013333333,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 20.192276318483543,
        "device_tokens_per_sec": 202.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 31315779072.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 31630392832.0,
        "latency_per_micro_batch": 0.16046159166566618,
        "latency_fwd": 0.05348719722188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 20.547971168484,
        "device_tokens_per_sec": 199.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31299390464.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 31928577024.0,
        "latency_per_micro_batch": 0.13547135921385295,
        "latency_fwd": 0.04515711973795098,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.007864576,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 17.348851713600194,
        "device_tokens_per_sec": 236.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43456661504.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 44714993664.0,
        "latency_per_micro_batch": 0.13085762481385294,
        "latency_fwd": 0.04361920827128431,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.009175338666666666,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 16.766811388826437,
        "device_tokens_per_sec": 122.15,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 20,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 906261760.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 288358400.0,
        "weight_memory_mlp_per_gpu": 576716800.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 1812129280.0,
        "optimizer_state_memory_per_gpu": 5436387840.0,
        "(weight+op_state)_memory_per_gpu": 6342649600.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 67763250944.0,
        "(weight+op_state+grad)_memory_per_gpu": 8154778880.0,
        "estimated_peak_memory_per_gpu": 67920578304.0,
        "latency_per_micro_batch": 0.36500907589723064,
        "latency_fwd": 0.12166969196574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 23.36502463644298,
        "device_tokens_per_sec": 701.22,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 18.23213617152,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 47143022592.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 47667310592.0,
        "latency_per_micro_batch": 0.07226089193957622,
        "latency_fwd": 0.02408696397985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.251024436976998,
        "device_tokens_per_sec": 1771.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56549849088.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 57074137088.0,
        "latency_per_micro_batch": 1.2181970631714834,
        "latency_fwd": 0.4060656877238278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 9.753797388968401,
        "device_tokens_per_sec": 3359.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 27061821440.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 27867160576.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 8.792113013916264,
        "device_tokens_per_sec": 465.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 30830784512.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 31233470464.0,
        "latency_per_micro_batch": 0.14245432181918838,
        "latency_fwd": 0.04748477393972946,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.133518220771144,
        "device_tokens_per_sec": 224.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17335728128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17537087488.0,
        "latency_per_micro_batch": 0.15264961409637,
        "latency_fwd": 0.050883204698790006,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.777796185764215,
        "device_tokens_per_sec": 418.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44782336000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 44983695360.0,
        "latency_per_micro_batch": 0.6188382169909534,
        "latency_fwd": 0.20627940566365113,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.917531816547097,
        "device_tokens_per_sec": 826.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24313847808.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 25119186944.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 8.743183136708481,
        "device_tokens_per_sec": 234.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 35588595712.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 36393934848.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 8.72365529432823,
        "device_tokens_per_sec": 234.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45416724480.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 45678868480.0,
        "latency_per_micro_batch": 1.2161837972514835,
        "latency_fwd": 0.4053945990838278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 9.74559072270371,
        "device_tokens_per_sec": 1681.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 16983078912.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17184438272.0,
        "latency_per_micro_batch": 0.07810724568380378,
        "latency_fwd": 0.02603574856126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 10.001998600575764,
        "device_tokens_per_sec": 409.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17758973952.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 18161659904.0,
        "latency_per_micro_batch": 0.07125425897957621,
        "latency_fwd": 0.02375141965985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.125137689223367,
        "device_tokens_per_sec": 448.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 62645235712.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 63169523712.0,
        "latency_per_micro_batch": 0.14362872693918838,
        "latency_fwd": 0.0478762423130628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.194856216528088,
        "device_tokens_per_sec": 1781.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 53380411392.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2843123215355944,
        "latency_fwd": 0.09477077384519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.130234692822743,
        "device_tokens_per_sec": 224.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13072168960.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 13273528320.0,
        "latency_per_micro_batch": 0.039364619821179316,
        "latency_fwd": 0.013121539940393105,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 10.078651529360044,
        "device_tokens_per_sec": 812.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 83237673984.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 83499817984.0,
        "latency_per_micro_batch": 1.4398163794893124,
        "latency_fwd": 0.4799387931631041,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.01872363270012533,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 11.526591351110401,
        "device_tokens_per_sec": 2842.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 9338253312.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 9740939264.0,
        "latency_per_micro_batch": 0.01821886328198086,
        "latency_fwd": 0.0060729544273269535,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 9.330675692794232,
        "device_tokens_per_sec": 219.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 41733528576.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 41864600576.0,
        "latency_per_micro_batch": 0.3614499124494149,
        "latency_fwd": 0.12048330414980496,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 11.570507711604535,
        "device_tokens_per_sec": 1416.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17794363392.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 18197049344.0,
        "latency_per_micro_batch": 0.07108648681957622,
        "latency_fwd": 0.023695495606525407,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.107612547578526,
        "device_tokens_per_sec": 224.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17054134272.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 17456820224.0,
        "latency_per_micro_batch": 0.018496344227333157,
        "latency_fwd": 0.006165448075777719,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 9.471758513105819,
        "device_tokens_per_sec": 432.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 4448689152.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 4549385216.0,
        "latency_per_micro_batch": 0.02237122424198086,
        "latency_fwd": 0.007457074747326953,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 11.456202424131137,
        "device_tokens_per_sec": 178.77,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7531502592.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 7632198656.0,
        "latency_per_micro_batch": 0.023139185739446937,
        "latency_fwd": 0.007713061913148979,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 11.849398710833768,
        "device_tokens_per_sec": 172.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14305598464.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.17420833665637,
        "latency_fwd": 0.05806944555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.157393861203582,
        "device_tokens_per_sec": 367.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41646845952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 42049531904.0,
        "latency_per_micro_batch": 0.28481563801559434,
        "latency_fwd": 0.09493854600519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.12264265117179,
        "device_tokens_per_sec": 897.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 20471225344.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 20571921408.0,
        "latency_per_micro_batch": 0.18035202863609864,
        "latency_fwd": 0.06011734287869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.550590147906215,
        "device_tokens_per_sec": 354.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33190735872.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 33593421824.0,
        "latency_per_micro_batch": 0.0720286384409854,
        "latency_fwd": 0.024009546146995134,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.222283412866163,
        "device_tokens_per_sec": 888.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17177131008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17378490368.0,
        "latency_per_micro_batch": 0.07785558744380378,
        "latency_fwd": 0.025951862481267927,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.973736076403418,
        "device_tokens_per_sec": 205.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13039099904.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 13844439040.0,
        "latency_per_micro_batch": 0.0005898693273977942,
        "latency_fwd": 0.00019662310913259808,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 0.6053118812602457,
        "device_tokens_per_sec": 3383.38,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24046853120.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 24308997120.0,
        "latency_per_micro_batch": 0.17831875457637003,
        "latency_fwd": 0.05943958485879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.413548454631457,
        "device_tokens_per_sec": 2870.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 68424855552.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 68827541504.0,
        "latency_per_micro_batch": 1.1289422740514832,
        "latency_fwd": 0.37631409135049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 9.063778596095588,
        "device_tokens_per_sec": 903.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8897247232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 9098606592.0,
        "latency_per_micro_batch": 0.03860964510117931,
        "latency_fwd": 0.012869881700393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.888340298950784,
        "device_tokens_per_sec": 207.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38914726912.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 39045798912.0,
        "latency_per_micro_batch": 0.18211363631609864,
        "latency_fwd": 0.06070454543869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.657408336467249,
        "device_tokens_per_sec": 1405.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 51371077632.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 51895365632.0,
        "latency_per_micro_batch": 0.2837313860899576,
        "latency_fwd": 0.09457712869665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.083996894716256,
        "device_tokens_per_sec": 1803.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25051183104.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 25252542464.0,
        "latency_per_micro_batch": 0.07861056216380378,
        "latency_fwd": 0.02620352072126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 10.064448244741937,
        "device_tokens_per_sec": 813.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19572055040.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 19672751104.0,
        "latency_per_micro_batch": 0.04643702409611147,
        "latency_fwd": 0.015479008032037157,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 11.889026330348312,
        "device_tokens_per_sec": 689.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 7240801280.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 7341497344.0,
        "latency_per_micro_batch": 0.044901101101179314,
        "latency_fwd": 0.014967033700393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 11.495830043645679,
        "device_tokens_per_sec": 712.61,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 50912376832.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 51436664832.0,
        "latency_per_micro_batch": 0.6113260724771329,
        "latency_fwd": 0.20377535749237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.785488312683006,
        "device_tokens_per_sec": 3348.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45875228672.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 46277914624.0,
        "latency_per_micro_batch": 0.5650209563171329,
        "latency_fwd": 0.1883403187723776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.056776925417214,
        "device_tokens_per_sec": 904.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 46684272640.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 47208560640.0,
        "latency_per_micro_batch": 0.15617282945637,
        "latency_fwd": 0.052057609818790006,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.99636994034582,
        "device_tokens_per_sec": 3277.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33912658944.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 34174802944.0,
        "latency_per_micro_batch": 1.3906668436514833,
        "latency_fwd": 0.46355561455049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 11.133395064407768,
        "device_tokens_per_sec": 2943.22,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23641815040.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 23843174400.0,
        "latency_per_micro_batch": 0.0400228725332931,
        "latency_fwd": 0.0133409575110977,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 10.247164223661173,
        "device_tokens_per_sec": 799.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17071021056.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 17202093056.0,
        "latency_per_micro_batch": 0.3491625284899576,
        "latency_fwd": 0.11638750949665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 11.177311424901902,
        "device_tokens_per_sec": 1465.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 36009439232.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 36412125184.0,
        "latency_per_micro_batch": 0.1429576382991884,
        "latency_fwd": 0.047652546099729465,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.153881390985669,
        "device_tokens_per_sec": 894.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11698255872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11899615232.0,
        "latency_per_micro_batch": 0.07679074025957622,
        "latency_fwd": 0.02559691341985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.833485906274635,
        "device_tokens_per_sec": 416.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24243134464.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 25048473600.0,
        "latency_per_micro_batch": 0.000751188712013179,
        "latency_fwd": 0.0002503962373377263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 0.385894310555652,
        "device_tokens_per_sec": 10614.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11469003776.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 11569699840.0,
        "latency_per_micro_batch": 0.17479553921637,
        "latency_fwd": 0.05826517973879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.191025023070939,
        "device_tokens_per_sec": 732.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 23800257536.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 23900953600.0,
        "latency_per_micro_batch": 0.18093923119609864,
        "latency_fwd": 0.06031307706536621,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.584221309773572,
        "device_tokens_per_sec": 707.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34212689920.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34414049280.0,
        "latency_per_micro_batch": 0.6083061735971329,
        "latency_fwd": 0.20276872453237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.749019122245969,
        "device_tokens_per_sec": 840.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22620551168.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 22821910528.0,
        "latency_per_micro_batch": 0.15528262494482514,
        "latency_fwd": 0.05176087498160838,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.946308880065343,
        "device_tokens_per_sec": 411.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 12014820352.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 12115516416.0,
        "latency_per_micro_batch": 0.04584982153611147,
        "latency_fwd": 0.015283273845370489,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 11.739689925481471,
        "device_tokens_per_sec": 348.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 5849193472.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 5949889536.0,
        "latency_per_micro_batch": 0.04431389854117931,
        "latency_fwd": 0.014771299513726436,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 11.346493638778838,
        "device_tokens_per_sec": 360.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 15987907584.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 16088603648.0,
        "latency_per_micro_batch": 0.11476018533970547,
        "latency_fwd": 0.03825339511323516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 14.697364038678202,
        "device_tokens_per_sec": 139.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 12905094144.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.11476018533970547,
        "latency_fwd": 0.03825339511323516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 14.697364038678202,
        "device_tokens_per_sec": 139.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24734953472.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 25137639424.0,
        "latency_per_micro_batch": 0.03634472094117931,
        "latency_fwd": 0.012114906980393103,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.305878829653146,
        "device_tokens_per_sec": 880.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23396380672.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 23799066624.0,
        "latency_per_micro_batch": 0.14174442369637003,
        "latency_fwd": 0.04724814123212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.080185351240452,
        "device_tokens_per_sec": 451.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 9690509312.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 10093195264.0,
        "latency_per_micro_batch": 0.009462447085057783,
        "latency_fwd": 0.0031541490283525943,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00011184810666666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 9.691176083810412,
        "device_tokens_per_sec": 211.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14252219392.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 14383291392.0,
        "latency_per_micro_batch": 0.17596994433637,
        "latency_fwd": 0.05865664811212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.264212049764616,
        "device_tokens_per_sec": 1454.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14940270592.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 15342956544.0,
        "latency_per_micro_batch": 0.03600917662117931,
        "latency_fwd": 0.012003058873726437,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.220966907441936,
        "device_tokens_per_sec": 444.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57008484352.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 57532772352.0,
        "latency_per_micro_batch": 0.5656920449571329,
        "latency_fwd": 0.18856401498571096,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.059614953986896,
        "device_tokens_per_sec": 1808.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 52288577536.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 53093916672.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 8.711806263391496,
        "device_tokens_per_sec": 940.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 60744036352.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 61146722304.0,
        "latency_per_micro_batch": 0.5681960931284062,
        "latency_fwd": 0.18939869770946877,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.123377893738223,
        "device_tokens_per_sec": 448.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14481537024.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 14682896384.0,
        "latency_per_micro_batch": 0.07729405673957622,
        "latency_fwd": 0.02576468557985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.895935550440809,
        "device_tokens_per_sec": 827.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46792630272.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 47597969408.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 8.709941696159191,
        "device_tokens_per_sec": 470.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12156956672.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 12559642624.0,
        "latency_per_micro_batch": 0.03584140446117931,
        "latency_fwd": 0.011947134820393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.179992081899515,
        "device_tokens_per_sec": 223.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11539658752.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11741018112.0,
        "latency_per_micro_batch": 0.039267897813293096,
        "latency_fwd": 0.013089299271097699,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 10.056852993251912,
        "device_tokens_per_sec": 203.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 35517882368.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 36323221504.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 8.715755940370409,
        "device_tokens_per_sec": 469.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 48093640704.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 48617928704.0,
        "latency_per_micro_batch": 0.30789057712995765,
        "latency_fwd": 0.10263019237665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.854794755933698,
        "device_tokens_per_sec": 3325.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 66556016640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 66818160640.0,
        "latency_per_micro_batch": 1.2372478840391246,
        "latency_fwd": 0.4124159613463748,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 9.914103417004839,
        "device_tokens_per_sec": 1652.6,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 73371868160.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 73634012160.0,
        "latency_per_micro_batch": 0.18446244655609867,
        "latency_fwd": 0.06148748218536622,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.80674474133409,
        "device_tokens_per_sec": 2775.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38194409472.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 38597095424.0,
        "latency_per_micro_batch": 0.28448009369559435,
        "latency_fwd": 0.09482669789853146,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.119804622602107,
        "device_tokens_per_sec": 449.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14534719488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 15287502848.0,
        "latency_per_micro_batch": 0.07653908201957621,
        "latency_fwd": 0.02551302733985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.80522338210229,
        "device_tokens_per_sec": 208.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 15857786880.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 16663126016.0,
        "latency_per_micro_batch": 0.01738000248198086,
        "latency_fwd": 0.005793334160660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 8.90182179926856,
        "device_tokens_per_sec": 230.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49643599872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 49905743872.0,
        "latency_per_micro_batch": 0.3111433329068679,
        "latency_fwd": 0.10371444430228931,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.960857806068653,
        "device_tokens_per_sec": 1644.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 28452075520.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.18446262393662857,
        "latency_fwd": 0.06148754131220952,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.82172827663607,
        "device_tokens_per_sec": 173.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 52921659392.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 53324345344.0,
        "latency_per_micro_batch": 0.5685316374484063,
        "latency_fwd": 0.18951054581613544,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.112947823517588,
        "device_tokens_per_sec": 898.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28275055616.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 28537199616.0,
        "latency_per_micro_batch": 0.6978965070371328,
        "latency_fwd": 0.2326321690123776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 11.170454625817383,
        "device_tokens_per_sec": 2933.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22708624384.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 22839696384.0,
        "latency_per_micro_batch": 0.6955476967971328,
        "latency_fwd": 0.23184923226571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 11.136823463950027,
        "device_tokens_per_sec": 1471.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 69200699392.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 70006038528.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0151001088,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 8.703084897074675,
        "device_tokens_per_sec": 941.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45415495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 45677639680.0,
        "latency_per_micro_batch": 0.07961719512380377,
        "latency_fwd": 0.026539065041267927,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 10.192309830985023,
        "device_tokens_per_sec": 1607.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57925951488.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 58731290624.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.004697811626666667,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 8.70204234220137,
        "device_tokens_per_sec": 941.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 4669143040.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 4870502400.0,
        "latency_per_micro_batch": 0.010162567412381637,
        "latency_fwd": 0.0033875224707938787,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.00033554432,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 10.407777885416936,
        "device_tokens_per_sec": 196.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8879519744.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 9080879104.0,
        "latency_per_micro_batch": 0.038861303341179314,
        "latency_fwd": 0.012953767780393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.950789943116957,
        "device_tokens_per_sec": 411.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 65463939072.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 65988227072.0,
        "latency_per_micro_batch": 0.2854867266555944,
        "latency_fwd": 0.09516224221853145,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.140167792816632,
        "device_tokens_per_sec": 1792.53,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 29880508416.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 30685847552.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 8.739233459729569,
        "device_tokens_per_sec": 468.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75471286272.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 75873972224.0,
        "latency_per_micro_batch": 1.1359636363140302,
        "latency_fwd": 0.3786545454380101,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 9.119949494195964,
        "device_tokens_per_sec": 898.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 69342126080.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 70147465216.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 8.718883604990321,
        "device_tokens_per_sec": 469.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34600415232.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 35003101184.0,
        "latency_per_micro_batch": 0.28306029744995764,
        "latency_fwd": 0.0943534324833192,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.066471753071415,
        "device_tokens_per_sec": 903.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 26144305152.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 26546991104.0,
        "latency_per_micro_batch": 0.07158980329957622,
        "latency_fwd": 0.023863267766525406,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.166112514765787,
        "device_tokens_per_sec": 893.73,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 33507391488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 33708750848.0,
        "latency_per_micro_batch": 0.3101366999468679,
        "latency_fwd": 0.1033788999822893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.932595281896306,
        "device_tokens_per_sec": 824.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 28610672640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.30436736176995766,
        "latency_fwd": 0.10145578725665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.755875921330487,
        "device_tokens_per_sec": 419.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8667995136.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 8768691200.0,
        "latency_per_micro_batch": 0.0876120445795762,
        "latency_fwd": 0.029204014859858737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.218452219409013,
        "device_tokens_per_sec": 365.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14833622016.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 14934318080.0,
        "latency_per_micro_batch": 0.09068389056944053,
        "latency_fwd": 0.030227963523146842,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.611648506111646,
        "device_tokens_per_sec": 352.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11099860992.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 11502546944.0,
        "latency_per_micro_batch": 0.018328572067333158,
        "latency_fwd": 0.006109524022444386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 9.386846590894608,
        "device_tokens_per_sec": 218.18,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 13530918912.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 13933604864.0,
        "latency_per_micro_batch": 0.01838663544198086,
        "latency_fwd": 0.006128878480660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 9.415587615005443,
        "device_tokens_per_sec": 435.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19555971072.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 19958657024.0,
        "latency_per_micro_batch": 0.0715253219609854,
        "latency_fwd": 0.023841773986995135,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.163783445678902,
        "device_tokens_per_sec": 223.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 20981455872.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 21082151936.0,
        "latency_per_micro_batch": 0.09127109312944053,
        "latency_fwd": 0.03042369770981351,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.684835532805323,
        "device_tokens_per_sec": 701.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 61235884032.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 61760172032.0,
        "latency_per_micro_batch": 0.0726997270809854,
        "latency_fwd": 0.024233242360328467,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.307195335077374,
        "device_tokens_per_sec": 1760.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8650202112.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 8750898176.0,
        "latency_per_micro_batch": 0.08819924713957622,
        "latency_fwd": 0.029399749046525404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.291639246102692,
        "device_tokens_per_sec": 725.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17300273152.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17501632512.0,
        "latency_per_micro_batch": 0.15315293057637003,
        "latency_fwd": 0.05105097685879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.806058709936561,
        "device_tokens_per_sec": 835.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 51618803712.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2825569809699576,
        "latency_fwd": 0.09418566032331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.074063794722367,
        "device_tokens_per_sec": 225.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 64,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 262152192.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 0.0,
        "weight_memory_mlp_per_gpu": 0.0,
        "weight_memory_layernorm_per_gpu": 0.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 524304384.0,
        "optimizer_state_memory_per_gpu": 1572913152.0,
        "(weight+op_state)_memory_per_gpu": 1835065344.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 46651203584.0,
        "(weight+op_state+grad)_memory_per_gpu": 2359369728.0,
        "estimated_peak_memory_per_gpu": 47456542720.0,
        "latency_per_micro_batch": 0.0010738274812439482,
        "latency_fwd": 0.0003579424937479827,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.0,
        "latency_fwd_mlp": 0.0,
        "latency_fwd_layernorm": 0.0,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 0.2761855252033551,
        "device_tokens_per_sec": 29661.22,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 25456253952.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 25718397952.0,
        "latency_per_micro_batch": 0.35151133872995766,
        "latency_fwd": 0.11717044624331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 11.25049845159558,
        "device_tokens_per_sec": 2912.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12754974720.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 12956334080.0,
        "latency_per_micro_batch": 0.020225711238037753,
        "latency_fwd": 0.006741903746012585,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 10.35687300901347,
        "device_tokens_per_sec": 395.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 3039288320.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 3139984384.0,
        "latency_per_micro_batch": 0.011546687732381634,
        "latency_fwd": 0.0038488959107938784,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.0007829367466666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 11.824956399702568,
        "device_tokens_per_sec": 173.19,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 6122101760.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 6222797824.0,
        "latency_per_micro_batch": 0.011930668481114674,
        "latency_fwd": 0.003976889493704891,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0007829367466666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 12.218152686405201,
        "device_tokens_per_sec": 167.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8720922624.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 8922281984.0,
        "latency_per_micro_batch": 0.019974052998037752,
        "latency_fwd": 0.006658017666012584,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 10.229011422770382,
        "device_tokens_per_sec": 200.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21282189312.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 21684875264.0,
        "latency_per_micro_batch": 0.0716930941209854,
        "latency_fwd": 0.023897698040328468,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 9.181308587323743,
        "device_tokens_per_sec": 446.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 37505326080.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 37636398080.0,
        "latency_per_micro_batch": 0.09244549824944052,
        "latency_fwd": 0.03081516608314684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.834171937672162,
        "device_tokens_per_sec": 1384.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55281072128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 55543216128.0,
        "latency_per_micro_batch": 0.6198448499509533,
        "latency_fwd": 0.20661494998365112,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.925738482811788,
        "device_tokens_per_sec": 1650.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 48552374272.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 49076662272.0,
        "latency_per_micro_batch": 0.14275105665637003,
        "latency_fwd": 0.04758368555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.138685318427713,
        "device_tokens_per_sec": 1792.82,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 25685571584.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 25947715584.0,
        "latency_per_micro_batch": 0.15415956353637003,
        "latency_fwd": 0.05138652117879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.868508354102735,
        "device_tokens_per_sec": 1660.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29069176832.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 30574710784.0,
        "latency_per_micro_batch": 0.14157665153637,
        "latency_fwd": 0.047192217178790005,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.077347322670768,
        "device_tokens_per_sec": 225.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46824863744.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 47087007744.0,
        "latency_per_micro_batch": 0.15679257438482516,
        "latency_fwd": 0.052264191461608385,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 10.037021048403863,
        "device_tokens_per_sec": 1632.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 4,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 67824793600.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 68349081600.0,
        "latency_per_micro_batch": 2.4319390445601843,
        "latency_fwd": 0.8106463481867281,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.08589934592,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 9.74387652293258,
        "device_tokens_per_sec": 3362.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34671194112.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 35073880064.0,
        "latency_per_micro_batch": 0.28272475312995765,
        "latency_fwd": 0.09424158437665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.063633724501733,
        "device_tokens_per_sec": 451.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 13918564352.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 14321250304.0,
        "latency_per_micro_batch": 0.036060822031883905,
        "latency_fwd": 0.012020274010627968,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.236162979999891,
        "device_tokens_per_sec": 221.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 14164342784.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 14365702144.0,
        "latency_per_micro_batch": 0.039519556053293096,
        "latency_fwd": 0.013173185351097698,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 10.119302637418086,
        "device_tokens_per_sec": 404.77,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34141779968.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 34403923968.0,
        "latency_per_micro_batch": 0.6093128065571328,
        "latency_fwd": 0.20310426885237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.75722578851066,
        "device_tokens_per_sec": 1679.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 7311554560.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 7512913920.0,
        "latency_per_micro_batch": 0.010327130590410082,
        "latency_fwd": 0.0034423768634700273,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00033554432,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 10.576290579718064,
        "device_tokens_per_sec": 193.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 71101345792.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 71625633792.0,
        "latency_per_micro_batch": 0.5692027260884064,
        "latency_fwd": 0.18973424202946879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.115785852087273,
        "device_tokens_per_sec": 1797.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 37,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 10605419520.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 10706115584.0,
        "latency_per_micro_batch": 0.02343278701944694,
        "latency_fwd": 0.007810929006482313,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 11.998735115700608,
        "device_tokens_per_sec": 341.37,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 4439792640.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 4540488704.0,
        "latency_per_micro_batch": 0.02266482552198086,
        "latency_fwd": 0.007554941840660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 11.605538828997975,
        "device_tokens_per_sec": 352.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 77600070656.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 77862214656.0,
        "latency_per_micro_batch": 0.7224712749560473,
        "latency_fwd": 0.24082375831868244,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 11.563650912520016,
        "device_tokens_per_sec": 2833.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 47371131904.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 47502203904.0,
        "latency_per_micro_batch": 0.7201224647160474,
        "latency_fwd": 0.24004082157201578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 11.53001975065266,
        "device_tokens_per_sec": 1420.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 57220820992.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.5646854119971328,
        "latency_fwd": 0.18822847066571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.062413783040000004,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 9.067206995637848,
        "device_tokens_per_sec": 451.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 7928901632.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 8331587584.0,
        "latency_per_micro_batch": 0.009407592692381634,
        "latency_fwd": 0.0031358642307938783,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.000991146299076923,
        "latency_fwd_mlp": 0.0017620378650256411,
        "latency_fwd_layernorm": 1.8284797558716142e-05,
        "latency_fwd_tp_comm": 0.00011184810666666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 5.3773128205128203e-05,
        "latency_per_iter": 9.635005185710035,
        "device_tokens_per_sec": 212.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 26919596032.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 27322281984.0,
        "latency_per_micro_batch": 0.1426220939791884,
        "latency_fwd": 0.0475406979930628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.136356249340828,
        "device_tokens_per_sec": 448.32,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17106607104.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 17207303168.0,
        "latency_per_micro_batch": 0.34798812336995766,
        "latency_fwd": 0.11599604112331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 11.143680263034547,
        "device_tokens_per_sec": 735.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29437860864.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 29538556928.0,
        "latency_per_micro_batch": 0.3602755073294149,
        "latency_fwd": 0.12009183577647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 11.536876549737178,
        "device_tokens_per_sec": 710.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 25809664000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.18446262393662857,
        "latency_fwd": 0.06148754131220952,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 11.82172827663607,
        "device_tokens_per_sec": 173.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18463485952.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 18866171904.0,
        "latency_per_micro_batch": 0.0362285941918839,
        "latency_fwd": 0.012076198063961301,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.27713780554231,
        "device_tokens_per_sec": 441.52,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 10350304256.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 10451000320.0,
        "latency_per_micro_batch": 0.049687776921243945,
        "latency_fwd": 0.016562592307081316,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 12.724181405061708,
        "device_tokens_per_sec": 160.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7267490816.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 7643898880.0,
        "latency_per_micro_batch": 0.049687776921243945,
        "latency_fwd": 0.016562592307081316,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 12.724181405061708,
        "device_tokens_per_sec": 160.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 68283297792.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 68807585792.0,
        "latency_per_micro_batch": 1.1296133626914833,
        "latency_fwd": 0.3765377875638278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 9.053348525874954,
        "device_tokens_per_sec": 1809.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 332419072.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 664821760.0,
        "optimizer_state_memory_per_gpu": 1994465280.0,
        "(weight+op_state)_memory_per_gpu": 2326884352.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 31781384192.0,
        "(weight+op_state+grad)_memory_per_gpu": 2991706112.0,
        "estimated_peak_memory_per_gpu": 32184070144.0,
        "latency_per_micro_batch": 0.036564138511883906,
        "latency_fwd": 0.012188046170627968,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 9.362049727753522,
        "device_tokens_per_sec": 875.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 32,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 58138091520.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 61149126656.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.031206891520000002,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 8.725740404074838,
        "device_tokens_per_sec": 234.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 118,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 8,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 234161152.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 468207616.0,
        "optimizer_state_memory_per_gpu": 1404622848.0,
        "(weight+op_state)_memory_per_gpu": 1638784000.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 12842818560.0,
        "(weight+op_state+grad)_memory_per_gpu": 2106991616.0,
        "estimated_peak_memory_per_gpu": 12973890560.0,
        "latency_per_micro_batch": 0.0893736522595762,
        "latency_fwd": 0.029791217419858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 11.440975650969529,
        "device_tokens_per_sec": 1432.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 6078511104.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 6279870464.0,
        "latency_per_micro_batch": 0.01964492664198086,
        "latency_fwd": 0.006548308880660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 10.060498728469254,
        "device_tokens_per_sec": 203.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18676473856.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 19481812992.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00201334784,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 8.794087852405719,
        "device_tokens_per_sec": 232.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 7470151680.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 7671511040.0,
        "latency_per_micro_batch": 0.019896584881980862,
        "latency_fwd": 0.0066321949606602875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 10.188360314712341,
        "device_tokens_per_sec": 402.03,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 27869919232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28071278592.0,
        "latency_per_micro_batch": 0.15578594142482516,
        "latency_fwd": 0.051928647141608386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.97457140423769,
        "device_tokens_per_sec": 821.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28963008512.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 29365694464.0,
        "latency_per_micro_batch": 0.14207996801637002,
        "latency_fwd": 0.04735998933879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 9.097710492885293,
        "device_tokens_per_sec": 900.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 59,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 16,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 266897408.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 67108864.0,
        "weight_memory_mlp_per_gpu": 134217728.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 533745664.0,
        "optimizer_state_memory_per_gpu": 1601236992.0,
        "(weight+op_state)_memory_per_gpu": 1868134400.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 24276203520.0,
        "(weight+op_state+grad)_memory_per_gpu": 2401880064.0,
        "estimated_peak_memory_per_gpu": 24538347520.0,
        "latency_per_micro_batch": 0.07830068969957621,
        "latency_fwd": 0.02610022989985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 10.023797136683894,
        "device_tokens_per_sec": 1634.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 49469890560.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 50275229696.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0006711159466666666,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 8.737258621240114,
        "device_tokens_per_sec": 937.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28504307712.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28766451712.0,
        "latency_per_micro_batch": 0.30587731120995765,
        "latency_fwd": 0.10195910373665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.792345111767524,
        "device_tokens_per_sec": 1673.14,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33895495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34096855040.0,
        "latency_per_micro_batch": 0.3096333834668679,
        "latency_fwd": 0.1032111278222893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.924388615631615,
        "device_tokens_per_sec": 412.72,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 74781268992.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 75043412992.0,
        "latency_per_micro_batch": 0.3637987226894149,
        "latency_fwd": 0.12126624089647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 11.643694738298212,
        "device_tokens_per_sec": 2814.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 64,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22937745408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 23139104768.0,
        "latency_per_micro_batch": 0.3048706782499576,
        "latency_fwd": 0.10162355941665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 9.764082587595178,
        "device_tokens_per_sec": 838.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 8.622972801969231,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 68424855552.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 68827541504.0,
        "latency_per_micro_batch": 1.1289422740514832,
        "latency_fwd": 0.37631409135049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 18.095316788507454,
        "device_tokens_per_sec": 452.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57925951488.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 58731290624.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 17.394899640440013,
        "device_tokens_per_sec": 470.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38194409472.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 38597095424.0,
        "latency_per_micro_batch": 0.28448009369559435,
        "latency_fwd": 0.09482669789853146,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.223167620861126,
        "device_tokens_per_sec": 224.77,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 28610672640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.30436736176995766,
        "latency_fwd": 0.10145578725665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.495631497969132,
        "device_tokens_per_sec": 210.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8897247232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 9098606592.0,
        "latency_per_micro_batch": 0.03860964510117931,
        "latency_fwd": 0.012869881700393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 19.772409444852688,
        "device_tokens_per_sec": 103.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18463485952.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 18866171904.0,
        "latency_per_micro_batch": 0.0362285941918839,
        "latency_fwd": 0.012076198063961301,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 18.55165791866459,
        "device_tokens_per_sec": 220.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 29880508416.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 30685847552.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 17.473231552475323,
        "device_tokens_per_sec": 234.42,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28504307712.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28766451712.0,
        "latency_per_micro_batch": 0.30587731120995765,
        "latency_fwd": 0.10195910373665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.58041907048617,
        "device_tokens_per_sec": 836.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2690367514214400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 47864323072.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 48388611072.0,
        "latency_per_micro_batch": 0.7025941275171328,
        "latency_fwd": 0.23419804250571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0032745969675941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 22.485147692785183,
        "device_tokens_per_sec": 2914.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34600415232.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 35003101184.0,
        "latency_per_micro_batch": 0.28306029744995764,
        "latency_fwd": 0.0943534324833192,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.12440127147006,
        "device_tokens_per_sec": 451.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38914726912.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 39045798912.0,
        "latency_per_micro_batch": 0.18211363631609864,
        "latency_fwd": 0.06070454543869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 23.31268106069756,
        "device_tokens_per_sec": 702.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14481537024.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 14682896384.0,
        "latency_per_micro_batch": 0.07729405673957622,
        "latency_fwd": 0.02576468557985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 19.789574813106565,
        "device_tokens_per_sec": 413.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 47371131904.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 47502203904.0,
        "latency_per_micro_batch": 0.7201224647160474,
        "latency_fwd": 0.24004082157201578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 23.051979186109417,
        "device_tokens_per_sec": 710.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 28452075520.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.15503096670482516,
        "latency_fwd": 0.051676988901608385,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.860084082909463,
        "device_tokens_per_sec": 103.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19555971072.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 19958657024.0,
        "latency_per_micro_batch": 0.0715253219609854,
        "latency_fwd": 0.023841773986995135,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 18.319024656685034,
        "device_tokens_per_sec": 111.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17758973952.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 18161659904.0,
        "latency_per_micro_batch": 0.07125425897957621,
        "latency_fwd": 0.02375141965985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 18.24568283860912,
        "device_tokens_per_sec": 224.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 16983078912.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17184438272.0,
        "latency_per_micro_batch": 0.07810724568380378,
        "latency_fwd": 0.02603574856126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 19.999726048102648,
        "device_tokens_per_sec": 204.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 49469890560.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 50275229696.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 17.471256713985866,
        "device_tokens_per_sec": 468.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 12014820352.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 12115516416.0,
        "latency_per_micro_batch": 0.04584982153611147,
        "latency_fwd": 0.015283273845370489,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 23.477244238726005,
        "device_tokens_per_sec": 174.47,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 5849193472.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 5949889536.0,
        "latency_per_micro_batch": 0.04431389854117931,
        "latency_fwd": 0.014771299513726436,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 22.69085166532074,
        "device_tokens_per_sec": 180.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 33507391488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 33708750848.0,
        "latency_per_micro_batch": 0.3101366999468679,
        "latency_fwd": 0.1033788999822893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.856969680196077,
        "device_tokens_per_sec": 412.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2690367514214400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56320728064.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 56845016064.0,
        "latency_per_micro_batch": 2.7809051373601843,
        "latency_fwd": 0.9269683791200614,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.20043180714666667,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0032745969675941366,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 22.255301414077376,
        "device_tokens_per_sec": 2944.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 77600070656.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 77862214656.0,
        "latency_per_micro_batch": 0.7224712749560473,
        "latency_fwd": 0.24082375831868244,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 23.123191311816772,
        "device_tokens_per_sec": 1417.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 8,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 67824793600.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 68349081600.0,
        "latency_per_micro_batch": 2.4319390445601843,
        "latency_fwd": 0.8106463481867281,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.08589934592,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 19.471632701173316,
        "device_tokens_per_sec": 1682.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 71101345792.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 71625633792.0,
        "latency_per_micro_batch": 0.5692027260884064,
        "latency_fwd": 0.18973424202946879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 18.223029469501775,
        "device_tokens_per_sec": 899.08,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 53380411392.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2843123215355944,
        "latency_fwd": 0.09477077384519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.228228981961763,
        "device_tokens_per_sec": 112.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34141779968.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 34403923968.0,
        "latency_per_micro_batch": 0.6093128065571328,
        "latency_fwd": 0.20310426885237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 19.506230693424783,
        "device_tokens_per_sec": 839.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 60744036352.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 61146722304.0,
        "latency_per_micro_batch": 0.5681960931284062,
        "latency_fwd": 0.18939869770946877,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 18.214515383792723,
        "device_tokens_per_sec": 224.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45875228672.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 46277914624.0,
        "latency_per_micro_batch": 0.5650209563171329,
        "latency_fwd": 0.1883403187723776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 18.09711222649134,
        "device_tokens_per_sec": 452.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 25051183104.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 25252542464.0,
        "latency_per_micro_batch": 0.07861056216380378,
        "latency_fwd": 0.02620352072126793,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 20.126600201708822,
        "device_tokens_per_sec": 407.02,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 48552374272.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 49076662272.0,
        "latency_per_micro_batch": 0.14275105665637003,
        "latency_fwd": 0.04758368555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.274752944435395,
        "device_tokens_per_sec": 896.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55281072128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 55543216128.0,
        "latency_per_micro_batch": 0.6198448499509533,
        "latency_fwd": 0.20661494998365112,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 19.84325608202704,
        "device_tokens_per_sec": 825.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 26144305152.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 26546991104.0,
        "latency_per_micro_batch": 0.07158980329957622,
        "latency_fwd": 0.023863267766525406,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 18.329607337111543,
        "device_tokens_per_sec": 446.93,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14252219392.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 14383291392.0,
        "latency_per_micro_batch": 0.17596994433637,
        "latency_fwd": 0.05865664811212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 22.526288487292295,
        "device_tokens_per_sec": 727.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 4448689152.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 4549385216.0,
        "latency_per_micro_batch": 0.02237122424198086,
        "latency_fwd": 0.007457074747326953,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 22.910269236025336,
        "device_tokens_per_sec": 89.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7531502592.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 7632198656.0,
        "latency_per_micro_batch": 0.023139185739446937,
        "latency_fwd": 0.007713061913148979,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0015658734933333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 23.696661809430598,
        "device_tokens_per_sec": 86.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33895495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34096855040.0,
        "latency_per_micro_batch": 0.3096333834668679,
        "latency_fwd": 0.1032111278222893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.83265688657139,
        "device_tokens_per_sec": 206.53,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29437860864.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 29538556928.0,
        "latency_per_micro_batch": 0.3602755073294149,
        "latency_fwd": 0.12009183577647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 23.065692784278454,
        "device_tokens_per_sec": 355.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 11539658752.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11741018112.0,
        "latency_per_micro_batch": 0.039267897813293096,
        "latency_fwd": 0.013089299271097699,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 20.109434833454944,
        "device_tokens_per_sec": 101.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 26919596032.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 27322281984.0,
        "latency_per_micro_batch": 0.1426220939791884,
        "latency_fwd": 0.0475406979930628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.264170264008886,
        "device_tokens_per_sec": 224.26,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17106607104.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 17207303168.0,
        "latency_per_micro_batch": 0.34798812336995766,
        "latency_fwd": 0.11599604112331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 22.279300210873192,
        "device_tokens_per_sec": 367.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 12156956672.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 12559642624.0,
        "latency_per_micro_batch": 0.03584140446117931,
        "latency_fwd": 0.011947134820393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 18.355391623961417,
        "device_tokens_per_sec": 111.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 48093640704.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 48617928704.0,
        "latency_per_micro_batch": 0.30789057712995765,
        "latency_fwd": 0.10263019237665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.707293224092343,
        "device_tokens_per_sec": 1662.73,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49643599872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 49905743872.0,
        "latency_per_micro_batch": 0.3111433329068679,
        "latency_fwd": 0.10371444430228931,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.917444459088426,
        "device_tokens_per_sec": 822.6,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 36009439232.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 36412125184.0,
        "latency_per_micro_batch": 0.1429576382991884,
        "latency_fwd": 0.047652546099729465,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.303170242133724,
        "device_tokens_per_sec": 447.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28275055616.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 28537199616.0,
        "latency_per_micro_batch": 0.6978965070371328,
        "latency_fwd": 0.2326321690123776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 22.336798738411506,
        "device_tokens_per_sec": 1467.0,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 69200699392.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 70006038528.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 17.3890853962288,
        "device_tokens_per_sec": 471.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 18676473856.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 19481812992.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 17.582940337827623,
        "device_tokens_per_sec": 116.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11099860992.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 11502546944.0,
        "latency_per_micro_batch": 0.018328572067333158,
        "latency_fwd": 0.006109524022444386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 18.771075489369185,
        "device_tokens_per_sec": 109.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22937745408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 23139104768.0,
        "latency_per_micro_batch": 0.3048706782499576,
        "latency_fwd": 0.10162355941665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 19.51994429159382,
        "device_tokens_per_sec": 419.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 25809664000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.15239795585637003,
        "latency_fwd": 0.05079931861879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.523058694307206,
        "device_tokens_per_sec": 104.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22708624384.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 22839696384.0,
        "latency_per_micro_batch": 0.6955476967971328,
        "latency_fwd": 0.23184923226571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 22.26558661270415,
        "device_tokens_per_sec": 735.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11698255872.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 11899615232.0,
        "latency_per_micro_batch": 0.07679074025957622,
        "latency_fwd": 0.02559691341985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 19.66270065950039,
        "device_tokens_per_sec": 208.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 23800257536.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 23900953600.0,
        "latency_per_micro_batch": 0.18093923119609864,
        "latency_fwd": 0.06031307706536621,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 23.164332106323883,
        "device_tokens_per_sec": 353.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 65463939072.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 65988227072.0,
        "latency_per_micro_batch": 0.2854867266555944,
        "latency_fwd": 0.09516224221853145,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.27574304579565,
        "device_tokens_per_sec": 896.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 11469003776.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 11569699840.0,
        "latency_per_micro_batch": 0.17479553921637,
        "latency_fwd": 0.05826517973879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 22.377939532918617,
        "device_tokens_per_sec": 366.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 30830784512.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 31233470464.0,
        "latency_per_micro_batch": 0.14245432181918838,
        "latency_fwd": 0.04748477393972946,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.2505948171992,
        "device_tokens_per_sec": 112.22,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 25456253952.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 25718397952.0,
        "latency_per_micro_batch": 0.35151133872995766,
        "latency_fwd": 0.11717044624331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 22.498861290954224,
        "device_tokens_per_sec": 1456.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24313847808.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 25119186944.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 17.477181229454235,
        "device_tokens_per_sec": 117.18,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14833622016.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 14934318080.0,
        "latency_per_micro_batch": 0.09068389056944053,
        "latency_fwd": 0.030227963523146842,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 23.21918649900003,
        "device_tokens_per_sec": 176.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 51618803712.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2825569809699576,
        "latency_fwd": 0.09418566032331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.11588718576101,
        "device_tokens_per_sec": 113.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8667995136.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 8768691200.0,
        "latency_per_micro_batch": 0.0876120445795762,
        "latency_fwd": 0.029204014859858737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 22.432793925594765,
        "device_tokens_per_sec": 182.59,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17335728128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17537087488.0,
        "latency_per_micro_batch": 0.15264961409637,
        "latency_fwd": 0.050883204698790006,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.547371487931894,
        "device_tokens_per_sec": 209.54,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17300273152.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 17501632512.0,
        "latency_per_micro_batch": 0.15315293057637003,
        "latency_fwd": 0.05105097685879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.607846266824243,
        "device_tokens_per_sec": 417.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 66556016640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 66818160640.0,
        "latency_per_micro_batch": 1.2372478840391246,
        "latency_fwd": 0.4124159613463748,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 19.812086489317835,
        "device_tokens_per_sec": 826.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 21282189312.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 21684875264.0,
        "latency_per_micro_batch": 0.0716930941209854,
        "latency_fwd": 0.023897698040328468,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 18.358024634809873,
        "device_tokens_per_sec": 223.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17794363392.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 18197049344.0,
        "latency_per_micro_batch": 0.07108648681957622,
        "latency_fwd": 0.023695495606525407,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 18.206682860484282,
        "device_tokens_per_sec": 112.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46792630272.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 47597969408.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 17.402798994397834,
        "device_tokens_per_sec": 235.36,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 25685571584.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 25947715584.0,
        "latency_per_micro_batch": 0.15415956353637003,
        "latency_fwd": 0.05138652117879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.734720420430417,
        "device_tokens_per_sec": 830.21,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 6078511104.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 6279870464.0,
        "latency_per_micro_batch": 0.01964492664198086,
        "latency_fwd": 0.006548308880660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 20.118701169163455,
        "device_tokens_per_sec": 101.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57008484352.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 57532772352.0,
        "latency_per_micro_batch": 0.5656920449571329,
        "latency_fwd": 0.18856401498571096,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 18.110687673301022,
        "device_tokens_per_sec": 904.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 17071021056.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 17202093056.0,
        "latency_per_micro_batch": 0.3491625284899576,
        "latency_fwd": 0.11638750949665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 22.350512336580543,
        "device_tokens_per_sec": 733.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17177131008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17378490368.0,
        "latency_per_micro_batch": 0.07785558744380378,
        "latency_fwd": 0.025951862481267927,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 19.9392512692103,
        "device_tokens_per_sec": 102.71,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 15857786880.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 16663126016.0,
        "latency_per_micro_batch": 0.01738000248198086,
        "latency_fwd": 0.005793334160660287,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 17.80038307004276,
        "device_tokens_per_sec": 115.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 20471225344.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 20571921408.0,
        "latency_per_micro_batch": 0.18035202863609864,
        "latency_fwd": 0.06011734287869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 23.093119980616528,
        "device_tokens_per_sec": 177.37,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 68283297792.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 68807585792.0,
        "latency_per_micro_batch": 1.1296133626914833,
        "latency_fwd": 0.3765377875638278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 18.09025542740682,
        "device_tokens_per_sec": 905.68,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14305598464.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.17420833665637,
        "latency_fwd": 0.05806944555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 22.306727407211262,
        "device_tokens_per_sec": 183.62,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41646845952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 42049531904.0,
        "latency_per_micro_batch": 0.28481563801559434,
        "latency_fwd": 0.09493854600519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.236743067670808,
        "device_tokens_per_sec": 449.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44782336000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 44983695360.0,
        "latency_per_micro_batch": 0.6188382169909534,
        "latency_fwd": 0.20627940566365113,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 19.81894328840235,
        "device_tokens_per_sec": 413.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 69342126080.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 70147465216.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 17.404884104144447,
        "device_tokens_per_sec": 235.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 14940270592.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 15342956544.0,
        "latency_per_micro_batch": 0.03600917662117931,
        "latency_fwd": 0.012003058873726437,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 18.43931612246384,
        "device_tokens_per_sec": 222.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34671194112.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 35073880064.0,
        "latency_per_micro_batch": 0.28272475312995765,
        "latency_fwd": 0.09424158437665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.110825824660377,
        "device_tokens_per_sec": 226.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 14164342784.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 14365702144.0,
        "latency_per_micro_batch": 0.039519556053293096,
        "latency_fwd": 0.013173185351097698,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 20.23630898706112,
        "device_tokens_per_sec": 202.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33912658944.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 34174802944.0,
        "latency_per_micro_batch": 1.3906668436514833,
        "latency_fwd": 0.46355561455049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 22.258729813619635,
        "device_tokens_per_sec": 1472.14,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75471286272.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 75873972224.0,
        "latency_per_micro_batch": 1.1359636363140302,
        "latency_fwd": 0.3786545454380101,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 18.207658584708206,
        "device_tokens_per_sec": 449.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 8720922624.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 8922281984.0,
        "latency_per_micro_batch": 0.019974052998037752,
        "latency_fwd": 0.006658017666012584,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00067108864,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 20.45572655776571,
        "device_tokens_per_sec": 100.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29069176832.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 30574710784.0,
        "latency_per_micro_batch": 0.14157665153637,
        "latency_fwd": 0.047192217178790005,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.138253020998448,
        "device_tokens_per_sec": 112.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 58138091520.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 61149126656.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0302002176,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 17.41859770231348,
        "device_tokens_per_sec": 117.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 28,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 9338253312.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 9740939264.0,
        "latency_per_micro_batch": 0.01821886328198086,
        "latency_fwd": 0.0060729544273269535,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.001982292598153846,
        "latency_fwd_mlp": 0.0035240757300512822,
        "latency_fwd_layernorm": 3.6569595117432285e-05,
        "latency_fwd_tp_comm": 0.00022369621333333332,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00010754625641025641,
        "latency_per_iter": 18.658733693168433,
        "device_tokens_per_sec": 109.76,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 28963008512.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 29365694464.0,
        "latency_per_micro_batch": 0.14207996801637002,
        "latency_fwd": 0.04735998933879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.190828445932972,
        "device_tokens_per_sec": 450.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 12905094144.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.09865340261970548,
        "latency_fwd": 0.032884467539901825,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 25.263331385840505,
        "device_tokens_per_sec": 81.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 74781268992.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 75043412992.0,
        "latency_per_micro_batch": 0.3637987226894149,
        "latency_fwd": 0.12126624089647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 23.285253864359486,
        "device_tokens_per_sec": 1407.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 32,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 664821760.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 16384.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1329643520.0,
        "optimizer_state_memory_per_gpu": 3988930560.0,
        "(weight+op_state)_memory_per_gpu": 4653752320.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 27061821440.0,
        "(weight+op_state+grad)_memory_per_gpu": 5983395840.0,
        "estimated_peak_memory_per_gpu": 27867160576.0,
        "latency_per_micro_batch": 0.03433145502117931,
        "latency_fwd": 0.011443818340393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 17.580965499338166,
        "device_tokens_per_sec": 232.98,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 15987907584.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 16088603648.0,
        "latency_per_micro_batch": 0.09865340261970548,
        "latency_fwd": 0.032884467539901825,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 25.263331385840505,
        "device_tokens_per_sec": 81.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 29,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 27869919232.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 28071278592.0,
        "latency_per_micro_batch": 0.15578594142482516,
        "latency_fwd": 0.051928647141608386,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.9448716554265,
        "device_tokens_per_sec": 410.73,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22620551168.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 22821910528.0,
        "latency_per_micro_batch": 0.15528262494482514,
        "latency_fwd": 0.05176087498160838,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 19.88439687653415,
        "device_tokens_per_sec": 205.99,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 36,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 20981455872.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 21082151936.0,
        "latency_per_micro_batch": 0.09127109312944053,
        "latency_fwd": 0.03042369770981351,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 23.36753545337371,
        "device_tokens_per_sec": 350.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 116,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 4,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 435553280.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 870877184.0,
        "optimizer_state_memory_per_gpu": 2612631552.0,
        "(weight+op_state)_memory_per_gpu": 3048184832.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8650202112.0,
        "(weight+op_state+grad)_memory_per_gpu": 3919062016.0,
        "estimated_peak_memory_per_gpu": 8750898176.0,
        "latency_per_micro_batch": 0.08819924713957622,
        "latency_fwd": 0.029399749046525404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 22.581142879968446,
        "device_tokens_per_sec": 362.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 51371077632.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 51895365632.0,
        "latency_per_micro_batch": 0.2837313860899576,
        "latency_fwd": 0.09457712869665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 18.163401249594898,
        "device_tokens_per_sec": 902.03,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 56,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 870975488.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1741754368.0,
        "optimizer_state_memory_per_gpu": 5225263104.0,
        "(weight+op_state)_memory_per_gpu": 6096238592.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 50912376832.0,
        "(weight+op_state+grad)_memory_per_gpu": 7837992960.0,
        "estimated_peak_memory_per_gpu": 51436664832.0,
        "latency_per_micro_batch": 0.6113260724771329,
        "latency_fwd": 0.20377535749237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 19.566705472317132,
        "device_tokens_per_sec": 1674.68,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 62645235712.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 63169523712.0,
        "latency_per_micro_batch": 0.14362872693918838,
        "latency_fwd": 0.0478762423130628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.387094740636144,
        "device_tokens_per_sec": 891.06,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 58,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 8879519744.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 9080879104.0,
        "latency_per_micro_batch": 0.038861303341179314,
        "latency_fwd": 0.012953767780393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.00134217728,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 19.89928359845886,
        "device_tokens_per_sec": 205.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1067491328.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 2134982656.0,
        "optimizer_state_memory_per_gpu": 6404947968.0,
        "(weight+op_state)_memory_per_gpu": 7472439296.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 52288577536.0,
        "(weight+op_state+grad)_memory_per_gpu": 9607421952.0,
        "estimated_peak_memory_per_gpu": 53093916672.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0013422318933333332,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 17.418377159799178,
        "device_tokens_per_sec": 470.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 83237673984.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 83499817984.0,
        "latency_per_micro_batch": 1.4398163794893124,
        "latency_fwd": 0.4799387931631041,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.01872363270012533,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 23.0451223870249,
        "device_tokens_per_sec": 1421.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 35588595712.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 36393934848.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.009395623253333334,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 17.43022619073591,
        "device_tokens_per_sec": 117.5,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 468256768.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 936415232.0,
        "optimizer_state_memory_per_gpu": 2809245696.0,
        "(weight+op_state)_memory_per_gpu": 3277502464.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46824863744.0,
        "(weight+op_state+grad)_memory_per_gpu": 4213917696.0,
        "estimated_peak_memory_per_gpu": 47087007744.0,
        "latency_per_micro_batch": 0.15679257438482516,
        "latency_fwd": 0.052264191461608385,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 20.071745809032674,
        "device_tokens_per_sec": 816.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 21,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 936448000.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1872830464.0,
        "optimizer_state_memory_per_gpu": 5618491392.0,
        "(weight+op_state)_memory_per_gpu": 6554939392.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 13918564352.0,
        "(weight+op_state+grad)_memory_per_gpu": 8427769856.0,
        "estimated_peak_memory_per_gpu": 14321250304.0,
        "latency_per_micro_batch": 0.036060822031883905,
        "latency_fwd": 0.012020274010627968,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00044739242666666664,
        "latency_fwd_sharded_dp_comm": 0.0026844637866666665,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 18.46773342016217,
        "device_tokens_per_sec": 110.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23396380672.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 23799066624.0,
        "latency_per_micro_batch": 0.14174442369637003,
        "latency_fwd": 0.04724814123212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.00805339136,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 18.151828467808134,
        "device_tokens_per_sec": 225.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56549849088.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 57074137088.0,
        "latency_per_micro_batch": 1.2181970631714834,
        "latency_fwd": 0.4060656877238278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 19.499373894340266,
        "device_tokens_per_sec": 1680.46,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34212689920.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34414049280.0,
        "latency_per_micro_batch": 0.6083061735971329,
        "latency_fwd": 0.20276872453237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 19.481917899800095,
        "device_tokens_per_sec": 420.49,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14534719488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 15287502848.0,
        "latency_per_micro_batch": 0.07653908201957621,
        "latency_fwd": 0.02551302733985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 19.602225880608042,
        "device_tokens_per_sec": 104.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 41733528576.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 41864600576.0,
        "latency_per_micro_batch": 0.3614499124494149,
        "latency_fwd": 0.12048330414980496,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 23.13690490998581,
        "device_tokens_per_sec": 708.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 533762048.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 134217728.0,
        "weight_memory_mlp_per_gpu": 268435456.0,
        "weight_memory_layernorm_per_gpu": 32768.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1067491328.0,
        "optimizer_state_memory_per_gpu": 3202473984.0,
        "(weight+op_state)_memory_per_gpu": 3736236032.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33190735872.0,
        "(weight+op_state+grad)_memory_per_gpu": 4803727360.0,
        "estimated_peak_memory_per_gpu": 33593421824.0,
        "latency_per_micro_batch": 0.0720286384409854,
        "latency_fwd": 0.024009546146995134,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 18.441949133312296,
        "device_tokens_per_sec": 444.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45416724480.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 45678868480.0,
        "latency_per_micro_batch": 1.2161837972514835,
        "latency_fwd": 0.4053945990838278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.037582493013333336,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 19.47506110071558,
        "device_tokens_per_sec": 841.28,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 8,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 52921659392.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 53324345344.0,
        "latency_per_micro_batch": 0.5685316374484063,
        "latency_fwd": 0.18951054581613544,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.018791246506666668,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 18.20945402269209,
        "device_tokens_per_sec": 449.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 7267490816.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 7643898880.0,
        "latency_per_micro_batch": 0.044020297261179314,
        "latency_fwd": 0.014673432420393104,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 7.313919023486457e-05,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 22.542502710947065,
        "device_tokens_per_sec": 90.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 16,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 57220820992.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.5646854119971328,
        "latency_fwd": 0.18822847066571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0604004352,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 18.102173587591974,
        "device_tokens_per_sec": 226.27,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 35,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 10350304256.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 10451000320.0,
        "latency_per_micro_batch": 0.045556220256111474,
        "latency_fwd": 0.015185406752037157,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.003964585196307692,
        "latency_fwd_mlp": 0.0070481514601025645,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0031317469866666667,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00021509251282051281,
        "latency_per_iter": 23.32889528435233,
        "device_tokens_per_sec": 87.79,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 35517882368.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 36323221504.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.00402669568,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 17.42232683677809,
        "device_tokens_per_sec": 235.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 112,
        "gradient_accumulation_steps": 16,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 32,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2690367514214400.0,
        "weight_memory_per_gpu": 838337536.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 268435456.0,
        "weight_memory_mlp_per_gpu": 536870912.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 1676216320.0,
        "optimizer_state_memory_per_gpu": 5028648960.0,
        "(weight+op_state)_memory_per_gpu": 5866986496.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 50683124736.0,
        "(weight+op_state+grad)_memory_per_gpu": 7543202816.0,
        "estimated_peak_memory_per_gpu": 51207412736.0,
        "latency_per_micro_batch": 1.3953644641314833,
        "latency_fwd": 0.46512148804382775,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0032745969675941366,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 22.32994193932699,
        "device_tokens_per_sec": 2934.89,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 17.245945603938463,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 24748280832.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 25377467392.0,
        "latency_per_micro_batch": 0.06834636327539141,
        "latency_fwd": 0.022782121091797138,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 69.99519372822782,
        "device_tokens_per_sec": 29.26,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75733552128.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 76362738688.0,
        "latency_per_micro_batch": 0.5956177243828544,
        "latency_fwd": 0.19853924146095148,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 76.25684346880112,
        "device_tokens_per_sec": 107.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29295915776.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.3522475957070721,
        "latency_fwd": 0.1174158652356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 90.19255694990153,
        "device_tokens_per_sec": 45.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 39865561856.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 40022889216.0,
        "latency_per_micro_batch": 0.36280706629723064,
        "latency_fwd": 0.12093568876574355,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 92.89578142098212,
        "device_tokens_per_sec": 44.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 14791322368.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 16098233600.0,
        "latency_per_micro_batch": 0.08864716529812654,
        "latency_fwd": 0.029549055099375512,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 90.78338393425874,
        "device_tokens_per_sec": 22.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 20076145408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 20233472768.0,
        "latency_per_micro_batch": 0.09128703294566617,
        "latency_fwd": 0.030429010981888726,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.005382690133333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 93.48660840533933,
        "device_tokens_per_sec": 21.91,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 34112042496.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34426656256.0,
        "latency_per_micro_batch": 0.16014701886566618,
        "latency_fwd": 0.05338233962188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 82.01264675165201,
        "device_tokens_per_sec": 24.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 74774744064.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 75403930624.0,
        "latency_per_micro_batch": 0.5951982939828544,
        "latency_fwd": 0.19839943132761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 76.22012756914381,
        "device_tokens_per_sec": 53.74,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 30244173824.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 31502505984.0,
        "latency_per_micro_batch": 0.05282578176724337,
        "latency_fwd": 0.017608593922414456,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.006057005161025641,
        "latency_fwd_mlp": 0.011012736656410256,
        "latency_fwd_layernorm": 9.142398779358072e-05,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 54.10137896222219,
        "device_tokens_per_sec": 37.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 58590930432.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.6293008927856663,
        "latency_fwd": 0.20976696426188873,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 80.58485868330061,
        "device_tokens_per_sec": 50.83,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31540587264.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 31697914624.0,
        "latency_per_micro_batch": 0.7054275103856662,
        "latency_fwd": 0.23514250346188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 90.30340799834244,
        "device_tokens_per_sec": 181.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34940931072.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 35570117632.0,
        "latency_per_micro_batch": 0.14855194876777505,
        "latency_fwd": 0.049517316255925016,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 76.07637251689658,
        "device_tokens_per_sec": 26.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34367698432.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 34682312192.0,
        "latency_per_micro_batch": 0.3152328629070721,
        "latency_fwd": 0.10507762096902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 80.71698599664138,
        "device_tokens_per_sec": 50.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50441407488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 51070594048.0,
        "latency_per_micro_batch": 0.2704070309992279,
        "latency_fwd": 0.09013567699974263,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 69.23271767002936,
        "device_tokens_per_sec": 118.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 49946484224.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 50261097984.0,
        "latency_per_micro_batch": 0.3203874959028543,
        "latency_fwd": 0.1067958319676181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 82.02808638640944,
        "device_tokens_per_sec": 99.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43456661504.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 44714993664.0,
        "latency_per_micro_batch": 0.13085762481385294,
        "latency_fwd": 0.04361920827128431,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 67.01613931734596,
        "device_tokens_per_sec": 30.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 58335274496.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.31944377750285435,
        "latency_fwd": 0.10648125916761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 81.81195144746604,
        "device_tokens_per_sec": 25.03,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43652428544.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 43809755904.0,
        "latency_per_micro_batch": 1.4073833205428543,
        "latency_fwd": 0.4691277735142848,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 90.08970496363375,
        "device_tokens_per_sec": 181.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 67650627072.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 67965240832.0,
        "latency_per_micro_batch": 0.6383518675772306,
        "latency_fwd": 0.2127839558590769,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 81.74338345662085,
        "device_tokens_per_sec": 50.11,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3473520640.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 1048576000.0,
        "weight_memory_mlp_per_gpu": 2097152000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 6947041280.0,
        "optimizer_state_memory_per_gpu": 20841123840.0,
        "(weight+op_state)_memory_per_gpu": 24314644480.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62598678528.0,
        "(weight+op_state+grad)_memory_per_gpu": 31261685760.0,
        "estimated_peak_memory_per_gpu": 63857010688.0,
        "latency_per_micro_batch": 0.2611795621992279,
        "latency_fwd": 0.08705985406640929,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.005243050666666666,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 66.8790033356556,
        "device_tokens_per_sec": 61.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 29582194176.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 32195975680.0,
        "latency_per_micro_batch": 0.15788427516777503,
        "latency_fwd": 0.05262809172259168,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 80.85412197833175,
        "device_tokens_per_sec": 25.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44511939584.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 45141126144.0,
        "latency_per_micro_batch": 0.2965682101070721,
        "latency_fwd": 0.0988560700356907,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 75.93923653520622,
        "device_tokens_per_sec": 53.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 16,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1586022400.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 419430400.0,
        "weight_memory_mlp_per_gpu": 838860800.0,
        "weight_memory_layernorm_per_gpu": 40960.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3172044800.0,
        "optimizer_state_memory_per_gpu": 9516134400.0,
        "(weight+op_state)_memory_per_gpu": 11102156800.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 49386190848.0,
        "(weight+op_state+grad)_memory_per_gpu": 14274201600.0,
        "estimated_peak_memory_per_gpu": 50644523008.0,
        "latency_per_micro_batch": 0.10511587610600871,
        "latency_fwd": 0.03503862536866957,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.012114010322051281,
        "latency_fwd_mlp": 0.022025473312820513,
        "latency_fwd_layernorm": 0.00018284797558716144,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 53.827106998841444,
        "device_tokens_per_sec": 76.1,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 53805426176.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 63340034560.0,
        "latency_per_micro_batch": 0.31491829010707206,
        "latency_fwd": 0.10497276336902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 80.65342667414578,
        "device_tokens_per_sec": 25.39,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31827090944.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 32141704704.0,
        "latency_per_micro_batch": 0.3158620085070721,
        "latency_fwd": 0.10528733616902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 80.86956161308919,
        "device_tokens_per_sec": 101.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 5,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 62183836672.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.29786699070566625,
        "latency_fwd": 0.09928899690188875,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 76.28869555998901,
        "device_tokens_per_sec": 26.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43427395072.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 43742008832.0,
        "latency_per_micro_batch": 0.3197583503028544,
        "latency_fwd": 0.10658611676761812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.009227468800000001,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 81.87551076996165,
        "device_tokens_per_sec": 50.03,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 7677575168.0,
        "activation_memory_attn_per_gpu": 2080145408.0,
        "activation_memory_mlp_per_gpu": 1845493760.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 32187986688.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 32345314048.0,
        "latency_per_micro_batch": 0.18167137686285434,
        "latency_fwd": 0.060557125620951446,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 93.0329174026725,
        "device_tokens_per_sec": 22.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 2392752128.0,
        "activation_memory_attn_per_gpu": 869040128.0,
        "activation_memory_mlp_per_gpu": 1038090240.0,
        "activation_memory_layernorm_per_gpu": 461373440.0,
        "activation_memory_input_embedding_per_gpu": 2621440.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 26903163648.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 31670488320.0,
        "latency_per_micro_batch": 0.17639164156777506,
        "latency_fwd": 0.05879721385592502,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.000300896342826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 90.32969293159191,
        "device_tokens_per_sec": 22.67,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 15355150336.0,
        "activation_memory_attn_per_gpu": 4160290816.0,
        "activation_memory_mlp_per_gpu": 3690987520.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 27753720576.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 27911047936.0,
        "latency_per_micro_batch": 0.18203837846285434,
        "latency_fwd": 0.06067945948761811,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 93.21233644195858,
        "device_tokens_per_sec": 43.94,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17184074496.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 17341401856.0,
        "latency_per_micro_batch": 0.17675864316777506,
        "latency_fwd": 0.058919547722591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.010765380266666668,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0004232302094926707,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 90.509111970878,
        "device_tokens_per_sec": 45.26,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63080723968.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 63395337728.0,
        "latency_per_micro_batch": 1.2593243893428543,
        "latency_fwd": 0.4197747964476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 80.6141340103736,
        "device_tokens_per_sec": 203.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 37339188224.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 37968374784.0,
        "latency_per_micro_batch": 0.1361570391223048,
        "latency_fwd": 0.045385679707434935,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0004571199389679036,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 69.72092176484708,
        "device_tokens_per_sec": 58.75,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 61420601344.0,
        "activation_memory_attn_per_gpu": 16641163264.0,
        "activation_memory_mlp_per_gpu": 14763950080.0,
        "activation_memory_layernorm_per_gpu": 29527900160.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 73819171584.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 73976498944.0,
        "latency_per_micro_batch": 0.7265464515659833,
        "latency_fwd": 0.24218215052199443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0011572334094926708,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 93.00663246942302,
        "device_tokens_per_sec": 176.16,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 11,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 50363629568.0,
        "activation_memory_attn_per_gpu": 16672882688.0,
        "activation_memory_mlp_per_gpu": 18454937600.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62521003008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 63150189568.0,
        "latency_per_micro_batch": 0.27177839081613164,
        "latency_fwd": 0.09059279693871054,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.030285025805128204,
        "latency_fwd_mlp": 0.05506368328205128,
        "latency_fwd_layernorm": 0.0009142398779358072,
        "latency_fwd_tp_comm": 0.0027962026666666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 69.58378578315671,
        "device_tokens_per_sec": 117.73,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 13,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 37261410304.0,
        "activation_memory_attn_per_gpu": 11104681984.0,
        "activation_memory_mlp_per_gpu": 11072962560.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 62058100224.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 62372713984.0,
        "latency_per_micro_batch": 0.6389810131772307,
        "latency_fwd": 0.21299367105907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 81.80694277911645,
        "device_tokens_per_sec": 100.14,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 43108870912.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 43266198272.0,
        "latency_per_micro_batch": 0.36354106949723064,
        "latency_fwd": 0.12118035649907688,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 93.0752004602682,
        "device_tokens_per_sec": 88.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21969578752.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 22126906112.0,
        "latency_per_micro_batch": 0.3529815989070721,
        "latency_fwd": 0.11766053296902403,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.021530760533333335,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 90.37197598918762,
        "device_tokens_per_sec": 90.65,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 12590907392.0,
        "activation_memory_attn_per_gpu": 4168220672.0,
        "activation_memory_mlp_per_gpu": 4613734400.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 37960829952.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 38590016512.0,
        "latency_per_micro_batch": 0.14930619666707212,
        "latency_fwd": 0.04976873222235737,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0015379114666666668,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 76.46254744133668,
        "device_tokens_per_sec": 26.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1422106621378560.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50969107968.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 51283721728.0,
        "latency_per_micro_batch": 0.6311883295856662,
        "latency_fwd": 0.21039610986188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0010174232761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 80.80099362224401,
        "device_tokens_per_sec": 202.77,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22256082432.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22570696192.0,
        "latency_per_micro_batch": 0.15819884796777506,
        "latency_fwd": 0.052732949322591684,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.00025141596643234697,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 81.00669759477957,
        "device_tokens_per_sec": 50.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 63653956608.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 64283143168.0,
        "latency_per_micro_batch": 0.5926007327856662,
        "latency_fwd": 0.19753357759522205,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.000458182742826004,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 75.87066854436102,
        "device_tokens_per_sec": 107.97,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68734946304.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 69364132864.0,
        "latency_per_micro_batch": 0.5921813023856661,
        "latency_fwd": 0.19739376746188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.006151645866666667,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 75.83395264470371,
        "device_tokens_per_sec": 54.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 9315352576.0,
        "activation_memory_attn_per_gpu": 2776170496.0,
        "activation_memory_mlp_per_gpu": 2768240640.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 22000426496.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 22315040256.0,
        "latency_per_micro_batch": 0.08049863954707209,
        "latency_fwd": 0.026832879849024032,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 82.43949433148056,
        "device_tokens_per_sec": 24.84,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 21728381952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 22357568512.0,
        "latency_per_micro_batch": 0.06800352332116548,
        "latency_fwd": 0.02266784110705516,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007571256451282051,
        "latency_fwd_mlp": 0.01376592082051282,
        "latency_fwd_layernorm": 0.0001142799847419759,
        "latency_fwd_tp_comm": 0.0006990506666666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 69.64412561510046,
        "device_tokens_per_sec": 29.41,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 8,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1736811520.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 524288000.0,
        "weight_memory_mlp_per_gpu": 1048576000.0,
        "weight_memory_layernorm_per_gpu": 102400.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3473520640.0,
        "optimizer_state_memory_per_gpu": 10420561920.0,
        "(weight+op_state)_memory_per_gpu": 12157373440.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 31299390464.0,
        "(weight+op_state+grad)_memory_per_gpu": 15630894080.0,
        "estimated_peak_memory_per_gpu": 31928577024.0,
        "latency_per_micro_batch": 0.13547135921385295,
        "latency_fwd": 0.04515711973795098,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015142512902564102,
        "latency_fwd_mlp": 0.02753184164102564,
        "latency_fwd_layernorm": 0.0002285599694839518,
        "latency_fwd_tp_comm": 0.0013981013333333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 69.36985365171972,
        "device_tokens_per_sec": 59.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4657676288.0,
        "activation_memory_per_gpu": 18630705152.0,
        "activation_memory_attn_per_gpu": 5552340992.0,
        "activation_memory_mlp_per_gpu": 5536481280.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 31315779072.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 31630392832.0,
        "latency_per_micro_batch": 0.16046159166566618,
        "latency_fwd": 0.05348719722188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.01665676419282051,
        "latency_fwd_mlp": 0.030285025805128204,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0046137344000000005,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0003882776761593373,
        "latency_fwd_output_embedding_loss": 0.000537731282051282,
        "latency_per_iter": 82.16522236809982,
        "device_tokens_per_sec": 49.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 25,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3542673920.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7084672000.0,
        "optimizer_state_memory_per_gpu": 21254016000.0,
        "(weight+op_state)_memory_per_gpu": 24796689920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 43938706944.0,
        "(weight+op_state+grad)_memory_per_gpu": 31881361920.0,
        "estimated_peak_memory_per_gpu": 44253320704.0,
        "latency_per_micro_batch": 0.6299300383856662,
        "latency_fwd": 0.20997667946188872,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.018454937600000002,
        "latency_fwd_sharded_dp_comm": 0.023069422933333333,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 80.6484180057962,
        "device_tokens_per_sec": 101.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 7,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7084897280.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14169344000.0,
        "optimizer_state_memory_per_gpu": 42508032000.0,
        "(weight+op_state)_memory_per_gpu": 49592929280.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 4785504256.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 59163937792.0,
        "(weight+op_state+grad)_memory_per_gpu": 63762273280.0,
        "estimated_peak_memory_per_gpu": 64391459840.0,
        "latency_per_micro_batch": 0.2963584949070721,
        "latency_fwd": 0.09878616496902404,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.0346041344,
        "latency_fwd_input_embedding": 0.000248467542826004,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 75.90252063554891,
        "device_tokens_per_sec": 26.98,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 30,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 4,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 1812298240.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3624258560.0,
        "optimizer_state_memory_per_gpu": 10872775680.0,
        "(weight+op_state)_memory_per_gpu": 12685073920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 4785504256.0,
        "activation_memory_attn_per_gpu": 1738080256.0,
        "activation_memory_mlp_per_gpu": 2076180480.0,
        "activation_memory_layernorm_per_gpu": 922746880.0,
        "activation_memory_input_embedding_per_gpu": 5242880.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17470578176.0,
        "(weight+op_state+grad)_memory_per_gpu": 16309332480.0,
        "estimated_peak_memory_per_gpu": 17785191936.0,
        "latency_per_micro_batch": 0.07936726769812653,
        "latency_fwd": 0.026455755899375513,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.008328382096410255,
        "latency_fwd_mlp": 0.015142512902564102,
        "latency_fwd_layernorm": 0.00012570798321617348,
        "latency_fwd_tp_comm": 0.0023068672000000003,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00028342007615933735,
        "latency_fwd_output_embedding_loss": 0.000268865641025641,
        "latency_per_iter": 81.28096955816031,
        "device_tokens_per_sec": 25.2,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 61,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 2,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 1771562240.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 576716800.0,
        "weight_memory_mlp_per_gpu": 1153433600.0,
        "weight_memory_layernorm_per_gpu": 450560.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 3542336000.0,
        "optimizer_state_memory_per_gpu": 10627008000.0,
        "(weight+op_state)_memory_per_gpu": 12398570240.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 50682604288.0,
        "(weight+op_state+grad)_memory_per_gpu": 15940906240.0,
        "estimated_peak_memory_per_gpu": 50944748288.0,
        "latency_per_micro_batch": 1.4103193333428543,
        "latency_fwd": 0.4701064444476181,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.08612304213333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 90.26912400291984,
        "device_tokens_per_sec": 363.0,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 7003425280.0,
        "weight_memory_embedding_per_gpu": 81920000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 81920000.0,
        "unsharded_weight_memory_per_layer": 157306880.0,
        "unsharded_weight_memory_attn_per_layer": 52428800.0,
        "unsharded_weight_memory_mlp_per_layer": 104857600.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14005498880.0,
        "optimizer_state_memory_per_gpu": 42016496640.0,
        "(weight+op_state)_memory_per_gpu": 49019921920.0,
        "estimated_fwd_prefetch_memory_per_gpu": 239226880.0,
        "estimated_bwd_prefetch_memory_per_gpu": 314613760.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2392752128.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 68161938944.0,
        "(weight+op_state+grad)_memory_per_gpu": 63025420800.0,
        "estimated_peak_memory_per_gpu": 68476552704.0,
        "latency_per_micro_batch": 1.2580660981428544,
        "latency_fwd": 0.41935536604761814,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.13325411354256408,
        "latency_fwd_mlp": 0.24228020644102563,
        "latency_fwd_layernorm": 0.0020113277314587757,
        "latency_fwd_tp_comm": 0.036909875200000004,
        "latency_fwd_sharded_dp_comm": 0.0692082688,
        "latency_fwd_input_embedding": 0.0005979928761593373,
        "latency_fwd_output_embedding_loss": 0.004301850256410256,
        "latency_per_iter": 80.55057468787801,
        "device_tokens_per_sec": 101.7,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 9571008512.0,
        "activation_memory_attn_per_gpu": 3476160512.0,
        "activation_memory_mlp_per_gpu": 4152360960.0,
        "activation_memory_layernorm_per_gpu": 1845493760.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 34081420032.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 34238747392.0,
        "latency_per_micro_batch": 0.7039595039856662,
        "latency_fwd": 0.23465316799522207,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 90.12398895905635,
        "device_tokens_per_sec": 90.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 9,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 355526655344640.0,
        "weight_memory_per_gpu": 3624371200.0,
        "weight_memory_embedding_per_gpu": 163840000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 163840000.0,
        "unsharded_weight_memory_per_layer": 314593280.0,
        "unsharded_weight_memory_attn_per_layer": 104857600.0,
        "unsharded_weight_memory_mlp_per_layer": 209715200.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7248517120.0,
        "optimizer_state_memory_per_gpu": 21745551360.0,
        "(weight+op_state)_memory_per_gpu": 25369922560.0,
        "estimated_fwd_prefetch_memory_per_gpu": 478433280.0,
        "estimated_bwd_prefetch_memory_per_gpu": 629186560.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 6295453696.0,
        "activation_memory_per_gpu": 25181814784.0,
        "activation_memory_attn_per_gpu": 8336441344.0,
        "activation_memory_mlp_per_gpu": 9227468800.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 50551737344.0,
        "(weight+op_state+grad)_memory_per_gpu": 32618439680.0,
        "estimated_peak_memory_per_gpu": 51180923904.0,
        "latency_per_micro_batch": 0.29807670590566626,
        "latency_fwd": 0.09935890196855542,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0010056638657293879,
        "latency_fwd_tp_comm": 0.0030758229333333336,
        "latency_fwd_sharded_dp_comm": 0.011534711466666667,
        "latency_fwd_input_embedding": 0.00031837260949267067,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 76.32541145964632,
        "device_tokens_per_sec": 53.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 15,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 711053310689280.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3838787584.0,
        "activation_memory_per_gpu": 30710300672.0,
        "activation_memory_attn_per_gpu": 8320581632.0,
        "activation_memory_mlp_per_gpu": 7381975040.0,
        "activation_memory_layernorm_per_gpu": 14763950080.0,
        "activation_memory_input_embedding_per_gpu": 10485760.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 55220712192.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 55378039552.0,
        "latency_per_micro_batch": 0.7250784451659833,
        "latency_fwd": 0.24169281505532778,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06662705677128204,
        "latency_fwd_mlp": 0.12114010322051282,
        "latency_fwd_layernorm": 0.008045310925835103,
        "latency_fwd_tp_comm": 0.04306152106666667,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0006678979428260041,
        "latency_fwd_output_embedding_loss": 0.002150925128205128,
        "latency_per_iter": 92.82721343013694,
        "device_tokens_per_sec": 88.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 50,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2844213242757120.0,
        "weight_memory_per_gpu": 3502163200.0,
        "weight_memory_embedding_per_gpu": 40960000.0,
        "weight_memory_attn_per_gpu": 1153433600.0,
        "weight_memory_mlp_per_gpu": 2306867200.0,
        "weight_memory_layernorm_per_gpu": 901120.0,
        "unsharded_weight_memory_embedding": 40960000.0,
        "unsharded_weight_memory_per_layer": 78663680.0,
        "unsharded_weight_memory_attn_per_layer": 26214400.0,
        "unsharded_weight_memory_mlp_per_layer": 52428800.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 7002749440.0,
        "optimizer_state_memory_per_gpu": 21008248320.0,
        "(weight+op_state)_memory_per_gpu": 24510411520.0,
        "estimated_fwd_prefetch_memory_per_gpu": 119623680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 157327360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1196376064.0,
        "activation_memory_per_gpu": 38284034048.0,
        "activation_memory_attn_per_gpu": 13904642048.0,
        "activation_memory_mlp_per_gpu": 16609443840.0,
        "activation_memory_layernorm_per_gpu": 7381975040.0,
        "activation_memory_input_embedding_per_gpu": 41943040.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 62794445568.0,
        "(weight+op_state+grad)_memory_per_gpu": 31513160960.0,
        "estimated_peak_memory_per_gpu": 63056589568.0,
        "latency_per_micro_batch": 2.8142309536572307,
        "latency_fwd": 0.9380769845524102,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.26650822708512817,
        "latency_fwd_mlp": 0.48456041288205126,
        "latency_fwd_layernorm": 0.0040226554629175515,
        "latency_fwd_tp_comm": 0.17224608426666668,
        "latency_fwd_sharded_dp_comm": 0.04613884586666667,
        "latency_fwd_input_embedding": 0.0021359043428260042,
        "latency_fwd_output_embedding_loss": 0.008603700512820512,
        "latency_per_iter": 90.07256296592246,
        "device_tokens_per_sec": 363.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 3,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 16,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 14005498880.0,
        "num_params_total_mlp": 9227468800,
        "num_params_total_embedding": 163840000,
        "num_params_total_others": 4614190080.0,
        "num_active_params_total": 14005498880.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 177763327672320.0,
        "weight_memory_per_gpu": 7248517120.0,
        "weight_memory_embedding_per_gpu": 327680000.0,
        "weight_memory_attn_per_gpu": 2306867200.0,
        "weight_memory_mlp_per_gpu": 4613734400.0,
        "weight_memory_layernorm_per_gpu": 225280.0,
        "unsharded_weight_memory_embedding": 327680000.0,
        "unsharded_weight_memory_per_layer": 629166080.0,
        "unsharded_weight_memory_attn_per_layer": 209715200.0,
        "unsharded_weight_memory_mlp_per_layer": 419430400.0,
        "unshared_weight_memory_layernorm": 20480.0,
        "gradient_memory_per_gpu": 14497034240.0,
        "optimizer_state_memory_per_gpu": 43491102720.0,
        "(weight+op_state)_memory_per_gpu": 50739619840.0,
        "estimated_fwd_prefetch_memory_per_gpu": 956846080.0,
        "estimated_bwd_prefetch_memory_per_gpu": 1258332160.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 9571008512.0,
        "activation_memory_per_gpu": 19142017024.0,
        "activation_memory_attn_per_gpu": 6952321024.0,
        "activation_memory_mlp_per_gpu": 8304721920.0,
        "activation_memory_layernorm_per_gpu": 3690987520.0,
        "activation_memory_input_embedding_per_gpu": 20971520.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 69881636864.0,
        "(weight+op_state+grad)_memory_per_gpu": 65236654080.0,
        "estimated_peak_memory_per_gpu": 71139969024.0,
        "latency_per_micro_batch": 0.28692131090707207,
        "latency_fwd": 0.09564043696902402,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.03331352838564102,
        "latency_fwd_mlp": 0.06057005161025641,
        "latency_fwd_layernorm": 0.0005028319328646939,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0173020672,
        "latency_fwd_input_embedding": 0.00017856247615933733,
        "latency_fwd_output_embedding_loss": 0.001075462564102564,
        "latency_per_iter": 73.48740496504027,
        "device_tokens_per_sec": 27.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 72.92854468608,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-13b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 58138091520.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 61149126656.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 69.57574149174533,
        "device_tokens_per_sec": 29.44,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 55281072128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 55543216128.0,
        "latency_per_micro_batch": 0.6198448499509533,
        "latency_fwd": 0.20661494998365112,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 79.34836167731856,
        "device_tokens_per_sec": 206.48,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 23396380672.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 23799066624.0,
        "latency_per_micro_batch": 0.14174442369637003,
        "latency_fwd": 0.04724814123212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 72.58168716721423,
        "device_tokens_per_sec": 56.43,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14534719488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 15287502848.0,
        "latency_per_micro_batch": 0.07653908201957621,
        "latency_fwd": 0.02551302733985874,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 78.38424087164258,
        "device_tokens_per_sec": 26.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 28610672640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.30436736176995766,
        "latency_fwd": 0.10145578725665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 77.934164957801,
        "device_tokens_per_sec": 52.56,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 35517882368.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 36323221504.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 69.66175221522418,
        "device_tokens_per_sec": 58.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 43547361280.0,
        "activation_memory_attn_per_gpu": 12918456320.0,
        "activation_memory_mlp_per_gpu": 12884901888.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 66556016640.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 66818160640.0,
        "latency_per_micro_batch": 1.2372478840391246,
        "latency_fwd": 0.4124159613463748,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 79.19998492319581,
        "device_tokens_per_sec": 206.87,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 38194409472.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 38597095424.0,
        "latency_per_micro_batch": 0.28448009369559435,
        "latency_fwd": 0.09482669789853146,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 72.84334561041524,
        "device_tokens_per_sec": 56.23,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 4483317760.0,
        "activation_memory_attn_per_gpu": 1210056704.0,
        "activation_memory_mlp_per_gpu": 1073741824.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 15987907584.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 16088603648.0,
        "latency_per_micro_batch": 0.09039028928944053,
        "latency_fwd": 0.03013009642981351,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 92.567716547583,
        "device_tokens_per_sec": 22.12,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 16384000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 1400504320.0,
        "activation_memory_attn_per_gpu": 505413632.0,
        "activation_memory_mlp_per_gpu": 603979776.0,
        "activation_memory_layernorm_per_gpu": 268435456.0,
        "activation_memory_input_embedding_per_gpu": 2097152.0,
        "activation_memory_output_embedding_per_gpu": 16384000.0,
        "(weight+op_state+act)_memory_per_gpu": 12905094144.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.08731844329957622,
        "latency_fwd": 0.029106147766525405,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.006263493973333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00024071707426080323,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 89.42214625396196,
        "device_tokens_per_sec": 22.9,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 22620551168.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 22821910528.0,
        "latency_per_micro_batch": 0.15528262494482514,
        "latency_fwd": 0.05176087498160838,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 79.51292485534701,
        "device_tokens_per_sec": 51.51,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 60744036352.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 61146722304.0,
        "latency_per_micro_batch": 0.5681960931284062,
        "latency_fwd": 0.18939869770946877,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 72.76134032411973,
        "device_tokens_per_sec": 56.29,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 28452075520.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.15503096670482516,
        "latency_fwd": 0.051676988901608385,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 79.39197529756233,
        "device_tokens_per_sec": 25.8,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22708624384.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 22839696384.0,
        "latency_per_micro_batch": 0.6955476967971328,
        "latency_fwd": 0.23184923226571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 89.0381655052289,
        "device_tokens_per_sec": 184.01,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 68283297792.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 68807585792.0,
        "latency_per_micro_batch": 1.1296133626914833,
        "latency_fwd": 0.3765377875638278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 72.31169683659802,
        "device_tokens_per_sec": 226.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 5443420160.0,
        "activation_memory_attn_per_gpu": 1614807040.0,
        "activation_memory_mlp_per_gpu": 1610612736.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 17177131008.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17378490368.0,
        "latency_per_micro_batch": 0.07785558744380378,
        "latency_fwd": 0.025951862481267927,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.00268435456,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 79.7323424260516,
        "device_tokens_per_sec": 25.69,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 10886840320.0,
        "activation_memory_attn_per_gpu": 3229614080.0,
        "activation_memory_mlp_per_gpu": 3221225472.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 33895495680.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34096855040.0,
        "latency_per_micro_batch": 0.3096333834668679,
        "latency_fwd": 0.1032111278222893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 79.28226651221003,
        "device_tokens_per_sec": 51.66,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 53380411392.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2843123215355944,
        "latency_fwd": 0.09477077384519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 72.81619471679589,
        "device_tokens_per_sec": 28.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 14305598464.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 14892180480.0,
        "latency_per_micro_batch": 0.17420833665637,
        "latency_fwd": 0.05806944555212334,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 89.20272868325735,
        "device_tokens_per_sec": 45.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 41646845952.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 42049531904.0,
        "latency_per_micro_batch": 0.28481563801559434,
        "latency_fwd": 0.09493854600519812,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 72.92134556666493,
        "device_tokens_per_sec": 112.34,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 8966635520.0,
        "activation_memory_attn_per_gpu": 2420113408.0,
        "activation_memory_mlp_per_gpu": 2147483648.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 20471225344.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 20571921408.0,
        "latency_per_micro_batch": 0.18035202863609864,
        "latency_fwd": 0.06011734287869955,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.012526987946666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00033858416759413655,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 92.34829897687841,
        "device_tokens_per_sec": 44.35,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45875228672.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 46277914624.0,
        "latency_per_micro_batch": 0.5650209563171329,
        "latency_fwd": 0.1883403187723776,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 72.3391240329361,
        "device_tokens_per_sec": 113.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 64,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 2690367514214400.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56320728064.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 56845016064.0,
        "latency_per_micro_batch": 2.7809051373601843,
        "latency_fwd": 0.9269683791200614,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.20043180714666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0032745969675941366,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 88.9970247107218,
        "device_tokens_per_sec": 736.38,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 30830784512.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 31233470464.0,
        "latency_per_micro_batch": 0.14245432181918838,
        "latency_fwd": 0.04748477393972946,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 72.95305439576754,
        "device_tokens_per_sec": 28.07,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 45416724480.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 45678868480.0,
        "latency_per_micro_batch": 1.2161837972514835,
        "latency_fwd": 0.4053945990838278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 77.85188336878679,
        "device_tokens_per_sec": 210.45,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 24313847808.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 25119186944.0,
        "latency_per_micro_batch": 0.0682343600995762,
        "latency_fwd": 0.022744786699858736,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 69.88116978592876,
        "device_tokens_per_sec": 29.31,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17335728128.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 17537087488.0,
        "latency_per_micro_batch": 0.15264961409637,
        "latency_fwd": 0.050883204698790006,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00031062214092746987,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 78.16482330093798,
        "device_tokens_per_sec": 52.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 35866542080.0,
        "activation_memory_attn_per_gpu": 9680453632.0,
        "activation_memory_mlp_per_gpu": 8589934592.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 47371131904.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 47502203904.0,
        "latency_per_micro_batch": 0.7201224647160474,
        "latency_fwd": 0.24004082157201578,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.009361816350062665,
        "latency_fwd_tp_comm": 0.05010795178666667,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0009257867275941366,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 92.18373579884997,
        "device_tokens_per_sec": 177.73,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 32768000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 2801008640.0,
        "activation_memory_attn_per_gpu": 1010827264.0,
        "activation_memory_mlp_per_gpu": 1207959552.0,
        "activation_memory_layernorm_per_gpu": 536870912.0,
        "activation_memory_input_embedding_per_gpu": 4194304.0,
        "activation_memory_output_embedding_per_gpu": 32768000.0,
        "(weight+op_state+act)_memory_per_gpu": 25809664000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 29783803904.0,
        "latency_per_micro_batch": 0.15239795585637003,
        "latency_fwd": 0.05079931861879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.00536870912,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0002267360609274699,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 78.0438737431533,
        "device_tokens_per_sec": 26.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34141779968.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 34403923968.0,
        "latency_per_micro_batch": 0.6093128065571328,
        "latency_fwd": 0.20310426885237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0008139386209274699,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 78.00026012290954,
        "device_tokens_per_sec": 210.05,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17794363392.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 18197049344.0,
        "latency_per_micro_batch": 0.07108648681957622,
        "latency_fwd": 0.023695495606525407,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.00014627838046972914,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 72.80110473791882,
        "device_tokens_per_sec": 28.13,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 69200699392.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 70006038528.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 69.50508839115356,
        "device_tokens_per_sec": 117.86,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 58908999680.0,
        "activation_memory_attn_per_gpu": 19394461696.0,
        "activation_memory_mlp_per_gpu": 21474836480.0,
        "activation_memory_layernorm_per_gpu": 17179869184.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 71101345792.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 71625633792.0,
        "latency_per_micro_batch": 0.5692027260884064,
        "latency_fwd": 0.18973424202946879,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 72.86649117398879,
        "device_tokens_per_sec": 224.85,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 68424855552.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 68827541504.0,
        "latency_per_micro_batch": 1.1289422740514832,
        "latency_fwd": 0.37631409135049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 72.28454594297865,
        "device_tokens_per_sec": 113.33,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 6,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6704865280.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13409730560.0,
        "optimizer_state_memory_per_gpu": 40229191680.0,
        "(weight+op_state)_memory_per_gpu": 46934056960.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 69342126080.0,
        "(weight+op_state+grad)_memory_per_gpu": 60343787520.0,
        "estimated_peak_memory_per_gpu": 70147465216.0,
        "latency_per_micro_batch": 0.5428750311971329,
        "latency_fwd": 0.18095834373237762,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.01610678272,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 69.5208870990692,
        "device_tokens_per_sec": 58.92,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 46792630272.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 47597969408.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 69.5599427838297,
        "device_tokens_per_sec": 58.88,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 57220820992.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.5646854119971328,
        "latency_fwd": 0.18822847066571094,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 72.31197313931673,
        "device_tokens_per_sec": 56.64,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 22937745408.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 23139104768.0,
        "latency_per_micro_batch": 0.3048706782499576,
        "latency_fwd": 0.10162355941665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 78.05511451558569,
        "device_tokens_per_sec": 104.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 71733084160.0,
        "activation_memory_attn_per_gpu": 19360907264.0,
        "activation_memory_mlp_per_gpu": 17179869184.0,
        "activation_memory_layernorm_per_gpu": 34359738368.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 83237673984.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 83499817984.0,
        "latency_per_micro_batch": 1.4398163794893124,
        "latency_fwd": 0.4799387931631041,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.01872363270012533,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 92.1563086025119,
        "device_tokens_per_sec": 355.57,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34671194112.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 35073880064.0,
        "latency_per_micro_batch": 0.28272475312995765,
        "latency_fwd": 0.09424158437665255,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 72.39397842561225,
        "device_tokens_per_sec": 56.58,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 32,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 67824793600.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 68349081600.0,
        "latency_per_micro_batch": 2.4319390445601843,
        "latency_fwd": 0.8106463481867281,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.2537334525636923,
        "latency_fwd_mlp": 0.4510816934465641,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.08589934592,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.01376592082051282,
        "latency_per_iter": 77.83816977061774,
        "device_tokens_per_sec": 420.98,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 44782336000.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 44983695360.0,
        "latency_per_micro_batch": 0.6188382169909534,
        "latency_fwd": 0.20627940566365113,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 79.22741211953388,
        "device_tokens_per_sec": 103.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 4,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 168147969638400.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 14727249920.0,
        "activation_memory_attn_per_gpu": 4848615424.0,
        "activation_memory_mlp_per_gpu": 5368709120.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 26919596032.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 27322281984.0,
        "latency_per_micro_batch": 0.1426220939791884,
        "latency_fwd": 0.0475406979930628,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0002546980875941365,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 73.03105435201724,
        "device_tokens_per_sec": 56.09,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 17106607104.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 17207303168.0,
        "latency_per_micro_batch": 0.34798812336995766,
        "latency_fwd": 0.11599604112331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 89.09301989790507,
        "device_tokens_per_sec": 91.95,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 33,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2241658880.0,
        "activation_memory_per_gpu": 17933271040.0,
        "activation_memory_attn_per_gpu": 4840226816.0,
        "activation_memory_mlp_per_gpu": 4294967296.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29437860864.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 29538556928.0,
        "latency_per_micro_batch": 0.3602755073294149,
        "latency_fwd": 0.12009183577647163,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.025053975893333334,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005343183542608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 92.23859019152611,
        "device_tokens_per_sec": 88.81,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 52,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 56549849088.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 57074137088.0,
        "latency_per_micro_batch": 1.2181970631714834,
        "latency_fwd": 0.4060656877238278,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.04294967296,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0014850272609274698,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 77.97283292657147,
        "device_tokens_per_sec": 420.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 19,
        "gradient_accumulation_steps": 1024,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 7363624960.0,
        "activation_memory_attn_per_gpu": 2424307712.0,
        "activation_memory_mlp_per_gpu": 2684354560.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 19555971072.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 19958657024.0,
        "latency_per_micro_batch": 0.0715253219609854,
        "latency_fwd": 0.023841773986995135,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.007929170392615384,
        "latency_fwd_mlp": 0.014096302920205129,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0008947848533333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.00043018502564102563,
        "latency_per_iter": 73.25047192272183,
        "device_tokens_per_sec": 27.96,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 75471286272.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 75873972224.0,
        "latency_per_micro_batch": 1.1359636363140302,
        "latency_fwd": 0.3786545454380101,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.004680908175031332,
        "latency_fwd_tp_comm": 0.014316557653333332,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 72.73391312778166,
        "device_tokens_per_sec": 112.63,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 27,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1676412928.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3352432640.0,
        "optimizer_state_memory_per_gpu": 10057297920.0,
        "(weight+op_state)_memory_per_gpu": 11733710848.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 2721710080.0,
        "activation_memory_per_gpu": 21773680640.0,
        "activation_memory_attn_per_gpu": 6459228160.0,
        "activation_memory_mlp_per_gpu": 6442450944.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 33507391488.0,
        "(weight+op_state+grad)_memory_per_gpu": 15086143488.0,
        "estimated_peak_memory_per_gpu": 33708750848.0,
        "latency_per_micro_batch": 0.3101366999468679,
        "latency_fwd": 0.1033788999822893,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.01073741824,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 79.40321606999471,
        "device_tokens_per_sec": 103.17,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 16,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 672591878553600.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57008484352.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 57532772352.0,
        "latency_per_micro_batch": 0.5656920449571329,
        "latency_fwd": 0.18856401498571096,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0005902424075941365,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 72.41712398918578,
        "device_tokens_per_sec": 226.24,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 22,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 29069176832.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 30574710784.0,
        "latency_per_micro_batch": 0.14157665153637,
        "latency_fwd": 0.047192217178790005,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0.0017895697066666666,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 72.50368721096453,
        "device_tokens_per_sec": 28.25,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 12,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 8,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1872830464.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 65536.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3745660928.0,
        "optimizer_state_memory_per_gpu": 11236982784.0,
        "(weight+op_state)_memory_per_gpu": 13109813248.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 524288000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 44816138240.0,
        "activation_memory_attn_per_gpu": 16173236224.0,
        "activation_memory_mlp_per_gpu": 19327352832.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 67108864.0,
        "activation_memory_output_embedding_per_gpu": 524288000.0,
        "(weight+op_state+act)_memory_per_gpu": 57925951488.0,
        "(weight+op_state+grad)_memory_per_gpu": 16855474176.0,
        "estimated_peak_memory_per_gpu": 58731290624.0,
        "latency_per_micro_batch": 0.2716517905699576,
        "latency_fwd": 0.09055059685665254,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 69.55204342987187,
        "device_tokens_per_sec": 117.78,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 16,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 2,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3352563712.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 262144.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6704865280.0,
        "optimizer_state_memory_per_gpu": 20114595840.0,
        "(weight+op_state)_memory_per_gpu": 23467159552.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 3681812480.0,
        "activation_memory_per_gpu": 29454499840.0,
        "activation_memory_attn_per_gpu": 9697230848.0,
        "activation_memory_mlp_per_gpu": 10737418240.0,
        "activation_memory_layernorm_per_gpu": 8589934592.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 52921659392.0,
        "(weight+op_state+grad)_memory_per_gpu": 30172024832.0,
        "estimated_peak_memory_per_gpu": 53324345344.0,
        "latency_per_micro_batch": 0.5685316374484063,
        "latency_fwd": 0.18951054581613544,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.007158278826666666,
        "latency_fwd_sharded_dp_comm": 0.010737855146666666,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 72.7884912177391,
        "device_tokens_per_sec": 112.55,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 44,
        "gradient_accumulation_steps": 128,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 4,
        "pp_size": 1,
        "sp_size": 4,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 3287287808.0,
        "weight_memory_embedding_per_gpu": 65536000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 65536000.0,
        "unsharded_weight_memory_per_layer": 100679680.0,
        "unsharded_weight_memory_attn_per_layer": 33554432.0,
        "unsharded_weight_memory_mlp_per_layer": 67108864.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6573789184.0,
        "optimizer_state_memory_per_gpu": 19721367552.0,
        "(weight+op_state)_memory_per_gpu": 23008655360.0,
        "estimated_fwd_prefetch_memory_per_gpu": 166215680.0,
        "estimated_bwd_prefetch_memory_per_gpu": 201359360.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 1400504320.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 34212689920.0,
        "(weight+op_state+grad)_memory_per_gpu": 29582444544.0,
        "estimated_peak_memory_per_gpu": 34414049280.0,
        "latency_per_micro_batch": 0.6083061735971329,
        "latency_fwd": 0.20276872453237763,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.06343336314092307,
        "latency_fwd_mlp": 0.11277042336164103,
        "latency_fwd_layernorm": 0.001170227043757833,
        "latency_fwd_tp_comm": 0.02147483648,
        "latency_fwd_sharded_dp_comm": 0.021475710293333332,
        "latency_fwd_input_embedding": 0.00047839430092746983,
        "latency_fwd_output_embedding_loss": 0.003441480205128205,
        "latency_per_iter": 77.87931056512485,
        "device_tokens_per_sec": 105.19,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 14,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 4,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 1,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 6574051328.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 2147483648.0,
        "weight_memory_mlp_per_gpu": 4294967296.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 13147578368.0,
        "optimizer_state_memory_per_gpu": 39442735104.0,
        "(weight+op_state)_memory_per_gpu": 46016786432.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 65536000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 5602017280.0,
        "activation_memory_attn_per_gpu": 2021654528.0,
        "activation_memory_mlp_per_gpu": 2415919104.0,
        "activation_memory_layernorm_per_gpu": 1073741824.0,
        "activation_memory_input_embedding_per_gpu": 8388608.0,
        "activation_memory_output_embedding_per_gpu": 65536000.0,
        "(weight+op_state+act)_memory_per_gpu": 51618803712.0,
        "(weight+op_state+grad)_memory_per_gpu": 59164364800.0,
        "estimated_peak_memory_per_gpu": 59567050752.0,
        "latency_per_micro_batch": 0.2825569809699576,
        "latency_fwd": 0.09418566032331921,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.03221356544,
        "latency_fwd_input_embedding": 0.0001987740342608032,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 72.36682753199288,
        "device_tokens_per_sec": 28.3,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 2,
        "max_batch_size_per_gpu": 10,
        "gradient_accumulation_steps": 512,
        "global_batch_size": 2048,
        "dp_size": 2,
        "rdp_size": 1,
        "tp_size": 1,
        "pp_size": 4,
        "sp_size": 1,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 84073984819200.0,
        "weight_memory_per_gpu": 3483508736.0,
        "weight_memory_embedding_per_gpu": 262144000.0,
        "weight_memory_attn_per_gpu": 1073741824.0,
        "weight_memory_mlp_per_gpu": 2147483648.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 262144000.0,
        "unsharded_weight_memory_per_layer": 402669568.0,
        "unsharded_weight_memory_attn_per_layer": 134217728.0,
        "unsharded_weight_memory_mlp_per_layer": 268435456.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 6967017472.0,
        "optimizer_state_memory_per_gpu": 20901052416.0,
        "(weight+op_state)_memory_per_gpu": 24384561152.0,
        "estimated_fwd_prefetch_memory_per_gpu": 664813568.0,
        "estimated_bwd_prefetch_memory_per_gpu": 805339136.0,
        "loss_bwd_memory": 131072000.0,
        "activation_memory_batch_size_1": 5602017280.0,
        "activation_memory_per_gpu": 11204034560.0,
        "activation_memory_attn_per_gpu": 4043309056.0,
        "activation_memory_mlp_per_gpu": 4831838208.0,
        "activation_memory_layernorm_per_gpu": 2147483648.0,
        "activation_memory_input_embedding_per_gpu": 16777216.0,
        "activation_memory_output_embedding_per_gpu": 131072000.0,
        "(weight+op_state+act)_memory_per_gpu": 35588595712.0,
        "(weight+op_state+grad)_memory_per_gpu": 31351578624.0,
        "estimated_peak_memory_per_gpu": 36393934848.0,
        "latency_per_micro_batch": 0.13604017025637002,
        "latency_fwd": 0.04534672341879001,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.015858340785230768,
        "latency_fwd_mlp": 0.028192605840410258,
        "latency_fwd_layernorm": 0.0002925567609394583,
        "latency_fwd_tp_comm": 0,
        "latency_fwd_sharded_dp_comm": 0.005368927573333333,
        "latency_fwd_input_embedding": 0.00014284998092746988,
        "latency_fwd_output_embedding_loss": 0.0008603700512820513,
        "latency_per_iter": 69.66965156918201,
        "device_tokens_per_sec": 29.4,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 8,
        "max_batch_size_per_gpu": 26,
        "gradient_accumulation_steps": 256,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 2,
        "pp_size": 4,
        "sp_size": 2,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 336295939276800.0,
        "weight_memory_per_gpu": 1741819904.0,
        "weight_memory_embedding_per_gpu": 131072000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 131072.0,
        "unsharded_weight_memory_embedding": 131072000.0,
        "unsharded_weight_memory_per_layer": 201342976.0,
        "unsharded_weight_memory_attn_per_layer": 67108864.0,
        "unsharded_weight_memory_mlp_per_layer": 134217728.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3483508736.0,
        "optimizer_state_memory_per_gpu": 10450526208.0,
        "(weight+op_state)_memory_per_gpu": 12192346112.0,
        "estimated_fwd_prefetch_memory_per_gpu": 332414976.0,
        "estimated_bwd_prefetch_memory_per_gpu": 402685952.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 2801008640.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 34600415232.0,
        "(weight+op_state+grad)_memory_per_gpu": 15675854848.0,
        "estimated_peak_memory_per_gpu": 35003101184.0,
        "latency_per_micro_batch": 0.28306029744995764,
        "latency_fwd": 0.0943534324833192,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.031716681570461536,
        "latency_fwd_mlp": 0.056385211680820516,
        "latency_fwd_layernorm": 0.0005851135218789166,
        "latency_fwd_tp_comm": 0.003579139413333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.00036654619426080323,
        "latency_fwd_output_embedding_loss": 0.0017207401025641025,
        "latency_per_iter": 72.47197838186193,
        "device_tokens_per_sec": 113.04,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    },
    {
        "batch_size_per_gpu": 32,
        "max_batch_size_per_gpu": 105,
        "gradient_accumulation_steps": 64,
        "global_batch_size": 2048,
        "dp_size": 1,
        "rdp_size": 1,
        "tp_size": 8,
        "pp_size": 1,
        "sp_size": 8,
        "ep_size": 1,
        "ds_zero": "NONE",
        "total_num_gpus": 8,
        "seq_len": 1024,
        "total_num_tokens": 32768,
        "num_params_total": 6573789184.0,
        "num_params_total_mlp": 4294967296,
        "num_params_total_embedding": 131072000,
        "num_params_total_others": 2147749888.0,
        "num_active_params_total": 6573789184.0,
        "activation_recomputation": "NONE",
        "layernorm_dtype_bytes": 4,
        "mlp_activation_quant_bits": null,
        "mlp_recompute_gelu": false,
        "mlp_gated_linear_units": false,
        "achieved_flops": 156.0,
        "flops_efficiency": 0.5,
        "hbm_memory_efficiency": 0.9,
        "num_flops_total_per_micro_batch": 1345183757107200.0,
        "weight_memory_per_gpu": 1643906048.0,
        "weight_memory_embedding_per_gpu": 32768000.0,
        "weight_memory_attn_per_gpu": 536870912.0,
        "weight_memory_mlp_per_gpu": 1073741824.0,
        "weight_memory_layernorm_per_gpu": 524288.0,
        "unsharded_weight_memory_embedding": 32768000.0,
        "unsharded_weight_memory_per_layer": 50348032.0,
        "unsharded_weight_memory_attn_per_layer": 16777216.0,
        "unsharded_weight_memory_mlp_per_layer": 33554432.0,
        "unshared_weight_memory_layernorm": 16384.0,
        "gradient_memory_per_gpu": 3286894592.0,
        "optimizer_state_memory_per_gpu": 9860683776.0,
        "(weight+op_state)_memory_per_gpu": 11504589824.0,
        "estimated_fwd_prefetch_memory_per_gpu": 83116032.0,
        "estimated_bwd_prefetch_memory_per_gpu": 100696064.0,
        "loss_bwd_memory": 262144000.0,
        "activation_memory_batch_size_1": 700252160.0,
        "activation_memory_per_gpu": 22408069120.0,
        "activation_memory_attn_per_gpu": 8086618112.0,
        "activation_memory_mlp_per_gpu": 9663676416.0,
        "activation_memory_layernorm_per_gpu": 4294967296.0,
        "activation_memory_input_embedding_per_gpu": 33554432.0,
        "activation_memory_output_embedding_per_gpu": 262144000.0,
        "(weight+op_state+act)_memory_per_gpu": 33912658944.0,
        "(weight+op_state+grad)_memory_per_gpu": 14791484416.0,
        "estimated_peak_memory_per_gpu": 34174802944.0,
        "latency_per_micro_batch": 1.3906668436514833,
        "latency_fwd": 0.46355561455049443,
        "activation_recomputation_num": 0,
        "latency_fwd_attn": 0.12686672628184614,
        "latency_fwd_mlp": 0.22554084672328206,
        "latency_fwd_layernorm": 0.002340454087515666,
        "latency_fwd_tp_comm": 0.10021590357333333,
        "latency_fwd_sharded_dp_comm": 0.0,
        "latency_fwd_input_embedding": 0.0017087234742608033,
        "latency_fwd_output_embedding_loss": 0.00688296041025641,
        "latency_per_iter": 89.01073830889084,
        "device_tokens_per_sec": 368.14,
        "total_training_latency": 0.0,
        "latency_per_iter_using_flops": 68.98378241575385,
        "total_training_latency_using_flops": 0.0,
        "gpu_hours": 0.0,
        "model_name": "decapoda-research_llama-7b-hf",
        "gpu_name": "a100-sxm-80gb"
    }
]